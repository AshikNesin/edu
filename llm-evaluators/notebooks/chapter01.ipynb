{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "from set_env import set_env\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env(\"GEMINI_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "print(\"Env set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import IPython\n",
    "    in_jupyter = True\n",
    "except ImportError:\n",
    "    in_jupyter = False\n",
    "if in_jupyter:\n",
    "    nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(f\"eval_course_ch1_dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Evaluate LLMs?\n",
    "\n",
    "### Traditional Software vs LLM Testing\n",
    "Unlike traditional software where outputs are deterministic and can be unit tested, LLMs produce:\n",
    "- Non-deterministic outputs that vary between runs\n",
    "- Complex, open-ended responses for tasks like summarization and dialogue\n",
    "- Outputs that require nuanced evaluation of quality, accuracy, and safety\n",
    "\n",
    "### Key Reasons for LLM Evaluation:\n",
    "\n",
    "1. Quality Assurance\n",
    "- Conventional metrics (n-grams, semantic similarity) are insufficient for complex LLM tasks\n",
    "- Need to assess multiple dimensions like factuality, coherence, and relevance\n",
    "- Important to catch potential hallucinations and factual inconsistencies\n",
    "\n",
    "2. Safety & Alignment \n",
    "- Ensure outputs are safe and non-toxic\n",
    "- Verify adherence to ethical guidelines and business policies\n",
    "- Maintain alignment with intended use cases and user expectations\n",
    "\n",
    "3. Performance Monitoring\n",
    "- Track model performance across different tasks and domains\n",
    "- Identify areas needing improvement or fine-tuning\n",
    "- Compare different model versions or configurations\n",
    "\n",
    "4. Business Goals\n",
    "- Validate that outputs meet specific business requirements\n",
    "- Ensure cost-effective deployment of LLM solutions\n",
    "- Maintain quality standards for production systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/traditional_llm_eval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_task = \"\"\"\n",
    "You are extracting insights from some medical records.\n",
    "The records contain a medical note and a\n",
    "dialogue between a doctor and a patient. You need\n",
    "to extract values for the following: Chief\n",
    "complaint, History of present illness, Physical\n",
    "examination, symptoms experienced by the patient,\n",
    "New medications prescribed or changed, including\n",
    "dosages (N/A if not provided), and Follow-up\n",
    "instructions (N/A if not provided). Your answer\n",
    "should not include any personal identifiable\n",
    "information (PII) such as name, age, gender, or\n",
    "ID. Use \"the patient\" instead of their name, for\n",
    "example. Return your answer as a bullet list,\n",
    "where each bullet is formatted like â€¢chief\n",
    "complaint: xx. If there is no value for the key,\n",
    "the value should be N/A. Keep your response\n",
    "around 150 words (you may have to summarize some\n",
    "extracted values to stay within the word limit).\n",
    "{transcript}\n",
    "\"\"\"\n",
    "\n",
    "medical_system_prompt = \"\"\"\n",
    "You are a medical data extraction AI assistant. Your task is to accurately extract and summarize key medical information from patient records, adhering strictly to privacy guidelines and formatting instructions provided in the user's prompt. Focus on relevance and conciseness while ensuring all required fields are addressed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Have a separate dataset called \"unannotated_medical_data\"\n",
    "annotated_medical_data = weave.ref(\"weave:///a-sh0ts/medical_data_results/object/medical_data_annotations:7GcCtWgyPTWtKY48Z7v5VxwCNZXTTTpSMbmubAbyHT8\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.render import print_dialogue_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Understanding Medical Data Extraction Evaluation\n",
    "\n",
    "## The Task: What Are We Trying to Do?\n",
    "\n",
    "### Raw Data Format\n",
    "Medical conversations are messy and unstructured. Looking at our example data:\n",
    "\n",
    "1. **Dialogue Format**:\n",
    "- Back-and-forth conversation between doctor and patient\n",
    "- Contains personal details, small talk, and medical information mixed together\n",
    "- Informal language (\"hey\", \"mm-hmm\", \"yeah\")\n",
    "- Important details scattered throughout\n",
    "\n",
    "2. **Medical Notes**:\n",
    "- More structured but still in prose\n",
    "- Contains standardized sections (CHIEF COMPLAINT, HISTORY, etc.)\n",
    "- Includes sensitive information (names, ages)\n",
    "- Medical terminology and abbreviations\n",
    "\n",
    "### Extraction Goals\n",
    "The LLM needs to:\n",
    "1. Find relevant information\n",
    "2. Ignore irrelevant details\n",
    "3. Standardize the format\n",
    "4. Protect patient privacy\n",
    "5. Maintain medical accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/medical_chatbot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dialogue_data(annotated_medical_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation: Building Quality Training Data\n",
    "\n",
    "### Why Annotate?\n",
    "Raw LLM outputs aren't enough - we need expert validation to:\n",
    "1. Establish ground truth\n",
    "2. Identify edge cases\n",
    "3. Understand failure modes\n",
    "4. Create evaluation standards\n",
    "\n",
    "### Annotation Process\n",
    "Medical experts should:\n",
    "\n",
    "1. **Review the Full Context**:\n",
    "   - Read entire conversation\n",
    "   - Review medical notes\n",
    "   - Understand complete patient story\n",
    "\n",
    "2. **Evaluate LLM Output**:\n",
    "   - Check factual accuracy\n",
    "   - Verify completeness\n",
    "   - Ensure privacy protection\n",
    "   - Validate formatting\n",
    "\n",
    "3. **Provide Structured Feedback**:\n",
    "   - Binary score (pass/fail)\n",
    "   - Written explanation\n",
    "   - Specific issue identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/annotation_ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Replace with and add human annotations\n",
    "#TODO: Load this dataset as annotated_medical_data while the previous one as unannotated_medical_data\n",
    "print_dialogue_data(annotated_medical_data, indexes_to_show=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_medical_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation: Measuring Performance\n",
    "\n",
    "### Direct vs Pairwise\n",
    "For medical extraction:\n",
    "- Use direct scoring (not A/B comparison)\n",
    "- Task has objective right/wrong answers\n",
    "- Need to catch critical errors\n",
    "\n",
    "### Key Evaluation Dimensions\n",
    "\n",
    "1. **Factual Accuracy**:\n",
    "   - Are extracted details correct?\n",
    "   - Do they match the source?\n",
    "   - Is medical terminology accurate?\n",
    "\n",
    "2. **Completeness**:\n",
    "   - All required fields present?\n",
    "   - Important details included?\n",
    "   - Appropriate use of N/A?\n",
    "\n",
    "3. **Privacy Protection**:\n",
    "   - PII properly removed?\n",
    "   - Patient identity protected?\n",
    "   - Sensitive details handled appropriately?\n",
    "\n",
    "4. **Format Compliance**:\n",
    "   - Bullet point structure followed?\n",
    "   - Within word limit?\n",
    "   - Clear and readable?\n",
    "\n",
    "### Using Evaluation Results\n",
    "\n",
    "Results help us:\n",
    "1. Improve prompts\n",
    "2. Identify system limitations\n",
    "3. Set quality standards\n",
    "4. Monitor performance\n",
    "5. Train better models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/eval_task_flowchart.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
