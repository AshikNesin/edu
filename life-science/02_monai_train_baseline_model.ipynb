{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4fbce3",
   "metadata": {},
   "source": [
    "# Train a Baseline Segmentation Model\n",
    "\n",
    "In this notebook we will learn:\n",
    "- We will learn how to use specific MONAI APIs to write our training workflow, including a SoTA neural network architecture and loss function and metrics for our task.\n",
    "- Use Weights & Biases for tracking our experiments and logging and verisioning our model checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e5724",
   "metadata": {},
   "source": [
    "## 🌴 Setup and Installation\n",
    "\n",
    "First, let us install the latest version of both MONAI and Weights and Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a51c32-6279-428d-90e8-96be4e4953c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U monai wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9e1e8",
   "metadata": {},
   "source": [
    "## 🌳 Initialize a W&B Run\n",
    "\n",
    "We will start a new W&B run to start tracking our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78244d6-353b-4dd8-aa51-d6a162ffbd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"brain-tumor-segmentation\",\n",
    "    entity=\"lifesciences\",\n",
    "    job_type=\"train_baseline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f85008",
   "metadata": {},
   "source": [
    "# 🌼 Reproducibility and Configuration Management\n",
    "\n",
    "`wandb.config` allows us to easily define and manage the configurations of our experiments. This includes hyperparameters, model settings, and any other experiment variables that we use in a particular run. By centralizing this information, we can ensure consistency across runs and make your experiments more organized and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26047ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef790e1",
   "metadata": {},
   "source": [
    "Next, we set random seed for modules to enable deterministic training by setting a global seed using `monai.utils.set_determinism`. Setting a random seed (or multiple random seeds) and storing them as a configuration, we can make sure that a particular run is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e692b9-5f34-4d32-922b-836c92aee0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.utils import set_determinism\n",
    "\n",
    "config.seed = 0\n",
    "set_determinism(seed=config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9333a301",
   "metadata": {},
   "source": [
    "## 💿 Loading and Transforming the Data\n",
    "\n",
    "We will now learn using the [`monai.transforms`](https://docs.monai.io/en/stable/transforms.html) API to create and apply transforms to our data.\n",
    "\n",
    "### Creating a Custom Transform\n",
    "\n",
    "First, we demonstrate the creation of a custom transform `ConvertToMultiChannelBasedOnBratsClassesd` using [`monai.transforms.MapTransform`](https://docs.monai.io/en/stable/transforms.html#maptransform) that converts labels to multi-channel tensors based on brats18 classes:\n",
    "- label 1 is the necrotic and non-enhancing tumor core\n",
    "- label 2 is the peritumoral edema\n",
    "- label 3 is the GD-enhancing tumor.\n",
    "\n",
    "The target classes for the semantic segmentation task after applying this transform on the dataset will be\n",
    "- Tumor core\n",
    "- Whole tumor\n",
    "- Enhancing tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534ed76-7b3e-4c66-8f5f-4358c8392472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from monai.transforms import MapTransform\n",
    "\n",
    "\n",
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi-channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor), and ET (Enhancing tumor).\n",
    "\n",
    "    Reference: https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        data_dict = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct Tumor Core\n",
    "            result.append(torch.logical_or(data_dict[key] == 2, data_dict[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct Whole Tumor\n",
    "            result.append(\n",
    "                torch.logical_or(\n",
    "                    torch.logical_or(data_dict[key] == 2, data_dict[key] == 3),\n",
    "                    data_dict[key] == 1,\n",
    "                )\n",
    "            )\n",
    "            # label 2 is Enhancing Tumor\n",
    "            result.append(data_dict[key] == 2)\n",
    "            data_dict[key] = torch.stack(result, axis=0).float()\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b23d5",
   "metadata": {},
   "source": [
    "Next, we compose all the necessary transforms for the training and validations sets of the data respectively using [`monai.transforms.Compose`](https://docs.monai.io/en/stable/transforms.html#monai.transforms.Compose).\n",
    "\n",
    "The difference between the set of transforms for is that for the training dataset we're applying several transforms that are augmenting the data that are not applied to the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d511f9-492f-4677-bd1f-e97156144e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "\n",
    "\n",
    "config.roi_size = [224, 224, 144]\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # Ensure loaded images are in channels-first format\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        # Ensure the input data to be a PyTorch Tensor or numpy array\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        # Convert labels to multi-channels based on brats18 classes\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        # Change the input image’s orientation into the specified based on axis codes\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Resample the input images to the specified pixel dimension\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # Augmentation: Crop image with random size or specific size ROI\n",
    "        RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"], roi_size=config.roi_size, random_size=False\n",
    "        ),\n",
    "        \n",
    "        # Augmentation: Randomly flip the image on the specified axes\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        \n",
    "        # Normalize input image intensity\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "        # Augmentation: Randomly scale the image intensity\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # Ensure loaded images are in channels-first format\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        # Ensure the input data to be a PyTorch Tensor or numpy array\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        # Convert labels to multi-channels based on brats18 classes\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        # Change the input image’s orientation into the specified based on axis codes\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        # Resample the input images to the specified pixel dimension\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        # Normalize input image intensity\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8e71d",
   "metadata": {},
   "source": [
    "For loading the dataset, we first fetch it from the W&B dataset artifact that we had created earlier. This enables us to use the dataset as an input artifact to our visualization run, and establish the necessary lineage for our experiment.\n",
    "\n",
    "![](./assets/artifact_usage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce52f78-48d1-4b98-940c-c3cb31ecb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = wandb.use_artifact(\n",
    "    \"lifesciences/brain-tumor-segmentation/decathlon_brain_tumor:v0\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cc684",
   "metadata": {},
   "source": [
    "We now use the [`monai.apps.DecathlonDataset`](https://docs.monai.io/en/stable/apps.html#monai.apps.DecathlonDataset) to load our dataset and apply the transforms we defined on the data samples so that we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ebc99-581b-4acc-853f-dab9c9772592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.apps import DecathlonDataset\n",
    "\n",
    "config.num_workers = 4\n",
    "\n",
    "# Create the dataset for the training split\n",
    "# of the brain tumor segmentation dataset\n",
    "train_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "# Create the dataset for the validation split\n",
    "# of the brain tumor segmentation dataset\n",
    "val_dataset = DecathlonDataset(\n",
    "    root_dir=artifact_dir,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c90a0",
   "metadata": {},
   "source": [
    "We now create DataLoaders for the train and validation datasets respectively using [`monai.data.DataLoader`](https://docs.monai.io/en/stable/data.html#dataloader) which provides an iterable over the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2192be-3276-446e-b8c6-65324123a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import DataLoader\n",
    "\n",
    "config.batch_size = 2\n",
    "\n",
    "# create the train_loader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    ")\n",
    "\n",
    "# create the val_loader\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb6112",
   "metadata": {},
   "source": [
    "## 🤖 Creating the Model, Loss, and Optimizer\n",
    "\n",
    "We will be training a **SegResNet** model based on the paper [3D MRI brain tumor segmentation using auto-encoder regularization](https://arxiv.org/pdf/1810.11654.pdf). The [SegResNet](https://docs.monai.io/en/stable/networks.html#segresnet) model that comes implemented as a PyTorch Module as part of the [`monai.networks.nets`](https://docs.monai.io/en/stable/networks.html#nets) API that provides out-of-the-box implementations of SoTA neural network models for different medical imaging tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa76d0b-7e9b-4e2b-964e-ca9aada74eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import SegResNet\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "config.model_blocks_down = [1, 2, 2, 4]\n",
    "config.model_blocks_up = [1, 1, 1]\n",
    "config.model_init_filters = 16\n",
    "config.model_in_channels = 4\n",
    "config.model_out_channels = 3\n",
    "config.model_dropout_prob = 0.2\n",
    "\n",
    "# create model\n",
    "model = SegResNet(\n",
    "    blocks_down=config.model_blocks_down,\n",
    "    blocks_up=config.model_blocks_up,\n",
    "    init_filters=config.model_init_filters,\n",
    "    in_channels=config.model_in_channels,\n",
    "    out_channels=config.model_out_channels,\n",
    "    dropout_prob=config.model_dropout_prob,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fbd02",
   "metadata": {},
   "source": [
    "We will be using [Adam Optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and the [cosine annealing schedule](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html) to schedule our learning rate. This approach is designed to help in finding global minima in the optimization landscape and to provide a form of reset mechanism during training, which can improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9aba9d-055c-45a8-93c8-5a27c8aab31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.initial_learning_rate = 1e-4\n",
    "config.weight_decay = 1e-5\n",
    "config.max_train_epochs = 25\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    config.initial_learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "\n",
    "# create learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config.max_train_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d91b5",
   "metadata": {},
   "source": [
    "Next, we would define the loss as multi-label DiceLoss as proposed by the paper [V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797) using the [`monai.losses`](https://docs.monai.io/en/stable/losses.html) API and the corresponding dice metrics using the [`monai.metrics`](https://docs.monai.io/en/stable/metrics.html) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd11cb2-61a3-435f-9b89-986e459db757",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.dice_loss_smoothen_numerator = 0\n",
    "config.dice_loss_smoothen_denominator = 1e-5\n",
    "config.dice_loss_squared_prediction = True\n",
    "config.dice_loss_target_onehot = False\n",
    "config.dice_loss_apply_sigmoid = True\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "\n",
    "loss_function = DiceLoss(\n",
    "    smooth_nr=config.dice_loss_smoothen_numerator,\n",
    "    smooth_dr=config.dice_loss_smoothen_denominator,\n",
    "    squared_pred=config.dice_loss_squared_prediction,\n",
    "    to_onehot_y=config.dice_loss_target_onehot,\n",
    "    sigmoid=config.dice_loss_apply_sigmoid,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49cb10-7718-4147-9901-85ee59fbe8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.metrics import DiceMetric\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae7eb0",
   "metadata": {},
   "source": [
    "## 🚀 Automatic Mixed Precision\n",
    "\n",
    "Mixed precision training is a technique used in training neural networks that utilizes both 16-bit and 32-bit floating-point types for different parts of the computation, rather than using a single precision type throughout the entire process. This method is primarily aimed at accelerating the training process while also reducing the memory usage of the models.\n",
    "\n",
    "We will be using [`torch.amp`](https://pytorch.org/docs/stable/amp.html#module-torch.amp) provides convenience methods for mixed precision, where some operations use the `torch.float32` datatype and other operations use lower precision floating point datatype such as `torch.float16` or `torch.bfloat16`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8633e",
   "metadata": {},
   "source": [
    "### ⚖️ Gradient and Loss Scaling\n",
    "\n",
    "If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will **gradient underflow**, so the update for the corresponding parameters will be lost. \n",
    "\n",
    "In order to counteract the gradient underflow issues of FP16, especially in handling small gradient values, gradient and loss scaling is applied. This involves scaling up the loss before the gradient computation and scaling it back down afterwards. We will be using [`torch.cuda.amp.GradScaler`](https://pytorch.org/docs/stable/amp.html#gradient-scaling) to perform the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4d531-1db1-4c29-9dfa-8e570665f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use automatic mixed-precision to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba3120",
   "metadata": {},
   "source": [
    "Next, we write a utility function to perform sliding window inference using [`from monai.inferers.sliding_window_inference`](https://docs.monai.io/en/stable/inferers.html#sliding-window-inference-function) and [AMP autocast](https://pytorch.org/docs/stable/amp.html#autocasting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb4253-3fcd-491d-a378-3a05b999cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "config.inference_roi_size = (240, 240, 160)\n",
    "\n",
    "\n",
    "def inference(model, input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=config.inference_roi_size,\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        return _compute(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf2a8f",
   "metadata": {},
   "source": [
    "## 🦾 Training the Model\n",
    "\n",
    "Let's finally get to training the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0736f970",
   "metadata": {},
   "source": [
    "### 🐝 Customize Log Axes on W&B\n",
    "\n",
    "We will use Use [`wandb.define_metric`](https://docs.wandb.ai/guides/track/log/customize-logging-axes) to set a custom x axis for our W&B charts. Custom x-axes are useful in contexts where you need to log to different time steps in the past during training, asynchronously. For example, for training our brain tumor segmentation model, we can log the training loss and metrics every training step but log the validation metrics every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de81064-3ef7-4cef-a5ed-6e1fd2733c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.define_metric(\"epoch/epoch_step\")\n",
    "wandb.define_metric(\"epoch/*\", step_metric=\"epoch/epoch_step\")\n",
    "wandb.define_metric(\"batch/batch_step\")\n",
    "wandb.define_metric(\"batch/*\", step_metric=\"batch/batch_step\")\n",
    "wandb.define_metric(\"validation/validation_step\")\n",
    "wandb.define_metric(\"validation/*\", step_metric=\"validation/validation_step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93fc74",
   "metadata": {},
   "source": [
    "Next, we write simple a PyTorch-based training loop to train the brain tumor segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b71668-13c2-4414-9f76-c5d78b99d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from monai.data import decollate_batch\n",
    "\n",
    "config.validation_intervals = 1\n",
    "config.checkpoint_dir = \"./checkpoints\"\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "batch_step = 0\n",
    "validation_step = 0\n",
    "metric_values = []\n",
    "metric_values_tumor_core = []\n",
    "metric_values_whole_tumor = []\n",
    "metric_values_enhanced_tumor = []\n",
    "\n",
    "epoch_progress_bar = tqdm(range(config.max_train_epochs), desc=\"Training:\")\n",
    "\n",
    "for epoch in epoch_progress_bar:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    total_batch_steps = len(train_dataset) // train_loader.batch_size\n",
    "    batch_progress_bar = tqdm(train_loader, total=total_batch_steps, leave=False)\n",
    "\n",
    "    # Training Step\n",
    "    for batch_data in batch_progress_bar:\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        batch_progress_bar.set_description(f\"train_loss: {loss.item():.4f}:\")\n",
    "        ## Log batch-wise training loss to W&B\n",
    "        wandb.log({\"batch/batch_step\": batch_step, \"batch/train_loss\": loss.item()})\n",
    "        batch_step += 1\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= total_batch_steps\n",
    "    ## Log batch-wise training loss and learning rate to W&B\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch/epoch_step\": epoch,\n",
    "            \"epoch/mean_train_loss\": epoch_loss,\n",
    "            \"epoch/learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "        }\n",
    "    )\n",
    "    epoch_progress_bar.set_description(f\"Training: train_loss: {epoch_loss:.4f}:\")\n",
    "\n",
    "    # Validation and model checkpointing step\n",
    "    if (epoch + 1) % config.validation_intervals == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(model, val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric_values.append(dice_metric.aggregate().item())\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_values_tumor_core.append(metric_batch[0].item())\n",
    "            metric_values_whole_tumor.append(metric_batch[1].item())\n",
    "            metric_values_enhanced_tumor.append(metric_batch[2].item())\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            checkpoint_path = os.path.join(config.checkpoint_dir, \"model.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "            # Log and versison model checkpoints using W&B artifacts.\n",
    "            wandb.log_model(\n",
    "                checkpoint_path,\n",
    "                name=f\"{wandb.run.id}-checkpoint\",\n",
    "                aliases=[f\"epoch_{epoch}\"],\n",
    "            )\n",
    "\n",
    "            # Log validation metrics to W&B dashboard.\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"validation/validation_step\": validation_step,\n",
    "                    \"validation/mean_dice\": metric_values[-1],\n",
    "                    \"validation/mean_dice_tumor_core\": metric_values_tumor_core[-1],\n",
    "                    \"validation/mean_dice_whole_tumor\": metric_values_whole_tumor[-1],\n",
    "                    \"validation/mean_dice_enhanced_tumor\": metric_values_enhanced_tumor[-1],\n",
    "                }\n",
    "            )\n",
    "            validation_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1663c6",
   "metadata": {},
   "source": [
    "Now we end the experiment by calling `wandb.finish()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aeee85-8a37-4a85-8808-e12fcb5c7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
