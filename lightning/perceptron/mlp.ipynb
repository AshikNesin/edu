{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c216tB5gbTG8"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{edu_lit_mlp} -->\n",
        "\n",
        "# A Multilayer Perceptron for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1M5zGebb9J"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5Gebw6uvDF"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning==1.3.8 torchviz wandb\n",
        "!git clone https://github.com/wandb/lit_utils\n",
        "!cd \"/content/lit_utils\" && git pull\n",
        "\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "import lit_utils as lu\n",
        "\n",
        "lu.utils.filter_warnings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBOAwt9IyiWk"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1iZhbQbfyy"
      },
      "source": [
        "## Defining the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElHWRKqcu8oL"
      },
      "source": [
        "class LitMLP(lu.nn.modules.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple MLP Model with under-the-hood logging features.\"\"\"\n",
        "\n",
        "  def __init__(self, config):  # make the model\n",
        "    super().__init__()\n",
        "\n",
        "    self.layers = torch.nn.Sequential(*[  # specify our LEGOs. edit this by adding to the list!\n",
        "      lu.nn.fc.FullyConnected(\n",
        "          in_features=28 * 28, activation=config[\"activation\"](),\n",
        "                     out_features=config[\"fc1.size\"]),  # hidden layer\n",
        "      lu.nn.fc.FullyConnected(\n",
        "          in_features=config[\"fc1.size\"], activation=config[\"activation\"](),\n",
        "                     out_features=config[\"fc2.size\"]),  # hidden layer\n",
        "      lu.nn.fc.FullyConnected(\n",
        "          in_features=config[\"fc2.size\"],  # \"read-out\" layer\n",
        "                     out_features=10),\n",
        "    ])\n",
        "\n",
        "    self.loss = config[\"loss_fn\"]\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    for layer in self.layers:  # snap together the LEGOs\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJRHnIwnFjrg"
      },
      "source": [
        "## Choosing hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv3tP0Qyjffq"
      },
      "source": [
        "config = {\n",
        "  \"batch_size\": 32,\n",
        "  \"train_size\": 1024,  # reducing to a small subset to observe overfitting; set to 50000 for full dataset\n",
        "  \"max_epochs\": 35,\n",
        "  \"fc1.size\": 128,\n",
        "  \"fc2.size\": 64,\n",
        "  \"activation\": torch.nn.ReLU,\n",
        "  \"loss_fn\": torch.nn.CrossEntropyLoss(),\n",
        "  \"optimizer\": torch.optim.Adam,\n",
        "  \"optimizer.params\": {\"lr\": 3e-3},\n",
        "}\n",
        "\n",
        "lmlp = LitMLP(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzEXmBPa5-Mp"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFmjURU06A5R"
      },
      "source": [
        "dmodule  = lu.datamodules.MNISTDataModule(batch_size=config[\"batch_size\"])\n",
        "dmodule.prepare_data()\n",
        "dmodule.setup()\n",
        "dmodule.training_data = torch.utils.data.Subset(  \n",
        "  dmodule.training_data, indices=range(config[\"train_size\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_qDw3Ubv8d"
      },
      "source": [
        "## Building and Training the `Model`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo9wuMQZe4y2"
      },
      "source": [
        "### Debugging Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVoS6dMzlaE9"
      },
      "source": [
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dloader = dmodule.train_dataloader()  # set up the Loader\n",
        "\n",
        "example_batch = next(iter(dloader))  # grab a batch from the Loader\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "\n",
        "lmlp.to(\"cuda\")\n",
        "outputs = lmlp.forward(example_x)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "print(f\"Loss : {lmlp.loss(outputs, example_y)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1VaHnue9jX"
      },
      "source": [
        "### Running `.fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RIrZI-LNeN"
      },
      "source": [
        "with wandb.init(project=\"lit-mlp\", entity=\"wandb\", config=config):\n",
        "  \n",
        "  # 🪵 configure logging\n",
        "  cbs=[lu.callbacks.WandbCallback(),  # callbacks add extra features, like better logging\n",
        "       lu.callbacks.FilterLogCallback(image_size=(28, 28), log_input=True),  # this one logs the weights as images\n",
        "       lu.callbacks.ImagePredLogCallback(labels=dmodule.classes, on_train=True)  # and this one logs the inputs and outputs\n",
        "       ]\n",
        "  wandblogger = pl.loggers.WandbLogger(save_code=True)\n",
        "  if hasattr(lmlp, \"_wandb_watch_called\") and lmlp._wandb_watch_called:\n",
        "    wandblogger.watch(lmlp)  # track gradients\n",
        "\n",
        "  # 👟 configure trainer\n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                      logger=wandblogger,  # log to Weights & Biases\n",
        "                      callbacks=cbs,  # use callbacks to log lots of run data\n",
        "                      max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                      progress_bar_refresh_rate=50)\n",
        "\n",
        "  # 🏃‍♀️ run the Trainer on the model\n",
        "  trainer.fit(lmlp, datamodule=dmodule)\n",
        "\n",
        "  # 🧪 test the model on unseen data\n",
        "  trainer.test(lmlp, datamodule=dmodule)\n",
        "\n",
        "  # 💾 save the model\n",
        "  lmlp.to_onnx(\"model.onnx\", example_x, export_params=True)\n",
        "  wandb.save(\"model.onnx\", \".\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BqPMCuuLhbE"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znGZNu8-GbFB"
      },
      "source": [
        "For the exercises below, you'll want to review the results\n",
        "of your training runs on [Weights & Biases](https://wandb.ai/site).\n",
        "The link to each run's dashboard will be printed in the cell output above,\n",
        "with the name \"Run Page\".\n",
        "\n",
        "You should be able to find your run,\n",
        "along with other runs created using this notebook,\n",
        "in [this Weights & Biases dashboard](http://wandb.me/lit-mlp-workspace),\n",
        "which shows results across many runs.\n",
        "\n",
        "You can see an example run [here](https://wandb.ai/wandb/lit-mlp/runs/3h3iu4ec?workspace=user-charlesfrye).\n",
        "\n",
        " > _Tip_: to launch new training runs,\n",
        " restart the Colab notebook and run all the cells at once\n",
        " (\"Runtime > Restart and run all\").\n",
        " That way, you can always be sure what code\n",
        " was run, in case you hit an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXryJQJAMapF"
      },
      "source": [
        "\n",
        "### Set A.\n",
        "\n",
        "#### **Exercise**: Deep learning models are built by snapping \"LEGOs\" together: modular, combinable pieces. What are the LEGOs of this model?\n",
        "\n",
        "#### **Exercise**: How \"wide\" are the layers in this network? How would you make it wider or skinnier? (_Hint_: check out the `lu.nn.fc.FullyConnected` layers).\n",
        "\n",
        "#### **Exercise**: How \"deep\" is this network? How would you make it deeper? (_Hint_: check out the `nn.Sequential` inside the `__init__` of the `LitMLP` class). Try making the network deeper and see what happens to the training, validation, and test accuracies.\n",
        "\n",
        "#### **Exercise**: What happens when you decrease the value of `max_epochs` (by, say, a factor of 10)? Does training take more or less time? What happens to the training accuracy?\n",
        "\n",
        "#### **Exercise**: At the end of training, compare the accuracy on the training set with that on the validation and test sets. Which one is lower and which one is higher?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLBzG7-OMcYN"
      },
      "source": [
        "### Set B.\n",
        "\n",
        "#### **Exercise**: Add a `torch.nn.Dropout` layer to the network. (_Hint_: it goes in the `Sequential`. But where?). What effect does this have on the validation accuracy after a large number (say, 50) of epochs? What about after only a few epochs? You may want to increase the `dataset_size` in the `config` to `50000`.\n",
        "\n",
        "#### **Exercise**: Add a `torch.nn.Dropout` layer after the final fully-connected layer -- the one that produces the un-normalized class probabilities. What impact does this have on training accuracy? What about validation accuracy? [Read about `model.eval`](https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch) and use what you've learned to explain this difference."
      ]
    }
  ]
}