{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " mlp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzZ4ylqlO81rirLep++4db",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/lightning/perceptron/mlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c216tB5gbTG8"
      },
      "source": [
        "# A Multilayer Perceptron for MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1M5zGebb9J"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5Gebw6uvDF"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning wandb\n",
        "\n",
        "repo_url = \"https://raw.githubusercontent.com/wandb/edu/main/\"\n",
        "utils_path = \"lightning/perceptron/utils.py\"\n",
        "# Download a util file of helper methods for this notebook\n",
        "!curl {repo_url + utils_path} --output utils.py\n",
        "\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets\n",
        "import wandb\n",
        "\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1iZhbQbfyy"
      },
      "source": [
        "## Defining the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElHWRKqcu8oL"
      },
      "source": [
        "class FullyConnected(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, in_features, out_features, activation=lambda xs: xs):\n",
        "    super().__init__()\n",
        "    self.linear = torch.nn.Linear(in_features, out_features)\n",
        "    self.activation = activation  # defaults to passing inputs unchanged\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.activation(self.linear(x))\n",
        "\n",
        "class LitMLP(utils.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple MLP Model, with under-the-hood wandb\n",
        "  and pytorch-lightning features (logging, metrics, etc.).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, max_images_to_display=32):  # make the model\n",
        "    super().__init__(max_images_to_display=max_images_to_display)\n",
        "\n",
        "    self.layers = torch.nn.ModuleList([  # specify our LEGOs. edit this by adding to the list!\n",
        "        FullyConnected(in_features=28 * 28, activation=config[\"activation\"],\n",
        "                       out_features=config[\"fc1.size\"]),  # hidden layer\n",
        "        FullyConnected(in_features=config[\"fc1.size\"],  # \"read-out\" layer\n",
        "                       out_features=10),\n",
        "    ])\n",
        "\n",
        "    self.loss = config[\"loss\"]\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    for layer in self.layers:  # snap together the LEGOs\n",
        "      x = layer(x)\n",
        "    return F.log_softmax(x, dim=1)  # compute log of softmax, for numerical reasons\n",
        "\n",
        "  def configure_optimizers(self):  # ‚ö°: setup for .fit\n",
        "    return self.optimizer(self.parameters(), **self.optimizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-0WwC57bigK"
      },
      "source": [
        "## Defining the `DataModule` & `DataLoader`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7InOksIvsTK"
      },
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "  def __init__(self, batch_size=64):\n",
        "    super().__init__()  # ‚ö°: we inherit from LightningDataModule\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def prepare_data(self, validation_size=10_000): # ‚ö°: how do we set up the data?\n",
        "    # download the data from the internet\n",
        "    mnist = torchvision.datasets.MNIST(\".\", train=True, download=True)\n",
        "\n",
        "    # set up shapes and types\n",
        "    self.digits, self.labels = mnist.data.float(), mnist.targets\n",
        "    self.digits = torch.divide(self.digits, 255.)\n",
        "\n",
        "    self.training_data = torch.utils.data.TensorDataset(self.digits[:-validation_size],\n",
        "                                                        self.labels[:-validation_size])\n",
        "    self.validation_data = torch.utils.data.TensorDataset(self.digits[-validation_size:],\n",
        "                                                          self.labels[-validation_size:])\n",
        "    self.validation_size = validation_size\n",
        "\n",
        "  def train_dataloader(self):  # ‚ö°: how do we go from dataset to dataloader?\n",
        "    \"\"\"The DataLoaders returned by a DataModule produce data for a model.\n",
        "    \n",
        "    This DataLoader is used during training.\"\"\"\n",
        "    return DataLoader(self.training_data, batch_size=self.batch_size)\n",
        "\n",
        "  def val_dataloader(self):  # ‚ö°: what about during validation?\n",
        "    \"\"\"The DataLoaders returned by a DataModule produce data for a model.\n",
        "    \n",
        "    This DataLoader is used during validation, at the end of each epoch.\"\"\"\n",
        "    return DataLoader(self.validation_data, batch_size=self.validation_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_qDw3Ubv8d"
      },
      "source": [
        "## Building and Training the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUXHdz2BPko3"
      },
      "source": [
        "config = {\n",
        "    \"batch_size\": 256,\n",
        "    \"max_epochs\": 10,\n",
        "    \"fc1.size\": 32,\n",
        "    \"activation\": torch.nn.ReLU(),\n",
        "    \"loss\": torch.nn.NLLLoss(), \n",
        "    \"optimizer\": torch.optim.SGD,\n",
        "    \"optimizer.params\": {\"lr\": 0.01},\n",
        "}\n",
        "\n",
        "dmodule  = MNISTDataModule(batch_size=config[\"batch_size\"])\n",
        "lmlp = LitMLP(config, max_images_to_display=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo9wuMQZe4y2"
      },
      "source": [
        "### Model Information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDfyzjo_dPtc"
      },
      "source": [
        "print(lmlp)\n",
        "print(f\"Parameter Count: {lmlp.count_params()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVoS6dMzlaE9"
      },
      "source": [
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dmodule.prepare_data()\n",
        "dloader = dmodule.train_dataloader()\n",
        "\n",
        "example_batch = next(iter(dloader))\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "\n",
        "lmlp.to(\"cuda\")\n",
        "outputs = lmlp.forward(example_x)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "print(f\"Loss : {lmlp.loss(outputs, example_y)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1VaHnue9jX"
      },
      "source": [
        "### Running `.fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RIrZI-LNeN"
      },
      "source": [
        "# üëü configure Trainer \n",
        "trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                     logger=pl.loggers.WandbLogger(\n",
        "                       project=\"lit-mlp\", entity=\"wandb\", config=config,\n",
        "                       save_code=True),  # log to Weights & Biases\n",
        "                     max_epochs=config[\"max_epochs\"], log_every_n_steps=10)\n",
        "\n",
        "# üèÉ‚Äç‚ôÄÔ∏è run the Trainer on the model\n",
        "trainer.fit(lmlp, dmodule)\n",
        "\n",
        "# üíæ save the model\n",
        "lmlp.to_onnx(\"model.onnx\", example_x, export_params=True)\n",
        "wandb.save(\"model.onnx\")\n",
        "\n",
        "# üèÅ close out the run\n",
        "wandb.finish()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}