{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "debug_hyperparameters.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/lightning/cnn/debug_hyperparameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c216tB5gbTG8"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# Debugging Hyperparameters for a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN1M5zGebb9J"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu5Gebw6uvDF"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning torchviz wandb\n",
        "\n",
        "repo_url = \"https://raw.githubusercontent.com/wandb/edu/main/\"\n",
        "utils_path = \"lightning/utils.py\"\n",
        "# Download a util file of helper methods for this notebook\n",
        "!curl {repo_url + utils_path} --output utils.py\n",
        "\n",
        "import math\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.datasets\n",
        "import wandb\n",
        "\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rApX_SrmWQAd"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wM1iZhbQbfyy"
      },
      "source": [
        "## Defining the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElHWRKqcu8oL"
      },
      "source": [
        "class FullyConnected(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, config, batchnorm=False, dropout=0, activation=None):\n",
        "    super().__init__()\n",
        "    out_features = config[\"out_features\"]\n",
        "    self.linear = torch.nn.Linear(**config)\n",
        "    if activation is None:\n",
        "      activation = torch.nn.Identity  # defaults to passing inputs unchanged\n",
        "    self.activation = activation()\n",
        "\n",
        "    # add batchnorm and dropout\n",
        "    post_act_layers = []\n",
        "    if batchnorm:\n",
        "      post_act_layers.append(torch.nn.BatchNorm1d(out_features))\n",
        "    if dropout:\n",
        "      post_act_layers.append(torch.nn.Dropout(dropout))\n",
        "    self.post_act = torch.nn.Sequential(*post_act_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.post_act(self.activation(self.linear(x)))\n",
        "\n",
        "\n",
        "class Convolution(pl.LightningModule):\n",
        "\n",
        "  def __init__(self, config, batchnorm=False, dropout=0, activation=None):\n",
        "    super().__init__()\n",
        "    out_channels = config[\"out_channels\"]\n",
        "    self.conv2d = torch.nn.Conv2d(**config)\n",
        "    if activation is None:\n",
        "      activation = torch.nn.Identity  # defaults to passing inputs unchanged\n",
        "    self.activation = activation()\n",
        "\n",
        "    # add batchnorm and dropout\n",
        "    post_act_layers = []\n",
        "    if batchnorm:\n",
        "      post_act_layers.append(torch.nn.BatchNorm2d(out_channels))\n",
        "    if dropout:\n",
        "      post_act_layers.append(torch.nn.Dropout2d(dropout))\n",
        "    self.post_act = torch.nn.Sequential(*post_act_layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.post_act(self.activation(self.conv2d(x)))\n",
        "\n",
        "class LitCNN(utils.LoggedImageClassifierModule):\n",
        "  \"\"\"A simple CNN Model, with under-the-hood wandb\n",
        "  and pytorch-lightning channels (logging, metrics, etc.).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, max_images_to_display=32):  # make the model\n",
        "    super().__init__(max_images_to_display=max_images_to_display)\n",
        "\n",
        "    # first, convolutional component\n",
        "    self.conv_layers = torch.nn.Sequential(\n",
        "      Convolution(config[\"conv\"][0],\n",
        "                  activation=config[\"conv\"][\"activation\"],\n",
        "                  batchnorm=config[\"conv\"][\"batchnorm\"],\n",
        "                  dropout=config[\"conv\"][\"dropout\"]),\n",
        "      Convolution(config[\"conv\"][1],\n",
        "                  activation=config[\"conv\"][\"activation\"],\n",
        "                  batchnorm=config[\"conv\"][\"batchnorm\"],\n",
        "                  dropout=config[\"conv\"][\"dropout\"]),\n",
        "      torch.nn.MaxPool2d(**config[\"pool\"]),\n",
        "    )\n",
        "\n",
        "    # need a fixed-size input for fully-connected component,\n",
        "    #  so apply a \"re-sizing\" layer, to size set in config\n",
        "    self.resize_layer = torch.nn.AdaptiveAvgPool2d(\n",
        "      (config[\"final_height\"], config[\"final_width\"]))\n",
        "\n",
        "    final_size = (config[\"final_height\"] * config[\"final_width\"]\n",
        "                  * config[\"conv\"][1][\"out_channels\"])\n",
        "    config[\"fc\"][0][\"in_features\"] = final_size\n",
        "\n",
        "    # now, we can apply our fully-connected component\n",
        "    self.fc_layers = torch.nn.Sequential(\n",
        "      FullyConnected(config[\"fc\"][0],\n",
        "                     activation=config[\"fc\"][\"activation\"],\n",
        "                     batchnorm=config[\"fc\"][\"batchnorm\"],\n",
        "                     dropout=config[\"fc\"][\"dropout\"]),\n",
        "      FullyConnected(config[\"fc\"][1],\n",
        "                     activation=config[\"fc\"][\"activation\"],\n",
        "                     batchnorm=config[\"fc\"][\"batchnorm\"],\n",
        "                     dropout=config[\"fc\"][\"dropout\"]),\n",
        "      # \"read-out\" layer produces class predictions\n",
        "      FullyConnected({\"in_features\": config[\"fc\"][1][\"out_features\"],\n",
        "                      \"out_features\": 10}),\n",
        "    )\n",
        "\n",
        "    self.loss = config[\"loss\"]\n",
        "    self.optimizer = config[\"optimizer\"]\n",
        "    self.optimizer_params = config[\"optimizer.params\"]\n",
        "\n",
        "  def forward(self, x):  # produce outputs\n",
        "    # first apply convolutional layers\n",
        "    for layer in self.conv_layers: \n",
        "      x = layer(x)\n",
        "\n",
        "    # then convert to a fixed-size vector\n",
        "    x = self.resize_layer(x)\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "\n",
        "    # then apply the fully-connected layers\n",
        "    for layer in self.fc_layers: # snap together the LEGOs\n",
        "      x = layer(x)\n",
        "\n",
        "    return F.log_softmax(x, dim=1)  # compute log of softmax, for numerical reasons\n",
        "\n",
        "  def configure_optimizers(self):  # ⚡: setup for .fit\n",
        "    return self.optimizer(self.parameters(), **self.optimizer_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq_qDw3Ubv8d"
      },
      "source": [
        "## Building and Training the `Model`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUXHdz2BPko3"
      },
      "source": [
        "config = {\n",
        "  \"batch_size\": 1024,\n",
        "  \"max_epochs\": 5,\n",
        "  \"conv\": {  # configuration for the convolutional layers\n",
        "      \"activation\": torch.nn.Tanh,  # which activation function in the conv layers?\n",
        "      \"batchnorm\": True,  # should we use batchnorm in the conv layers?\n",
        "      \"dropout\": 0.9,  # how much dropout should we apply? set to 0 to deactivate\n",
        "      0: {  # these are passed as kwargs to the first torch.nn.Conv2d\n",
        "          \"in_channels\": 1,  # must match number of channels in data\n",
        "          \"out_channels\": 3,  # must match conv[1][\"out_channels\"]\n",
        "          \"kernel_size\": [7, 3],\n",
        "          \"padding\": [5, 0],\n",
        "          \"stride\": [1, 2], \n",
        "          \"dilation\": [2, 1],\n",
        "      },\n",
        "      1: {  # these are passed as kwargs to the second torch.nn.Conv2d\n",
        "          \"in_channels\": 3,  # must match conv[0][\"out_channels\"]\n",
        "          \"out_channels\": 128,\n",
        "          \"kernel_size\": [2, 5],\n",
        "          \"padding\": [1, 3],\n",
        "          \"stride\": [1, 4], \n",
        "          \"dilation\": [6, 1],\n",
        "      },\n",
        "  },\n",
        "  \"pool\": {  # these are passed as kwargs to torch.nn.MaxPool2d\n",
        "      \"kernel_size\": 3,\n",
        "      \"stride\": 1,\n",
        "  },\n",
        "  \"final_height\": 8,  # how large should we resize conv outputs to?\n",
        "  \"final_width\": 8,   #  this hyperparameter can stay fixed\n",
        "  \"fc\": {  # configuration for the fully-connected/torch.nn.Linear layers\n",
        "        \"activation\": torch.nn.Identity,  # which activation function in the linear layers?\n",
        "        \"batchnorm\": False,  # should we use batchnorm in the linear layers?\n",
        "        \"dropout\": 0.,  # how much dropout should we apply? set to 0 to deactivate\n",
        "        0 : {  # these are passed as kwargs to the first torch.nn.Linear\n",
        "            \"in_features\": None,  # calculated from other values\n",
        "            \"out_features\": 10,  # must match fc[1][\"in_features\"]\n",
        "        },\n",
        "        1 : {  # these are passed as kwargs to the second torch.nn.Linear\n",
        "            \"in_features\": 10,  # must match fc[0][\"out_features\"]\n",
        "            \"out_features\": 16,\n",
        "        },\n",
        "  },\n",
        "  \"loss\": torch.nn.NLLLoss(),  # cross-entropy loss\n",
        "  \"optimizer\": torch.optim.Adam,\n",
        "  \"optimizer.params\": {\"lr\": 0.1},\n",
        "}\n",
        "\n",
        "dmodule = utils.MNISTDataModule(batch_size=config[\"batch_size\"])\n",
        "lcnn = LitCNN(config, max_images_to_display=32)\n",
        "dmodule.prepare_data()\n",
        "dmodule.setup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo9wuMQZe4y2"
      },
      "source": [
        "### Debugging Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVoS6dMzlaE9"
      },
      "source": [
        "# for debugging purposes (checking shapes, etc.), make these available\n",
        "dloader = dmodule.train_dataloader()  # set up the Loader\n",
        "\n",
        "example_batch = next(iter(dloader))  # grab a batch from the Loader\n",
        "example_x, example_y = example_batch[0].to(\"cuda\"), example_batch[1].to(\"cuda\")\n",
        "\n",
        "print(f\"Input Shape: {example_x.shape}\")\n",
        "\n",
        "lcnn.to(\"cuda\")\n",
        "conv_outs = lcnn.conv_layers(example_x)\n",
        "print(f\"Conv Output Shape: {conv_outs.shape}\")\n",
        "fc_inputs = torch.flatten(lcnn.resize_layer(conv_outs), start_dim=1)\n",
        "print(f\"FC Input Shape: {fc_inputs.shape}\")\n",
        "outputs = F.log_softmax(lcnn.fc_layers(fc_inputs), dim=1)\n",
        "print(f\"Output Shape: {outputs.shape}\")\n",
        "print(f\"Target Shape: {example_y.shape}\")\n",
        "print(f\"Loss : {lcnn.loss(outputs, example_y)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vi1VaHnue9jX"
      },
      "source": [
        "### Running `.fit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0RIrZI-LNeN"
      },
      "source": [
        "with wandb.init(config=config, project=\"debug-cnn\", entity=\"wandb\"):\n",
        "  lcnn = LitCNN(config, max_images_to_display=32)\n",
        "  dmodule = utils.MNISTDataModule(batch_size=config[\"batch_size\"])\n",
        "  # 👟 configure Trainer \n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                      logger=pl.loggers.WandbLogger(\n",
        "                        save_code=True),  # log to Weights & Biases\n",
        "                      callbacks=[utils.FilterLogCallback((), log_input=True)],\n",
        "                      max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                      progress_bar_refresh_rate=50)\n",
        "  \n",
        "  # 🏃‍♀️ run the Trainer on the model\n",
        "  trainer.fit(lcnn, dmodule)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGhZuCABR8r0"
      },
      "source": [
        "## Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crL5i4kKAlao"
      },
      "source": [
        "#### 1. Back to the Defaults\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v69Ys-rHA7lu"
      },
      "source": [
        "With the original, intentionally bad values in the config above,\n",
        "the accuracy on the validation set after 5 epochs should typically be\n",
        "around 50%.\n",
        "\n",
        "This performance is far better than chance for this dataset (10% accuracy),\n",
        "but very far away from the best possible performance with\n",
        "the right hyperparameters\n",
        "(near 100% accuracy).\n",
        "It's important to reflect on what this means for debugging neural network\n",
        "hyperparameter choices --\n",
        "unless you know, because of your or others' past work,\n",
        "what the ceiling for performance on your metric for your model is,\n",
        "you never know whether you need to keep tweaking the hyperparameters\n",
        "or go back to the drawing board.\n",
        "\n",
        "We can start by returning some of the hyperparameter values\n",
        "to their defaults.\n",
        "\n",
        "Walk through the `config` and find hyperparameters that have default values\n",
        "in PyTorch:\n",
        "`padding`, `stride`, and `dilation` for the `conv`\n",
        "and `pool` layers, to start.\n",
        "\n",
        "Look up the documentation for [`Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "and [`MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html),\n",
        "find the default values for these hyperparameters,\n",
        "and set the values in the config to those default values,\n",
        "then run training again.\n",
        "\n",
        "Does the validation accuracy metric improve?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiQg4gdBAt_1"
      },
      "source": [
        "#### 2. Standing on the Shoulders of Giants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXvmctYA67_"
      },
      "source": [
        "Setting as many values to their defaults as possible is a good start,\n",
        "but there are other hyperparameters,\n",
        "like kernel size,\n",
        "that don't have default values.\n",
        "How do we set those?\n",
        "\n",
        "There are many tutorials online about how to train\n",
        "a convolutional neural network\n",
        "to solve an image classification problem.\n",
        "They will typically get 90% validation accuracy\n",
        "or more on this problem.\n",
        "These tutorials will generally include their hyperparameter choices,\n",
        "either explicitly, inline (ideally with an explanation!),\n",
        "or implicitly, in their code.\n",
        "\n",
        "Review some of the tutorials and examples below to find\n",
        "good values for the hyperparameters --\n",
        "skip over the text and find the code,\n",
        "so that you can focus on their hyperparameter choices.\n",
        "All of them will choose kernel sizes,\n",
        "channel/feature counts in convolutional/linear layers,\n",
        "and activation functions.\n",
        "Compare these against each other and the hyperparameters above\n",
        "to find reasonable values.\n",
        "\n",
        "- [\"Convolutional Neural Networks in PyTorch\", from Adventures in ML](https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/).\n",
        "Look for the \"Creating the Model\" section.\n",
        "Compare the channel counts and linear layer sizes\n",
        "from this example with the values above.\n",
        "- [\"Dropout in PyTorch\", by Ayush Thakur](https://wandb.ai/authors/ayusht/reports/Dropout-in-PyTorch-An-Example--VmlldzoxNTgwOTE).\n",
        "Note where the Dropout is applied and what its value is\n",
        "(`_forward_features` here applies the conv and pooling layers).\n",
        "Compare those to the hyperparameters above.\n",
        "- [Official PyTorch MNIST example code](https://github.com/pytorch/examples/blob/cbb760d5e50a03df667cdc32a61f75ac28e11cbf/mnist/main.py#L11).\n",
        "Which activation function is used? What values are used for Dropout?\n",
        "- [\"MNIST Handwritten Digit Recognition in PyTorch\", by Gregor Koehler](https://nextjournal.com/gkoehler/pytorch-mnist).\n",
        "Note the choice of pooling size and stride here.\n",
        "\n",
        "There are some shared themes you might notice:\n",
        "- Which activation function is most commonly used?\n",
        "- Are dropout values typically above or below `0.5`?\n",
        "- None of these examples use batch normalization.\n",
        "What happens when you deactivate batch normalization?\n",
        "- Are kernels/padding/dilation/stride typically symmetric\n",
        "(height = width) or asymmetric? Do any of the examples deviate\n",
        "from the default values for the parameters that have defaults?\n",
        "\n",
        "You can also look further afield:\n",
        "- Search the web for \"deep learning pytorch mnist\",\n",
        "possibly including search terms like \"dropout\", \"cnn\", etc.,\n",
        "to find even more examples and tutorials.\n",
        "You might also find helpful nuggets\n",
        "if you look for examples in Keras/TensorFlow!\n",
        "- Research papers often contain useful insight,\n",
        "even though they're generally harder to read\n",
        "than blog posts.\n",
        "The architecture above is inspired by the\n",
        "[VGGNet paper](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "from ICLR 2015, by Simonyan et al.\n",
        "See Section 2, \"ConvNet Configurations\",\n",
        "for most of the hyperparameter choices.\n",
        "Note that their classifier has 1000 classes,\n",
        "so you'll want to scale the channel/feature\n",
        "dimensions down accordingly.\n",
        "\n",
        "> _Note_: some examples use a slightly different API\n",
        "for the max-pooling/dropout layers\n",
        "(e.g. [this line](https://github.com/pytorch/examples/blob/cbb760d5e50a03df667cdc32a61f75ac28e11cbf/mnist/main.py#L26)\n",
        "in the official PyTorch example),\n",
        "but that API has the same arguments\n",
        "(details [here](https://stackoverflow.com/questions/58514197/difference-between-nn-maxpool2d-vs-nn-functional-max-pool2d))."
      ]
    }
  ]
}