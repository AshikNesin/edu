{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lit-gan",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "dBYvpY9LJK2R",
        "UQEhKxVIiChr",
        "lwJcf96FGBvf"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/lightning/gan/gan-mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ukHDVrs7ka"
      },
      "source": [
        "<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "\n",
        "# Generative Adversarial Networks for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlLA5HqNosnL"
      },
      "source": [
        "%%capture\n",
        "!pip install pytorch-lightning==1.3.8 torchviz wandb\n",
        "!git clone https://github.com/wandb/lit_utils\n",
        "!cd \"/content/lit_utils\" && git pull\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import wandb\n",
        "\n",
        "import lit_utils as lu\n",
        "\n",
        "# remove slow mirror from list of MNIST mirrors\n",
        "torchvision.datasets.MNIST.mirrors = lu.datamodules.ClassificationMNIST.mirrors\n",
        "\n",
        "lu.utils.filter_warnings()\n",
        "lu.datamodules.mnist.reverse_palette = lambda img: img  # work-around to turn off palette-reversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N7ejUSwpU8Z"
      },
      "source": [
        "# GAN Module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a3WKfWTpTnT"
      },
      "source": [
        "class LitGAN(lu.nn.modules.LoggedLitModule):\n",
        "  \"\"\"A basic image GAN in PyTorch Lightning.\n",
        "\n",
        "  Also includes some under-the-hood Weights & Biases logging.\n",
        "\n",
        "  Instantiates a generator and discriminator based on the provided config dictionary,\n",
        "  defines separate optimizers for each, and defines logging, loss, and forward\n",
        "  pass logic.\n",
        "\n",
        "  NOTE: training_step is defined a few cells down, rather than inside the class.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config): \n",
        "    super().__init__()\n",
        "\n",
        "    # hyperparameters\n",
        "    self.image_size = config[\"image_size\"]\n",
        "    self.latent_dim = config[\"latent_dim\"]\n",
        "    self.loss = config[\"loss\"]\n",
        "\n",
        "    # networks\n",
        "    self.generator = Generator(config)\n",
        "    self.generator_optim_config = config[\"generator.optim\"]\n",
        "    self.discriminator = Discriminator(config)\n",
        "    self.discriminator_optim_config = config[\"discriminator.optim\"]\n",
        "\n",
        "    # for logging purposes\n",
        "    self.log_interval = config[\"log_interval\"]\n",
        "    self.logged_images = 8\n",
        "    self.logged_metadata = False\n",
        "    # keep a fixed set of locations in latent space around for logging\n",
        "    self.validation_z = torch.randn(self.logged_images, self.latent_dim)\n",
        "\n",
        "    # metrics for GAN training\n",
        "    #   on fake, how often is discriminator fooled?\n",
        "    self.generator_win_percentage = pl.metrics.Accuracy()  \n",
        "    #   on even mix of fake+real, how often is discriminator correct?\n",
        "    self.discriminator_win_percentage = pl.metrics.Accuracy()\n",
        "\n",
        "  def forward(self, z):\n",
        "    return self.discriminator(self.generator(z))\n",
        "\n",
        "  def adversarial_loss(self, y_hat, y):\n",
        "    return self.loss(y_hat, y)\n",
        "\n",
        "  # for a GAN, we need two optimizers: one for generator, one for discriminator\n",
        "  def configure_optimizers(self):\n",
        "    generator_optimizer = self.optim_from_config(self.generator_optim_config, self.generator.parameters())\n",
        "    discriminator_optimizer = self.optim_from_config(self.discriminator_optim_config, self.discriminator.parameters())\n",
        "    return [generator_optimizer, discriminator_optimizer], []\n",
        "\n",
        "  # defined below in a different cell\n",
        "  def training_step(self):\n",
        "    pass\n",
        "\n",
        "  ##\n",
        "  # Logging code\n",
        "  ##\n",
        "\n",
        "  # on each epoch, log images and some image statistics\n",
        "  def on_epoch_end(self):\n",
        "    # Grab some images from the set sampled during training to log\n",
        "    sample_imgs = self.generated_images[:self.logged_images].detach()\n",
        "    # Turn them into a nice grid of images for logging\n",
        "    sampled_grid = torchvision.utils.make_grid(sample_imgs, nrow=1, value_range=(0, 1), pad_value=0.5)\n",
        "    # Across the sample, what are the means and variances at each pixel?\n",
        "    sampled_mean, sampled_var = torch.mean(sample_imgs, dim=0), torch.var(sample_imgs, dim=0)\n",
        "\n",
        "    # Check the outputs at a fixed set of positions in the latent space\n",
        "    z = self.validation_z.type_as(self.generator.layers[0].weight)\n",
        "    valid_imgs = self.generator(z).detach()\n",
        "    # Turn them into a nice grid of images for logging\n",
        "    valid_grid = torchvision.utils.make_grid(valid_imgs, nrow=1, value_range=(0, 1), pad_value=0.5)\n",
        "    # Across the sample, what are the means and variances at each pixel?\n",
        "    valid_mean, valid_var = torch.mean(valid_imgs, dim=0), torch.var(valid_imgs, dim=0)\n",
        "\n",
        "    # Log everything to W&B\n",
        "    self.logger.experiment.log({\"image/sampled_images\": wandb.Image(sampled_grid),\n",
        "                                \"image/validation_images\": wandb.Image(valid_grid),\n",
        "                                \"image/sampled_image_mean\": wandb.Image(sampled_mean),\n",
        "                                \"image/sampled_image_var\": wandb.Histogram(sampled_var.cpu()),\n",
        "                                \"image/valid_image_mean\": wandb.Image(valid_mean),\n",
        "                                \"image/valid_image_var\": wandb.Histogram(valid_var.cpu()),\n",
        "                                \"trainer/epoch\": self.current_epoch})\n",
        "\n",
        "    # Log metadata to W&B\n",
        "    if not self.logged_metadata:\n",
        "      self.max_logged_images = 0  # deactivate automated logging\n",
        "      self.do_logging(sample_imgs, None, 0, self.discriminator_outputs, {}, step=\"training\")\n",
        "      self.logged_metadata = True\n",
        "      wandb.run.config[\"generator_nparams\"] = lu.callbacks.count_params(gan.generator)\n",
        "      wandb.run.config[\"discriminator_nparams\"] = lu.callbacks.count_params(gan.discriminator)\n",
        "\n",
        "  # on each training step (defined below),\n",
        "  #  log these quantities and report them to W&B, with averages over epochs\n",
        "  def training_step_end(self, metrics):\n",
        "    if \"g_loss\" in metrics.keys():\n",
        "      loss = metrics[\"g_loss\"]\n",
        "      win_perc = metrics[\"generator_win_percentage\"]\n",
        "      prefix = \"train/generator\"\n",
        "    else:\n",
        "      loss = metrics[\"d_loss\"]\n",
        "      win_perc = metrics[\"discriminator_win_percentage\"]\n",
        "      prefix = \"train/discriminator\"\n",
        "  \n",
        "    batch_idx = metrics[\"batch_idx\"]\n",
        "    if not batch_idx % self.log_interval:\n",
        "      self.log_dict({prefix + \"/loss/batch\": loss, prefix + \"/win_perc/batch\": win_perc},\n",
        "                    on_epoch=False, on_step=True)\n",
        "    else:\n",
        "      self.log_dict({prefix + \"/loss/epoch\": loss, prefix + \"/win_perc/epoch\": win_perc},\n",
        "                    on_epoch=True, on_step=False)\n",
        "  \n",
        "    return metrics\n",
        "\n",
        "  @staticmethod\n",
        "  def optim_from_config(config, parameters):\n",
        "    optimizer = config[\"optimizer\"](parameters, **config[\"optimizer.params\"])\n",
        "    return optimizer\n",
        "\n",
        "# minor implementation detail, due to Python inheritance\n",
        "try:\n",
        "  # we don't have a separate validation_step, so need to remove\n",
        "  del lu.nn.modules.LoggedLitModule.validation_step\n",
        "  del lu.nn.modules.LoggedLitModule.test_step\n",
        "except AttributeError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbsTDvtOo0xW"
      },
      "source": [
        "# Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGgt6PlDo0Kv"
      },
      "source": [
        "class Generator(pl.LightningModule):\n",
        "  \"\"\"Generator module for an image GAN in PyTorch Lightning.\n",
        "\n",
        "  .forward takes a batch of vectors with dimension config[\"latent_dim\"]\n",
        "  as input and returns images of size config[\"image_size\"] as output.\n",
        "\n",
        "  Try defining different .block methods or changing the hyperparameters\n",
        "  of the blocks.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.latent_dim = config[\"latent_dim\"]\n",
        "    self.image_size = config[\"image_size\"]\n",
        "    self.activation = config[\"activation\"]\n",
        "    self.normalize = config[\"normalize\"]\n",
        "\n",
        "\n",
        "    self.layers = torch.nn.Sequential(\n",
        "      *self.block(self.latent_dim, 128, self.activation),\n",
        "      *self.block(128, 256, self.activation, normalize=self.normalize),\n",
        "      *self.block(256, 512, self.activation, normalize=self.normalize),\n",
        "      *self.block(512, 1024, self.activation, normalize=self.normalize),\n",
        "      torch.nn.Linear(1024, get_flat_size(self.image_size)),\n",
        "      torch.nn.Sigmoid()  # image pixels are in [0, 1]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    img = x.view(x.shape[0], *self.image_size)\n",
        "    return img\n",
        "\n",
        "  @staticmethod\n",
        "  def block(in_dims, out_dims, activation=torch.nn.ReLU, normalize=False):\n",
        "      layers = [torch.nn.Linear(in_dims, out_dims)]\n",
        "      if normalize:\n",
        "        layers.append(torch.nn.BatchNorm1d(out_dims))\n",
        "      layers.append(activation())\n",
        "      return layers\n",
        "\n",
        "\n",
        "def get_flat_size(image_size):\n",
        "  return np.prod(image_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVEXcvekqFOp"
      },
      "source": [
        "# Discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVAu7ntdqF6G"
      },
      "source": [
        "class Discriminator(pl.LightningModule):\n",
        "  \"\"\"Discriminator module for an image GAN in PyTorch Lightning.\n",
        "\n",
        "  .forward takes a batch of images with size config[\"image_size\"]\n",
        "  as input and returns scalars in [0, 1].\n",
        "\n",
        "  Try adding convolutional components at the beginning of self.layers.\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.image_size = config[\"image_size\"]\n",
        "    self.activation = config[\"activation\"]\n",
        "\n",
        "    self.layers = torch.nn.Sequential(\n",
        "      torch.nn.Linear(get_flat_size(self.image_size), 512),\n",
        "      self.activation(),\n",
        "      torch.nn.Linear(512, 256),\n",
        "      self.activation(),\n",
        "      torch.nn.Linear(256, 1),\n",
        "      torch.nn.Sigmoid(),\n",
        "      )\n",
        "\n",
        "  def forward(self, img):\n",
        "    x = torch.flatten(img, start_dim=1) # flatten all except batch dimension\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFvS4oOfqGHU"
      },
      "source": [
        "# Training Step\n",
        "\n",
        "The training for a GAN is more complex than for other types of networks:\n",
        "we have to alternate between training the discriminator and the generator,\n",
        "and each trains on slightly different data.\n",
        "\n",
        "So for clarity, we've split the definition of the `training_step` out\n",
        "from the rest of the module code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhIMOGXEqHyX"
      },
      "source": [
        "def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "  imgs, _ = batch  # ignore labels\n",
        "  batch_sz = imgs.shape[0]\n",
        "\n",
        "  # implementation detail: we use two optimizers, stored in a lsit\n",
        "  training_generator = optimizer_idx == 0\n",
        "  training_discriminator = not training_generator\n",
        "\n",
        "  if training_generator:\n",
        "\n",
        "    # 1. Sample random input for the generator\n",
        "    z = torch.randn(batch_sz, self.latent_dim).type_as(imgs)\n",
        "\n",
        "    # 2. Generator makes images from random input\n",
        "    self.generated_images = self.generator(z)\n",
        "\n",
        "    # 3. We pass those images through the discriminator\n",
        "    self.discriminator_outputs =  self.discriminator(self.generated_images)\n",
        "\n",
        "    # For the generator, the \"target\" on a fake input is a 1,\n",
        "    #  indicating that the discriminator classifies it as real,\n",
        "    #  even though the ground truth label is 0.\n",
        "    is_fake = torch.ones(batch_sz, 1).type_as(imgs)\n",
        "    g_loss = self.adversarial_loss(self.discriminator_outputs, is_fake)\n",
        "\n",
        "    # Return a dictionary of outputs for logging and automated backward pass by Lightning\n",
        "    output = {\"loss\": g_loss, \"g_loss\": g_loss, \"batch_idx\": batch_idx,\n",
        "              \"generator_win_percentage\": self.generator_win_percentage(self.discriminator_outputs, is_fake.int())}\n",
        "\n",
        "  if training_discriminator:\n",
        "\n",
        "    # 1. Obtain the discriminator outputs on real and fake images\n",
        "    outputs_on_real = self.discriminator(imgs)\n",
        "    outputs_on_fake = self.discriminator(self.generated_images.detach())\n",
        "\n",
        "    # For the discriminator, the \"target\" on a real input is a 1\n",
        "    targets_real = torch.ones(batch_sz, 1).type_as(imgs)\n",
        "    # and the \"target\" on a fake input is a 0\n",
        "    targets_fake = torch.zeros(batch_sz, 1).type_as(imgs)\n",
        "\n",
        "    # 2. Combine (concatenate) the outputs/targets in the two cases\n",
        "    outputs = torch.cat([outputs_on_real, outputs_on_fake])\n",
        "    targets = torch.cat([targets_real, targets_fake])\n",
        "\n",
        "    d_loss = self.adversarial_loss(outputs, targets)\n",
        "\n",
        "    # Return a dictionary of outputs for logging and automated backward pass by Lightning\n",
        "    output = {\"loss\": d_loss, \"d_loss\": d_loss, \"batch_idx\": batch_idx,\n",
        "              \"discriminator_win_percentage\": self.discriminator_win_percentage(outputs, targets.int())}\n",
        "\n",
        "  return output\n",
        "\n",
        "# add the training step code from above to the LitGAN class\n",
        "LitGAN.training_step = training_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEmKT0ogs1tL"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzRROdWIs2wc"
      },
      "source": [
        "To run training, execute the cell below.\n",
        "You can configure the network and training procedure\n",
        "by changing the values of the `config` dictionary.\n",
        "\n",
        "In between training runs,\n",
        "especially runs that crashed,\n",
        "you may wish to restart the notebook\n",
        "and re-run the preceding cells\n",
        "to get rid of accumulated state\n",
        "(`Runtime > Restart runtime`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYlW8fZ2zi41"
      },
      "source": [
        "###\n",
        "# Setup Hyperparameters, Data, and Model\n",
        "###\n",
        "\n",
        "\n",
        "config = {  # dictionary of configuration hyperparameters\n",
        "  \"batch_size\": 256,  # number of examples in a single batch\n",
        "  \"max_epochs\": 32,  # number of times to pass over the whole dataset\n",
        "  \"image_size\": (1, 28, 28),  # size of images in this dataset\n",
        "  \"latent_dim\": 128,  # size of input\n",
        "  \"loss\": torch.nn.BCELoss(),  # loss function for adversarial loss\n",
        "  \"activation\": torch.nn.ReLU,  # activation function class (instantiated later)\n",
        "  \"normalize\": True,  # whether to use BatchNorm in Generator\n",
        "  \"discriminator.optim\" : {\n",
        "    \"optimizer\": torch.optim.Adam,  # optimizer class (instantiated later)\n",
        "    \"optimizer.params\":  # dict of hyperparameters for optimizer\n",
        "      {\"lr\": 0.002,  # learning rate to scale gradients\n",
        "      \"betas\": (0.5, 0.999),  # momentum parameters\n",
        "      \"weight_decay\": 0}  # if non-zero, reduce weights each batch\n",
        "  },\n",
        "  \"generator.optim\" : {\n",
        "    \"optimizer\": torch.optim.Adam,  # optimizer class (instantiated later)\n",
        "    \"optimizer.params\":  # dict of hyperparameters for optimizer\n",
        "      {\"lr\": 0.0002,  # learning rate to scale gradients\n",
        "      \"betas\": (0.5, 0.999),  # momentum parameters\n",
        "      \"weight_decay\": 0}  # if non-zero, reduce weights each batch\n",
        "  }\n",
        "}\n",
        "\n",
        "config[\"log_interval\"] = max(int((50000 // config[\"batch_size\"]) / 10), 1)\n",
        "\n",
        "# 📸 set up the dataset of images\n",
        "dmodule = lu.datamodules.mnist.MNISTDataModule(\n",
        "    batch_size=config[\"batch_size\"])\n",
        "dmodule.prepare_data()\n",
        "dmodule.setup()\n",
        "\n",
        "# 🥅 instantiate the network\n",
        "gan = LitGAN(config)\n",
        "\n",
        "###\n",
        "# Train the model\n",
        "###\n",
        "\n",
        "\n",
        "with wandb.init(project=\"lit-gan\", entity=\"wandb\", config=config) as run:\n",
        "  # 👀 watch the gradients, log to Weights & Biases\n",
        "  wandb.watch(gan)\n",
        "\n",
        "  # 👟 configure Trainer \n",
        "  trainer = pl.Trainer(gpus=1,  # use the GPU for .forward\n",
        "                      logger=pl.loggers.WandbLogger(\n",
        "                        log_model=True, save_code=True),  # log to Weights & Biases\n",
        "                      max_epochs=config[\"max_epochs\"], log_every_n_steps=1,\n",
        "                      progress_bar_refresh_rate=50)\n",
        "                      \n",
        "  # 🏃‍♀️ run the Trainer on the model\n",
        "  trainer.fit(gan, dmodule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGWrjF1dF8Mh"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "The cell above will output links to Weights & Biases dashboards where you can review the training process and the final resulting model.\n",
        "\n",
        "These dashboards will be useful in working through the exercises below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBYvpY9LJK2R"
      },
      "source": [
        "#### 1. Balancing Act: Speed\n",
        "\n",
        "Though our eventual goal is for the generator to \"win\" the competition\n",
        "between the two networks,\n",
        "we need the discriminator to also learn effectively\n",
        "if the generator is to learn to make realistic images.\n",
        "But if the discriminator is too good,\n",
        "training can also fail.\n",
        "This balancing act makes GAN training difficult.\n",
        "\n",
        "Let's see this phenomenon in action.\n",
        "First, reduce the `lr` of the discriminator by a factor of at least 1000\n",
        "so that it learns much more\n",
        "slowly than the generator. What happens?\n",
        "Then, return the `lr` of the discriminator to its original value\n",
        "and decrease the `lr` of the generator\n",
        "by a factor of at least 100\n",
        "so that it learns much more\n",
        "slowly than the discriminator. What happens?\n",
        "\n",
        "Make sure to return the `lr`s to their original values\n",
        "(`0.002` for the discriminator and `0.0002` for the generator)\n",
        "before proceeding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQEhKxVIiChr"
      },
      "source": [
        "#### 2. Balancing Act: Capacity\n",
        "\n",
        "Another balancing act involves the _capacity_ or _expressivity_\n",
        "of the two networks --\n",
        "how powerful and flexible the networks are,\n",
        "or how much they can learn from the data.\n",
        "Loosely, we want as high capacity of a generator\n",
        "as we can comfortably execute and train,\n",
        "and we want a discriminator with sufficient capacity\n",
        "to prevent the generator from winning with a \"cheap trick\",\n",
        "like always returning the same input.\n",
        "But we don't want the discriminator to have\n",
        "such high capacity that it's impossible to fool\n",
        "(see the **Challenge** section for GAN training tricks\n",
        "that enable the use of higher-capacity discriminators).\n",
        "\n",
        "Capacity is hard to quantify and even harder to measure.\n",
        "As a first pass at the capacity,\n",
        "we just count the total number of parameters\n",
        "(mostly, the weights and biases of the linear layers)\n",
        "in each network.\n",
        "More parameters generally result in greater capacity.\n",
        "\n",
        "First, decrease the size, depth, and `latent_dim` of the generator \n",
        "until the results at the end of training drop in quality.\n",
        "\n",
        "Then, return to the default hyperparameters and \n",
        "increase the size and depth of the discriminator\n",
        "until the results at the end of training drop in quality.\n",
        "Compare the outputs\n",
        "(in qualitative terms and in terms of the image pixel statistics)\n",
        "and the training dynamics (e.g. win percentages)\n",
        "with those from the previous set of experiments.\n",
        "\n",
        "_Note_: the parameter counts are logged to W&B as\n",
        "`generator_nparams` and `discriminator_nparams`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwJcf96FGBvf"
      },
      "source": [
        "#### 3. Just Use More Compute\n",
        "\n",
        "In addition to GAN training being finicky,\n",
        "is notoriously difficult to tell when GAN training is finished.\n",
        "The goal is for the output images to fool a _human_ discriminator --\n",
        "to have a high \"perceptual quality\" --\n",
        "but we can't cheaply measure this,\n",
        "let alone backpropagate through the procedure.\n",
        "Values of the generator/discriminator loss have little meaning in terms\n",
        "of this perceptual quality.\n",
        "\n",
        "One common, if inelegant, solution, is to simply train for much longer\n",
        "than would otherwise seem reasonable.\n",
        "Increase the number of epochs (`config[\"max_epochs\"]`)\n",
        "and decrease the batch size\n",
        "until training takes at least 20 minutes --\n",
        "with the default hyperparameters and a batch size of `32`,\n",
        "this would be about `100` epochs.\n",
        "The iteration time should be roughly linear in the number of epochs.\n",
        "\n",
        "Does the \"perceptual quality\" of the images seem to increase?\n",
        "What happens to the generator and discriminator losses?\n",
        "Do they appear to converge?\n",
        "\n",
        "> _Note_: You can increase the runtime further if you'd like, but\n",
        "[Google Colab places limits on GPU usage](https://research.google.com/colaboratory/faq.html),\n",
        "so if you run for multiple hours, you may find your access temporarily curtailed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLNeL3CVHTDC"
      },
      "source": [
        "#### **Challenge**: Stability Tricks for GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sMOguDLXyFA"
      },
      "source": [
        "[Generative modeling is hard](https://wandb.ai/ayush-thakur/keras-gan/reports/Towards-Deep-Generative-Modeling-with-W-B--Vmlldzo4MDI4Mw),\n",
        "and GANs are notoriously a particularly challenging type of generative model.\n",
        "\n",
        "There are [many tricks for making GAN training easier](https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b).\n",
        "\n",
        "A few are listed below. Try implementing them!\n",
        "\n",
        "1. _One-Sided Label Smoothing_. In\n",
        "[label smoothing](https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06),\n",
        "we train networks on \"not-quite-one-hot\" vectors,\n",
        "whose entries are close to, but not quite, `0` and `1`.\n",
        "In GANs, this can prevent the discriminator from over-fitting\n",
        "to the current generator.\n",
        "Add this to the `training_step` by adjusting the values of the\n",
        "`target`s for the discriminator.\n",
        "2. _Noisy Inputs_. A clever discriminator could memorize\n",
        "every single digit in the dataset and prevent the generator\n",
        "from learning to generate new digits.\n",
        "One way around this is to add noise\n",
        "to both real and fake inputs so that the discriminator.\n",
        "Add this to the `training_step`.\n",
        "See if this allows you to use a bigger discriminator\n",
        "and generate better digits.\n",
        "\n",
        "_Hint_: Generate random tensors with\n",
        "`torch.rand*_like` methods, \n",
        "like [this one](https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch-randn_like),\n",
        "as in this snippet:\n",
        "```\n",
        "random_noise = torch.randn_like(??)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnszrMr54wvb"
      },
      "source": [
        "#### **Challenge**: Convolutional GANs\n",
        "\n",
        "As with classification and auto-encoding tasks,\n",
        "GANs for images benefit from the use of convolutional layers.\n",
        "Rework the discriminator and generator to use\n",
        "`torch.nn.Conv2d` and `torch.nn.ConvTranspose2d` layers,\n",
        "respectively.\n",
        "Check out the\n",
        "[MNIST Autoencoder notebook](https://colab.research.google.com/github/wandb/edu/blob/main/lightning/autoencoder/autoencoder-mnist.ipynb)\n",
        "for examples of convolutional layers in a generative model.\n",
        "\n",
        "_Hint_: max-pooling in the discriminator can make it too easy to fool.\n",
        "Use [strided convolutions](https://www.reddit.com/r/MachineLearning/comments/5x4jbt/d_strided_convolutions_vs_pooling_layers_pros_and/)\n",
        "instead.\n",
        "For more tips, see\n",
        "[this article](https://www.kdnuggets.com/2017/11/generative-adversarial-networks-part2.html)."
      ]
    }
  ]
}