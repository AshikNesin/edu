{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "<!--- @wandbcode{llmapps-generation} -->\n",
    "\n",
    "In this notebook we will dive deeper on prompting the model by passing a better context by using available data from users questions and using the documentation files to generate better answers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqqq openai tiktoken wandb tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import tiktoken\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")  \n",
    "import wandb\n",
    "from wandb.integration.openai import autolog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an OpenAI API key to run this notebook. You can get one [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enable W&B autologging to track our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/edu/llm-apps-course/wandb/run-20230530_141406-xr6f4zpu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/llmapps/runs/xr6f4zpu' target=\"_blank\">likely-violet-3</a></strong> to <a href='https://wandb.ai/capecape/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/llmapps' target=\"_blank\">https://wandb.ai/capecape/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/llmapps/runs/xr6f4zpu' target=\"_blank\">https://wandb.ai/capecape/llmapps/runs/xr6f4zpu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start logging to W&B\n",
    "autolog({\"project\":\"llmapps\", \"job_type\": \"generation\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic support questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will add a retry behavior in case we hit the API rate limit\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"Hello, as a W&B user, I am having trouble understanding how to properly use the W&B Dashboard for visualizing my experimentation logs. Can you provide some guidance and best practices?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"Hello, as a W&B user, I am having difficulty figuring out how to visualize my model's training progress using the dashboard. Can you please guide me through the process or provide some resources to assist me in setting up this feature? Thank you!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"Hello, I'm a W&B user and I've recently encountered an issue while trying to log my training data. Can you please guide me through the steps to ensure I'm doing it correctly? Thanks!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\"Hello, I am a W&B user, and I am having difficulty in setting up my project. Could you guide me on how to correctly configure the W&B integration with my machine learning code to track experiments and visualize results? Thanks!\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hi, I'm a user of W&B and I have a question about integrating it with my PyTorch project. Can you please guide me on how to implement W&B logging in my training and validation processes? Thank you!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Generate a support question from a W&B user\"\n",
    "\n",
    "def generate_and_print(system_prompt, user_prompt):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    responses = completion_with_backoff(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        n = 5,\n",
    "        )\n",
    "    for response in responses.choices:\n",
    "        generation = response.message.content\n",
    "        display(Markdown(generation))\n",
    "    \n",
    "generate_and_print(system_prompt, user_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read some user submitted queries from the file `examples.txt`. This file contains multiline questions separated by tabs (`\\t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We have 228 real queries:'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sample one: \n",
       "\"How do I perform large rearrangements in web ui table efficiently? Dragging columns to the wanted positions one-by-one is very slow.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiter = \"\\t\" # tab separated queries\n",
    "with open(\"examples.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    real_queries = data.split(delimiter)\n",
    "\n",
    "pprint(f\"We have {len(real_queries)} real queries:\")  \n",
    "Markdown(f\"Sample one: \\n\\\"{random.choice(real_queries)}\\\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use those real user questions to guide our model to produce synthetic questions like those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Generate a support question from a W&B user\n",
       "Below you will find a few examples of real user queries:\n",
       "Is there any way to name different experiments within the same run? W&B process takes some time to init and to finish, so I am losing efficiency in the experiment tracking\n",
       "What are the risk of supplying proprietary data into a third party cloud provider such as weights and biases?\n",
       "how can i do .tight_layout() on my seaborn heatmap\n",
       "Let's start!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_few_shot_prompt(queries, n=3):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"Below you will find a few examples of real user queries:\\n\"\n",
    "    for _ in range(n):\n",
    "        prompt += random.choice(queries) + \"\\n\"\n",
    "    prompt += \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "generation_prompt = generate_few_shot_prompt(real_queries)\n",
    "Markdown(generation_prompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI `Chat` models are really good at following instructions with a few examples. Let's see how it does here. This is going to use some context from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Hello, I'm using W&B for my experiments, and I encountered an issue \"\n",
      " 'regarding authentication on a cloud service. Our team has a team account, '\n",
      " 'and we use a client application to initiate the training on the cloud '\n",
      " \"server. However, the cloud server requires authentication. I'm wondering if \"\n",
      " 'each user has to authenticate personally, or is there a way to use our team '\n",
      " 'account to enable authentication for all users, even if they are not logged '\n",
      " 'in to the server machines? Thanks for your help!')\n",
      "(\"Hello! I've been using W&B to track my experiments, and I'm running multiple \"\n",
      " \"training jobs simultaneously. However, I'm not sure how to properly organize \"\n",
      " 'the runs and visualize their results in the dashboard. Is there an efficient '\n",
      " 'way to group related runs and filter them easily on the W&B interface?')\n",
      "('Hey there, I recently started using W&B for my machine learning experiments, '\n",
      " 'and I have a question regarding runs and authentication. When I am training '\n",
      " 'my model on a cloud service with a team account, how do I ensure that the '\n",
      " 'wandb library is authenticated for each run without requiring individual '\n",
      " 'users to personally authenticate? Is there any efficient way to handle this '\n",
      " 'for a team training their models on a cloud platform? Thanks in advance for '\n",
      " 'your help!')\n",
      "(\"Hello, I'm using W&B for my training experiments, and I have a team account. \"\n",
      " 'We typically execute our training code on cloud servers, while each team '\n",
      " 'member only interacts with the training using a client application. I need '\n",
      " 'to understand how to authenticate with W&B when performing training on our '\n",
      " 'cloud service. Is it mandatory for every user to authenticate individually, '\n",
      " 'or is there a way to authenticate the server so that all team members can '\n",
      " 'access the training without individually logging in?')\n",
      "('Hello, I am training my model using W&B and I have started to notice that '\n",
      " 'the runs are not being displayed as separate entries on my dashboard. '\n",
      " 'Instead, they are being combined into a single run. How can I ensure that '\n",
      " 'each run is displayed separately on my dashboard? Should I be using a '\n",
      " 'different method to authenticate or set up my runs? Thanks!')\n"
     ]
    }
   ],
   "source": [
    "generate_and_print(system_prompt, user_prompt=generation_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Context & Response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to find all the markdown files in a directory and return it's content and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_md_files(directory):\n",
    "    \"Find all markdown files in a directory and return their content and path\"\n",
    "    md_files = []\n",
    "    for file in Path(directory).rglob(\"*.md\"):\n",
    "        with open(file, 'r', encoding='utf-8') as md_file:\n",
    "            content = md_file.read()\n",
    "        md_files.append((file.relative_to(directory), content))\n",
    "    return md_files\n",
    "\n",
    "documents = find_md_files('docs_sample/')\n",
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the documents are not too long for our context window. We need to compute the number of tokens in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4179, 365, 1206, 2596, 2940, 537, 956, 803, 1644, 2529, 2093]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(MODEL)\n",
    "tokens_per_document = [len(tokenizer.encode(document)) for _, document in documents]\n",
    "pprint(tokens_per_document)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are too long - instead of using entire documents, we'll extract a random chunk from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a random chunk from a document\n",
    "def extract_random_chunk(document, max_tokens=512):\n",
    "    tokens = tokenizer.encode(document)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return document\n",
    "    start = random.randint(0, len(tokens) - max_tokens)\n",
    "    end = start + max_tokens\n",
    "    return tokenizer.decode(tokens[start:end])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use that extracted chunk to create a question that can be answered by the document. This way we can generate questions that our current documentation is capable of answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"The question should be answerable by provided fragment of W&B documentation.\\n\" +\\\n",
    "        \"Below you will find a fragment of W&B documentation:\\n\" +\\\n",
    "        chunk + \"\\n\" +\\\n",
    "        \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "chunk = extract_random_chunk(documents[0][1])\n",
    "generation_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Generate a support question from a W&B user\\n'\n",
      " 'The question should be answerable by provided fragment of W&B '\n",
      " 'documentation.\\n'\n",
      " 'Below you will find a fragment of W&B documentation:\\n'\n",
      " ' import WandbLogger\\n'\n",
      " 'from pytorch_lightning import Trainer\\n'\n",
      " '\\n'\n",
      " 'wandb_logger = WandbLogger()\\n'\n",
      " 'trainer = Trainer(logger=wandb_logger)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '![Interactive dashboards accessible anywhere, and '\n",
      " 'more!](@site/static/images/integrations/n6P7K4M.gif)\\n'\n",
      " '\\n'\n",
      " '## Sign up and Log in to wandb\\n'\n",
      " '\\n'\n",
      " 'a) [**Sign up**](https://wandb.ai/site) for a free account\\n'\n",
      " '\\n'\n",
      " 'b) Pip install the `wandb` library\\n'\n",
      " '\\n'\n",
      " \"c) To login in your training script, you'll need to be signed in to you \"\n",
      " 'account at www.wandb.ai, then **you will find your API key on the** '\n",
      " '[**Authorize page**](https://wandb.ai/authorize)**.**\\n'\n",
      " '\\n'\n",
      " 'If you are using Weights and Biases for the first time you might want to '\n",
      " 'check out our [**quickstart**](../../quickstart.md)****\\n'\n",
      " '\\n'\n",
      " '<Tabs\\n'\n",
      " '  defaultValue=\"cli\"\\n'\n",
      " '  values={[\\n'\n",
      " \"    {label: 'Command Line', value: 'cli'},\\n\"\n",
      " \"    {label: 'Notebook', value: 'notebook'},\\n\"\n",
      " '  ]}>\\n'\n",
      " '  <TabItem value=\"cli\">\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'pip install wandb\\n'\n",
      " '\\n'\n",
      " 'wandb login\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '</TabItem>\\n'\n",
      " '  <TabItem value=\"notebook\">\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " '!pip install wandb\\n'\n",
      " '\\n'\n",
      " 'import wandb\\n'\n",
      " 'wandb.login()\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '  </TabItem>\\n'\n",
      " '</Tabs>\\n'\n",
      " '\\n'\n",
      " \"## Using PyTorch Lightning's `WandbLogger`\\n\"\n",
      " '\\n'\n",
      " 'PyTorch Lightning has a '\n",
      " '[**`WandbLogger`**](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch\\\\_lightning.loggers.WandbLogger.html?highlight=wandblogger) '\n",
      " 'class that can be used to seamlessly log metrics, model weights, media and '\n",
      " \"more. Just instantiate the WandbLogger and pass it to Lightning's \"\n",
      " '`Trainer`.\\n'\n",
      " '\\n'\n",
      " '```\\n'\n",
      " 'wandb_logger = WandbLogger()\\n'\n",
      " 'trainer = Trainer(logger=wandb_logger)\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Logger arguments\\n'\n",
      " '\\n'\n",
      " 'Below are some of the most used parameters in WandbLogger, see the PyTorch '\n",
      " 'Lightning [**`WandbLogger` '\n",
      " 'documentation**](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch\\\\_lightning.loggers.WandbLogger.html?highlight=wandblogger) '\n",
      " 'for a full list and description\\n'\n",
      " '\\n'\n",
      " '| Parameter   | '\n",
      " 'Description                                                                   '\n",
      " '|\\n'\n",
      " '| ----------- | '\n",
      " '----------------------------------------------------------------------------- '\n",
      " '|\\n'\n",
      " '| `project`   | Define what wandb Project to log '\n",
      " 'to                                           |\\n'\n",
      " '| `name`     \\n'\n",
      " \"Let's start!\")\n"
     ]
    }
   ],
   "source": [
    "pprint(generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: How do I use the WandbLogger in PyTorch Lightning to log metrics, '\n",
      " 'model weights, and media?\\n'\n",
      " '\\n'\n",
      " 'Fragment from W&B Documentation:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'import WandbLogger\\n'\n",
      " 'from pytorch_lightning import Trainer\\n'\n",
      " '\\n'\n",
      " 'wandb_logger = WandbLogger()\\n'\n",
      " 'trainer = Trainer(logger=wandb_logger)\\n'\n",
      " '```')\n",
      "(\"Question: How can I use the WandbLogger with PyTorch Lightning's Trainer to \"\n",
      " 'log my training metrics and model weights with Weights & Biases?')\n",
      "(\"Question: How can I use the WandbLogger with PyTorch Lightning's Trainer to \"\n",
      " 'log metrics and model weights?\\n'\n",
      " '\\n'\n",
      " 'Documentation Fragment:\\n'\n",
      " '\\n'\n",
      " '```\\n'\n",
      " 'import WandbLogger\\n'\n",
      " 'from pytorch_lightning import Trainer\\n'\n",
      " '\\n'\n",
      " 'wandb_logger = WandbLogger()\\n'\n",
      " 'trainer = Trainer(logger=wandb_logger)\\n'\n",
      " '```')\n",
      "('Question: How do I use W&B with PyTorch Lightning, and where can I find my '\n",
      " 'API key?')\n",
      "('Question: How can I use the WandbLogger class from PyTorch Lightning to log '\n",
      " 'metrics, model weights, and media in my project?')\n"
     ]
    }
   ],
   "source": [
    "generate_and_print(system_prompt, generation_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 5 prompt\n",
    "\n",
    "Complex directive that includes the following:\n",
    "- Description of high-level goal\n",
    "- A detailed bulleted list of sub-tasks\n",
    "- An explicit statement asking LLM to explain its own output\n",
    "- A guideline on how LLM output will be evaluated\n",
    "- Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prompt_template.txt file into an f-string\n",
    "with open(\"prompt_template.txt\", \"r\") as file:\n",
    "    prompt_template = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk, n_questions=3):\n",
    "    questions = '\\n'.join(random.sample(real_queries, n_questions))\n",
    "    prompt = prompt_template.format(QUESTIONS=questions, CHUNK=chunk)\n",
    "    return prompt\n",
    "\n",
    "generation_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are a creative assistant with the goal to generate a synthetic dataset '\n",
      " 'of Weights & Biases (W&B) user questions.\\n'\n",
      " \"W&B users are asking these questions to a bot, so they don't know the answer \"\n",
      " \"and their questions are grounded in what they're trying to achieve. \\n\"\n",
      " 'We are interested in questions that can be answered by W&B documentation. \\n'\n",
      " \"But the users don't have access to this documentation, so you need to \"\n",
      " \"imagine what they're trying to do and use according language. \\n\"\n",
      " 'Here are some examples of real user questions, you will be judged by how '\n",
      " 'well you match this distribution.\\n'\n",
      " '***\\n'\n",
      " 'when I try ```api = wandb.Api()\\n'\n",
      " 'project = api.project(\"<entity>/<project_name>\")\\n'\n",
      " \"sweeps = project.sweeps()) ``` it gives me ``` 'NoneType' object is not \"\n",
      " 'subscriptable.``` Do you know why?\\n'\n",
      " 'how do you log a dataset?\\n'\n",
      " 'how do i prevent my wandb.Image from cutting off the ticklabels?\\n'\n",
      " '***\\n'\n",
      " 'In the next step, you will read a fragment of W&B documentation.\\n'\n",
      " 'This will serve as inspiration for synthetic user question and the source of '\n",
      " 'the answer. \\n'\n",
      " 'Here is the document fragment:\\n'\n",
      " '***\\n'\n",
      " \" steps are similar'''\\n\"\n",
      " '        x, y = batch\\n'\n",
      " '        logits = self(x)\\n'\n",
      " '        preds = torch.argmax(logits, dim=1)\\n'\n",
      " '        loss = self.loss(logits, y)\\n'\n",
      " '        acc = accuracy(preds, y)\\n'\n",
      " '        return preds, loss, acc\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Log the min/max of your metric\\n'\n",
      " '\\n'\n",
      " \"Using wandb's \"\n",
      " '[`define_metric`](https://docs.wandb.ai/ref/python/run#define\\\\_metric) '\n",
      " \"function you can define whether you'd like your W&B summary metric to \"\n",
      " 'display the min, max, mean or best value for that metric. If '\n",
      " \"`define`_`metric` _ isn't used, then the last value logged with appear in \"\n",
      " 'your summary metrics. See the `define_metric` [reference docs '\n",
      " 'here](https://docs.wandb.ai/ref/python/run#define\\\\_metric) and the [guide '\n",
      " 'here](https://docs.wandb.ai/guides/track/log#customize-axes-and-summaries-with-define\\\\_metric) '\n",
      " 'for more.\\n'\n",
      " '\\n'\n",
      " 'To tell W&B to keep track of the max validation accuracy in the W&B summary '\n",
      " 'metric, you just need to call `wandb.define_metric` once, e.g. you can call '\n",
      " 'it at the beginning of training like so:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'class My_LitModule(LightningModule):\\n'\n",
      " '    ...\\n'\n",
      " '    \\n'\n",
      " '    def validation_step(self, batch, batch_idx):\\n'\n",
      " '        if trainer.global_step == 0: \\n'\n",
      " \"            wandb.define_metric('val_accuracy', summary='max')\\n\"\n",
      " '        \\n'\n",
      " '        preds, loss, acc = self._get_preds_loss_accuracy(batch)\\n'\n",
      " '\\n'\n",
      " '        # Log loss and metric\\n'\n",
      " \"        self.log('val_loss', loss)\\n\"\n",
      " \"        self.log('val_accuracy', acc)\\n\"\n",
      " '        return preds\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Model Checkpointing\\n'\n",
      " '\\n'\n",
      " 'Custom checkpointing to W&B can be set up through the PyTorch Lightning '\n",
      " '[`ModelCheckpoint`](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch\\\\_lightning.callbacks.ModelCheckpoint.html#pytorch\\\\_lightning.callbacks.ModelCheckpoint) '\n",
      " 'when the log\\\\_model argument is used in the `WandbLogger`:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " '# log model only if `val_accuracy` increases\\n'\n",
      " 'wandb_logger = WandbLogger(log_model=\"all\")\\n'\n",
      " 'checkpoint_callback = ModelCheckpoint(monitor=\"val_accuracy\", mode=\"max\")\\n'\n",
      " 'trainer = Trainer(logger=wandb_logger, callbacks=[checkpoint_callback])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'The _latest_ and _best_ aliases are automatically set to easily retrieve a '\n",
      " 'model checkpoint from W&B Artifacts:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " '# reference can be retrieved in artifacts\\n'\n",
      " '***\\n'\n",
      " 'You will now generate a user question and corresponding answer based on the '\n",
      " 'above document. \\n'\n",
      " 'First, explain the user context and what problems they might be trying to '\n",
      " 'solve. \\n'\n",
      " 'Second, generate user question. \\n'\n",
      " 'Third, provide the accurate and concise answer in markdown format to the '\n",
      " 'user question using the documentation. \\n'\n",
      " \"You'll be evaluated on:\\n\"\n",
      " '- how realistic is that this question will come from a real user one day? \\n'\n",
      " '- is this question about W&B? \\n'\n",
      " '- can the question be answered using the W&B document fragment above? \\n'\n",
      " '- how accurate is the answer?\\n'\n",
      " 'Remember that users have different styles and can be imprecise. You are very '\n",
      " 'good at impersonating them!\\n'\n",
      " 'Use the following format:\\n'\n",
      " 'CONTEXT: \\n'\n",
      " 'QUESTION: \\n'\n",
      " 'ANSWER: \\n'\n",
      " \"Let's start!\")\n"
     ]
    }
   ],
   "source": [
    "pprint(generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(documents, n_questions=3, n_generations=5):\n",
    "    questions = []\n",
    "    for _, document in documents:\n",
    "        chunk = extract_random_chunk(document)\n",
    "        prompt = generate_context_prompt(chunk, n_questions)\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        response = completion_with_backoff(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            n = n_generations,\n",
    "            )\n",
    "        questions.extend([response.choices[i].message.content for i in range(n_generations)])\n",
    "    return questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Note about the `system` role: For GPT4 based pipelines you probably want to move some part of the context prompt to the `system` context. As we are using `gpt3.5-turbo` here, you can put the instruction on the user prompt, you can read more about this on [OpenAI docs here](https://platform.openai.com/docs/guides/chat/instructing-chat-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse model generation and extract CONTEXT, QUESTION and ANSWER\n",
    "def parse_generation(generation):\n",
    "    lines = generation.split(\"\\n\")\n",
    "    context = []\n",
    "    question = []\n",
    "    answer = []\n",
    "    flag = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"CONTEXT:\" in line:\n",
    "            flag = \"context\"\n",
    "            line = line.replace(\"CONTEXT:\", \"\").strip()\n",
    "        elif \"QUESTION:\" in line:\n",
    "            flag = \"question\"\n",
    "            line = line.replace(\"QUESTION:\", \"\").strip()\n",
    "        elif \"ANSWER:\" in line:\n",
    "            flag = \"answer\"\n",
    "            line = line.replace(\"ANSWER:\", \"\").strip()\n",
    "\n",
    "        if flag == \"context\":\n",
    "            context.append(line)\n",
    "        elif flag == \"question\":\n",
    "            question.append(line)\n",
    "        elif flag == \"answer\":\n",
    "            answer.append(line)\n",
    "\n",
    "    context = \"\\n\".join(context)\n",
    "    question = \"\\n\".join(question)\n",
    "    answer = \"\\n\".join(answer)\n",
    "    return context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"A user is trying to log their model metrics and hyperparameters using W&B. They have read the documentation and know that they can log their metrics when using `WandbLogger` by calling `self.log('my_metric_name', metric_vale)`. However, they are unsure about how to use the `self.log()` method within their code to log their metrics and hyperparameters.\\n\",\n",
       " \"Hi there! I'm new to W&B and PyTorch Lightning, and I'm trying to use the `WandbLogger` to log my model metrics and hyperparameters during training. I read the documentation on how to log metrics, but I'm still a bit confused on how to use the `self.log()` method in my code. Can you help me out?\\n\",\n",
       " 'Absolutely! In order to use the `self.log()` method to log your model metrics and hyperparameters during training with `WandbLogger`, first you need to define your `LightningModule` to log your metrics and hyperparameters as shown in the documentation. Then, you can call `self.log(\\'my_metric_name\\', metric_value)` within your `LightningModule`, such as in your `training_step` or `validation_step` methods to log your metrics. Similarly, you can log your hyperparameters using `wandb.config[\"key\"] = value` or `wandb.config.update()` statements. Feel free to let me know if you have any further questions!')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = generate_questions([documents[0]], n_questions=3, n_generations=5)\n",
    "parse_generation(generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_generations = []\n",
    "generations = generate_questions(documents, n_questions=3, n_generations=5)\n",
    "for generation in generations:\n",
    "    context, question, answer = parse_generation(generation)\n",
    "    parsed_generations.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "# log df as a table to W&B\n",
    "wandb.log({\"generated_examples\": wandb.Table(dataframe=pd.DataFrame(parsed_generations))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-puddle-6</strong> at: <a href='https://wandb.ai/darek/llmapps/runs/mfpc8ody' target=\"_blank\">https://wandb.ai/darek/llmapps/runs/mfpc8ody</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230524_233134-mfpc8ody/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHow can I share my report with my team members in a public project?\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_generations[8]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " 'To share a report with your team members in a public project, select the '\n",
      " 'Share button on the upper right hand corner of the report. You can either '\n",
      " 'provide an email account or copy the magic link. Users invited by email will '\n",
      " 'need to log into Weights & Biases to view the report. Users who are given a '\n",
      " 'magic link do not need to log in to Weights & Biases to view the report. '\n",
      " \"It's important to note that shared reports are view-only, and only the \"\n",
      " 'administrator or the member who created the report can toggle permissions '\n",
      " 'between edit or view access for other team members.')\n"
     ]
    }
   ],
   "source": [
    "pprint(parsed_generations[8]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
