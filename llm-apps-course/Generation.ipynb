{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/llm-apps-course/Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{llmapps-generation} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "<!--- @wandbcode{llmapps-generation} -->\n",
    "\n",
    "In this notebook we will dive deeper on prompting the model by passing a better context by using available data from users questions and using the documentation files to generate better answers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqqq rich openai tiktoken wandb tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import tiktoken\n",
    "from pprint import pprint\n",
    "from rich.markdown import Markdown\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")  \n",
    "import wandb\n",
    "from wandb.integration.openai import autolog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an OpenAI API key to run this notebook. You can get one [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key configured\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enable W&B autologging to track our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/darek/Projects/edu/llm-apps-course/wandb/run-20230601_134554-6478i8zh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/darek/llmapps/runs/6478i8zh' target=\"_blank\">earnest-bush-30</a></strong> to <a href='https://wandb.ai/darek/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/darek/llmapps' target=\"_blank\">https://wandb.ai/darek/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/darek/llmapps/runs/6478i8zh' target=\"_blank\">https://wandb.ai/darek/llmapps/runs/6478i8zh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start logging to W&B\n",
    "autolog({\"project\":\"llmapps\", \"job_type\": \"generation\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic support questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a retry behavior in case we hit the API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"How do I track the performance of different models over time on W&amp;B?\"                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"How do I track the performance of different models over time on W&B?\"                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How do I connect and monitor my machine learning experiments in W&amp;B?                                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How do I connect and monitor my machine learning experiments in W&B?                                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"How do I save my project to a specific directory in W&amp;B?\"                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"How do I save my project to a specific directory in W&B?\"                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"How do I track multiple experiments within a single W&amp;B project?\"                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"How do I track multiple experiments within a single W&B project?\"                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"How do I share my W&amp;B project with my team members and give them the ability to collaborate and make changes?\"    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"How do I share my W&B project with my team members and give them the ability to collaborate and make changes?\"    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Generate a support question from a W&B user\"\n",
    "\n",
    "def generate_and_print(system_prompt, user_prompt, n=5):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    responses = completion_with_backoff(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n = n,\n",
    "        )\n",
    "    for response in responses.choices:\n",
    "        generation = response.message.content\n",
    "        display(Markdown(generation))\n",
    "    \n",
    "generate_and_print(system_prompt, user_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read some user submitted queries from the file `examples.txt`. This file contains multiline questions separated by tabs (`\\t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We have 228 real queries:'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample one: \"What are the risk of supplying proprietary data into a third party cloud provider such as weights and \n",
       "biases?\"                                                                                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sample one: \"What are the risk of supplying proprietary data into a third party cloud provider such as weights and \n",
       "biases?\"                                                                                                           \n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiter = \"\\t\" # tab separated queries\n",
    "with open(\"examples.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    real_queries = data.split(delimiter)\n",
    "\n",
    "pprint(f\"We have {len(real_queries)} real queries:\")  \n",
    "Markdown(f\"Sample one: \\n\\\"{random.choice(real_queries)}\\\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use those real user questions to guide our model to produce synthetic questions like those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user Below you will find a few examples of real user queries: what is a     \n",
       "weave query expression that can calculate the median of a column of numbers in a wandb Table can I use the column  \n",
       "subsample parameter config file for xgboost parameters through sweeps? I'm getting a \"Importing a module script    \n",
       "failed. An application error occurred.\" error on a computer, but the same link works on my phone. Let's start!     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate a support question from a W&B user Below you will find a few examples of real user queries: what is a     \n",
       "weave query expression that can calculate the median of a column of numbers in a wandb Table can I use the column  \n",
       "subsample parameter config file for xgboost parameters through sweeps? I'm getting a \"Importing a module script    \n",
       "failed. An application error occurred.\" error on a computer, but the same link works on my phone. Let's start!     \n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_few_shot_prompt(queries, n=3):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"Below you will find a few examples of real user queries:\\n\"\n",
    "    for _ in range(n):\n",
    "        prompt += random.choice(queries) + \"\\n\"\n",
    "    prompt += \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "generation_prompt = generate_few_shot_prompt(real_queries)\n",
    "Markdown(generation_prompt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI `Chat` models are really good at following instructions with a few examples. Let's see how it does here. This is going to use some context from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I use W&amp;B to track and analyze the performance of my deep learning model?                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How can I use W&B to track and analyze the performance of my deep learning model?                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How do I visualize the distribution of a specific column in my W&amp;B Table using histograms or density plots?        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How do I visualize the distribution of a specific column in my W&B Table using histograms or density plots?        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sure, here's a support question based on these examples:                                                           \n",
       "\n",
       "How can I visualize the ROC curve for my model in W&amp;B? What is the format of the input required for the <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">roc_curve</span>  \n",
       "function in W&amp;B?                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sure, here's a support question based on these examples:                                                           \n",
       "\n",
       "How can I visualize the ROC curve for my model in W&B? What is the format of the input required for the \u001b[97;40mroc_curve\u001b[0m  \n",
       "function in W&B?                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I use W&amp;B to track and visualize model performance over time when training on multiple datasets with       \n",
       "different labels?                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How can I use W&B to track and visualize model performance over time when training on multiple datasets with       \n",
       "different labels?                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I plot a graph in WandB that shows the trend of validation and training loss during training?              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How can I plot a graph in WandB that shows the trend of validation and training loss during training?              \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_print(system_prompt, user_prompt=generation_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Context & Response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to find all the markdown files in a directory and return it's content and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_md_files(directory):\n",
    "    \"Find all markdown files in a directory and return their content and path\"\n",
    "    md_files = []\n",
    "    for file in Path(directory).rglob(\"*.md\"):\n",
    "        with open(file, 'r', encoding='utf-8') as md_file:\n",
    "            content = md_file.read()\n",
    "        md_files.append((file.relative_to(directory), content))\n",
    "    return md_files\n",
    "\n",
    "documents = find_md_files('docs_sample/')\n",
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the documents are not too long for our context window. We need to compute the number of tokens in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4179, 365, 1206, 2596, 2940, 537, 956, 803, 1644, 2529, 2093]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "tokens_per_document = [len(tokenizer.encode(document)) for _, document in documents]\n",
    "pprint(tokens_per_document)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are too long - instead of using entire documents, we'll extract a random chunk from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a random chunk from a document\n",
    "def extract_random_chunk(document, max_tokens=512):\n",
    "    tokens = tokenizer.encode(document)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return document\n",
    "    start = random.randint(0, len(tokens) - max_tokens)\n",
    "    end = start + max_tokens\n",
    "    return tokenizer.decode(tokens[start:end])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use that extracted chunk to create a question that can be answered by the document. This way we can generate questions that our current documentation is capable of answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"The question should be answerable by provided fragment of W&B documentation.\\n\" +\\\n",
    "        \"Below you will find a fragment of W&B documentation:\\n\" +\\\n",
    "        chunk + \"\\n\" +\\\n",
    "        \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "chunk = extract_random_chunk(documents[0][1])\n",
    "generation_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user The question should be answerable by provided fragment of W&amp;B          \n",
       "documentation. Below you will find a fragment of W&amp;B documentation: 2 = Linear(n_layer_1, n_layer_2) self.layer_3 =\n",
       "Linear(n_layer_2, n_classes)                                                                                       \n",
       "\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.loss = CrossEntropyLoss()</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.lr = lr</span><span style=\"background-color: #272822\">                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # save hyper-parameters to self.hparams (auto-logged by W&amp;B)</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.save_hyperparameters()</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def forward(self, x):</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''method used for inference input -&gt; output'''</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"background-color: #272822\">                                                                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # (b, 1, 28, 28) -&gt; (b, 1*28*28)</span><span style=\"background-color: #272822\">                                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    batch_size, channels, width, height = x.size()</span><span style=\"background-color: #272822\">                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = x.view(batch_size, -1)</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # let's do 3 x (linear + relu)</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = F.relu(self.layer_1(x))</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = F.relu(self.layer_2(x))</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = self.layer_3(x)</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return x</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def training_step(self, batch, batch_idx):</span><span style=\"background-color: #272822\">                                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''needs to return a loss from a single batch'''</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    _, loss, acc = self._get_preds_loss_accuracy(batch)</span><span style=\"background-color: #272822\">                                                            </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # Log loss and metric</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('train_loss', loss)</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('train_accuracy', acc)</span><span style=\"background-color: #272822\">                                                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return loss</span><span style=\"background-color: #272822\">                                                                                                    </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def validation_step(self, batch, batch_idx):</span><span style=\"background-color: #272822\">                                                                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''used for logging metrics'''</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    preds, loss, acc = self._get_preds_loss_accuracy(batch)</span><span style=\"background-color: #272822\">                                                        </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # Log loss and metric</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('val_loss', loss)</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('val_accuracy', acc)</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return preds</span><span style=\"background-color: #272822\">                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def configure_optimizers(self):</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''defines model optimizer'''</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return Adam(self.parameters(), lr=self.lr)</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def _get_preds_loss_accuracy(self, batch):</span><span style=\"background-color: #272822\">                                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''convenience function since train/valid/test steps are similar'''</span><span style=\"background-color: #272822\">                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x, y = batch</span><span style=\"background-color: #272822\">                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    logits = self(x)</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    preds = torch.argmax(logits, dim=1)</span><span style=\"background-color: #272822\">                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    loss = self.loss(logits, y)</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    acc = accuracy(preds, y)</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return preds, loss, acc</span><span style=\"background-color: #272822\">                                                                                        </span>\n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">### Log the min/max of your metric</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Using wandb's [`define_metric`](https://docs.wandb.ai/ref/python/run#define\\_metric) function you can define </span><span style=\"background-color: #272822\">      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">whether you'd like your W&amp;B summary metric to display the min, max, mean or best value for that metric. If </span><span style=\"background-color: #272822\">        </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">`define`_`metric` _ isn't used, then the last value logged with appear in your summary metrics. See the </span><span style=\"background-color: #272822\">           </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">`define_metric` [reference docs here](https://docs.wandb</span><span style=\"background-color: #272822\">                                                           </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Let's start!</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate a support question from a W&B user The question should be answerable by provided fragment of W&B          \n",
       "documentation. Below you will find a fragment of W&B documentation: 2 = Linear(n_layer_1, n_layer_2) self.layer_3 =\n",
       "Linear(n_layer_2, n_classes)                                                                                       \n",
       "\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.loss = CrossEntropyLoss()\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.lr = lr\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # save hyper-parameters to self.hparams (auto-logged by W&B)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.save_hyperparameters()\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef forward(self, x):\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''method used for inference input -> output'''\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m                                                                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # (b, 1, 28, 28) -> (b, 1*28*28)\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    batch_size, channels, width, height = x.size()\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = x.view(batch_size, -1)\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # let's do 3 x (linear + relu)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = F.relu(self.layer_1(x))\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = F.relu(self.layer_2(x))\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = self.layer_3(x)\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return x\u001b[0m\u001b[48;2;39;40;34m                                                                                                       \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef training_step(self, batch, batch_idx):\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''needs to return a loss from a single batch'''\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    _, loss, acc = self._get_preds_loss_accuracy(batch)\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # Log loss and metric\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('train_loss', loss)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('train_accuracy', acc)\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return loss\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef validation_step(self, batch, batch_idx):\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''used for logging metrics'''\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    preds, loss, acc = self._get_preds_loss_accuracy(batch)\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # Log loss and metric\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('val_loss', loss)\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('val_accuracy', acc)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return preds\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef configure_optimizers(self):\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''defines model optimizer'''\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return Adam(self.parameters(), lr=self.lr)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef _get_preds_loss_accuracy(self, batch):\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''convenience function since train/valid/test steps are similar'''\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x, y = batch\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    logits = self(x)\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    preds = torch.argmax(logits, dim=1)\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    loss = self.loss(logits, y)\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    acc = accuracy(preds, y)\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return preds, loss, acc\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m\n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m### Log the min/max of your metric\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mUsing wandb's [`define_metric`](https://docs.wandb.ai/ref/python/run#define\\_metric) function you can define \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mwhether you'd like your W&B summary metric to display the min, max, mean or best value for that metric. If \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m`define`_`metric` _ isn't used, then the last value logged with appear in your summary metrics. See the \u001b[0m\u001b[48;2;39;40;34m           \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m`define_metric` [reference docs here](https://docs.wandb\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mLet's start!\u001b[0m\u001b[48;2;39;40;34m                                                                                                       \u001b[0m\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(generation_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 3 possible questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the purpose of the <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">define_metric</span> function in W&amp;B, and how can it be used to control the display of summary \n",
       "metrics such as min, max, mean and best value?                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the purpose of the \u001b[97;40mdefine_metric\u001b[0m function in W&B, and how can it be used to control the display of summary \n",
       "metrics such as min, max, mean and best value?                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the purpose of the <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">_get_preds_loss_accuracy</span> function in this W&amp;B documentation fragment and how is it used \n",
       "in the <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">training_step</span> and <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">validation_step</span> methods?                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "What is the purpose of the \u001b[97;40m_get_preds_loss_accuracy\u001b[0m function in this W&B documentation fragment and how is it used \n",
       "in the \u001b[97;40mtraining_step\u001b[0m and \u001b[97;40mvalidation_step\u001b[0m methods?                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What method should I use in my WandB project to log the min and max values of a specific metric during training?   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "What method should I use in my WandB project to log the min and max values of a specific metric during training?   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_print(system_prompt, generation_prompt, n=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, sometimes the generation contains an intro phrase like: \"Sure, here's a support question based on the documentation:\", we may want to put some instructions to avoid this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 5 prompt\n",
    "\n",
    "Complex directive that includes the following:\n",
    "- Description of high-level goal\n",
    "- A detailed bulleted list of sub-tasks\n",
    "- An explicit statement asking LLM to explain its own output\n",
    "- A guideline on how LLM output will be evaluated\n",
    "- Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use GPT4 from here, as it gives better answers and abides to instructions better\n",
    "MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read system_template.txt file into an f-string\n",
    "with open(\"system_template.txt\", \"r\") as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are a creative assistant with the goal to generate a synthetic dataset of Weights &amp; Biases (W&amp;B) user          \n",
       "questions. W&amp;B users are asking these questions to a bot, so they don't know the answer and their questions are    \n",
       "grounded in what they're trying to achieve. We are interested in questions that can be answered by W&amp;B             \n",
       "documentation. But the users don't have access to this documentation, so you need to imagine what they're trying to\n",
       "do and use according language.                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are a creative assistant with the goal to generate a synthetic dataset of Weights & Biases (W&B) user          \n",
       "questions. W&B users are asking these questions to a bot, so they don't know the answer and their questions are    \n",
       "grounded in what they're trying to achieve. We are interested in questions that can be answered by W&B             \n",
       "documentation. But the users don't have access to this documentation, so you need to imagine what they're trying to\n",
       "do and use according language.                                                                                     \n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prompt_template.txt file into an f-string\n",
    "with open(\"prompt_template.txt\", \"r\") as file:\n",
    "    prompt_template = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "{QUESTIONS}                                                                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of W&amp;B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "{CHUNK}                                                                                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how realistic is that this question will come from a real user one day?                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>is this question about W&amp;B?                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>can the question be answered using the W&amp;B document fragment above?                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>impersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "{QUESTIONS}                                                                                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of W&B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "{CHUNK}                                                                                                            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0mhow realistic is that this question will come from a real user one day?                                         \n",
       "\u001b[1;33m • \u001b[0mis this question about W&B?                                                                                     \n",
       "\u001b[1;33m • \u001b[0mcan the question be answered using the W&B document fragment above?                                             \n",
       "\u001b[1;33m • \u001b[0mhow accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "\u001b[1;33m   \u001b[0mimpersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk, n_questions=3):\n",
    "    questions = '\\n'.join(random.sample(real_queries, n_questions))\n",
    "    user_prompt = prompt_template.format(QUESTIONS=questions, CHUNK=chunk)\n",
    "    return user_prompt\n",
    "\n",
    "user_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "how should a project setup look like for a team that uses W&amp;B for experiment tracking and artifacts and now want to\n",
       "use LLMs in the same project. What are best practices? For distributed parallel runs, how can I get the <span style=\"color: #ffffff; text-decoration-color: #ffffff; background-color: #000000\">run_id</span> when\n",
       "I'm on a process other than RANK 0? How should I cite W&amp;B in a manuscript?                                         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of W&amp;B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "2 = Linear(n_layer_1, n_layer_2) self.layer_3 = Linear(n_layer_2, n_classes)                                       \n",
       "\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.loss = CrossEntropyLoss()</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.lr = lr</span><span style=\"background-color: #272822\">                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # save hyper-parameters to self.hparams (auto-logged by W&amp;B)</span><span style=\"background-color: #272822\">                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.save_hyperparameters()</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def forward(self, x):</span><span style=\"background-color: #272822\">                                                                                              </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''method used for inference input -&gt; output'''</span><span style=\"background-color: #272822\">                                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"background-color: #272822\">                                                                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # (b, 1, 28, 28) -&gt; (b, 1*28*28)</span><span style=\"background-color: #272822\">                                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    batch_size, channels, width, height = x.size()</span><span style=\"background-color: #272822\">                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = x.view(batch_size, -1)</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # let's do 3 x (linear + relu)</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = F.relu(self.layer_1(x))</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = F.relu(self.layer_2(x))</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x = self.layer_3(x)</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return x</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def training_step(self, batch, batch_idx):</span><span style=\"background-color: #272822\">                                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''needs to return a loss from a single batch'''</span><span style=\"background-color: #272822\">                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    _, loss, acc = self._get_preds_loss_accuracy(batch)</span><span style=\"background-color: #272822\">                                                            </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # Log loss and metric</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('train_loss', loss)</span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('train_accuracy', acc)</span><span style=\"background-color: #272822\">                                                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return loss</span><span style=\"background-color: #272822\">                                                                                                    </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def validation_step(self, batch, batch_idx):</span><span style=\"background-color: #272822\">                                                                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''used for logging metrics'''</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    preds, loss, acc = self._get_preds_loss_accuracy(batch)</span><span style=\"background-color: #272822\">                                                        </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    # Log loss and metric</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('val_loss', loss)</span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    self.log('val_accuracy', acc)</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return preds</span><span style=\"background-color: #272822\">                                                                                                   </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def configure_optimizers(self):</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''defines model optimizer'''</span><span style=\"background-color: #272822\">                                                                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return Adam(self.parameters(), lr=self.lr)</span><span style=\"background-color: #272822\">                                                                     </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">def _get_preds_loss_accuracy(self, batch):</span><span style=\"background-color: #272822\">                                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    '''convenience function since train/valid/test steps are similar'''</span><span style=\"background-color: #272822\">                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    x, y = batch</span><span style=\"background-color: #272822\">                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    logits = self(x)</span><span style=\"background-color: #272822\">                                                                                               </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    preds = torch.argmax(logits, dim=1)</span><span style=\"background-color: #272822\">                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    loss = self.loss(logits, y)</span><span style=\"background-color: #272822\">                                                                                    </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    acc = accuracy(preds, y)</span><span style=\"background-color: #272822\">                                                                                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    return preds, loss, acc</span><span style=\"background-color: #272822\">                                                                                        </span>\n",
       "\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">### Log the min/max of your metric</span><span style=\"background-color: #272822\">                                                                                 </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Using wandb's [`define_metric`](https://docs.wandb.ai/ref/python/run#define\\_metric) function you can define </span><span style=\"background-color: #272822\">      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">whether you'd like your W&amp;B summary metric to display the min, max, mean or best value for that metric. If </span><span style=\"background-color: #272822\">        </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">`define`_`metric` _ isn't used, then the last value logged with appear in your summary metrics. See the </span><span style=\"background-color: #272822\">           </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">`define_metric` [reference docs here](https://docs.wandb</span><span style=\"background-color: #272822\">                                                           </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">***</span><span style=\"background-color: #272822\">                                                                                                                </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">You will now generate a user question and corresponding answer based on the above document. </span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">First, explain the user context and what problems they might be trying to solve. </span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Second, generate user question. </span><span style=\"background-color: #272822\">                                                                                   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Third, provide the accurate and concise answer in markdown format to the user question using the documentation. </span><span style=\"background-color: #272822\">   </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">You'll be evaluated on:</span><span style=\"background-color: #272822\">                                                                                            </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">- how realistic is that this question will come from a real user one day? </span><span style=\"background-color: #272822\">                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">- is this question about W&amp;B? </span><span style=\"background-color: #272822\">                                                                                     </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">- can the question be answered using the W&amp;B document fragment above? </span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">- how accurate is the answer?</span><span style=\"background-color: #272822\">                                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Remember that users have different styles and can be imprecise. You are very good at impersonating them!</span><span style=\"background-color: #272822\">           </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Use the following format:</span><span style=\"background-color: #272822\">                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">CONTEXT: </span><span style=\"background-color: #272822\">                                                                                                          </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">QUESTION: </span><span style=\"background-color: #272822\">                                                                                                         </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">ANSWER: </span><span style=\"background-color: #272822\">                                                                                                           </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Let's start!</span><span style=\"background-color: #272822\">                                                                                                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "how should a project setup look like for a team that uses W&B for experiment tracking and artifacts and now want to\n",
       "use LLMs in the same project. What are best practices? For distributed parallel runs, how can I get the \u001b[97;40mrun_id\u001b[0m when\n",
       "I'm on a process other than RANK 0? How should I cite W&B in a manuscript?                                         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of W&B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "2 = Linear(n_layer_1, n_layer_2) self.layer_3 = Linear(n_layer_2, n_classes)                                       \n",
       "\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.loss = CrossEntropyLoss()\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.lr = lr\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # save hyper-parameters to self.hparams (auto-logged by W&B)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.save_hyperparameters()\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef forward(self, x):\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''method used for inference input -> output'''\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m                                                                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # (b, 1, 28, 28) -> (b, 1*28*28)\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    batch_size, channels, width, height = x.size()\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = x.view(batch_size, -1)\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # let's do 3 x (linear + relu)\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = F.relu(self.layer_1(x))\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = F.relu(self.layer_2(x))\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x = self.layer_3(x)\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return x\u001b[0m\u001b[48;2;39;40;34m                                                                                                       \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef training_step(self, batch, batch_idx):\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''needs to return a loss from a single batch'''\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    _, loss, acc = self._get_preds_loss_accuracy(batch)\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # Log loss and metric\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('train_loss', loss)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('train_accuracy', acc)\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return loss\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef validation_step(self, batch, batch_idx):\u001b[0m\u001b[48;2;39;40;34m                                                                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''used for logging metrics'''\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    preds, loss, acc = self._get_preds_loss_accuracy(batch)\u001b[0m\u001b[48;2;39;40;34m                                                        \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    # Log loss and metric\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('val_loss', loss)\u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    self.log('val_accuracy', acc)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return preds\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef configure_optimizers(self):\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''defines model optimizer'''\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return Adam(self.parameters(), lr=self.lr)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mdef _get_preds_loss_accuracy(self, batch):\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    '''convenience function since train/valid/test steps are similar'''\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    x, y = batch\u001b[0m\u001b[48;2;39;40;34m                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    logits = self(x)\u001b[0m\u001b[48;2;39;40;34m                                                                                               \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    preds = torch.argmax(logits, dim=1)\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    loss = self.loss(logits, y)\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    acc = accuracy(preds, y)\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m    return preds, loss, acc\u001b[0m\u001b[48;2;39;40;34m                                                                                        \u001b[0m\n",
       "\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m### Log the min/max of your metric\u001b[0m\u001b[48;2;39;40;34m                                                                                 \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mUsing wandb's [`define_metric`](https://docs.wandb.ai/ref/python/run#define\\_metric) function you can define \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mwhether you'd like your W&B summary metric to display the min, max, mean or best value for that metric. If \u001b[0m\u001b[48;2;39;40;34m        \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m`define`_`metric` _ isn't used, then the last value logged with appear in your summary metrics. See the \u001b[0m\u001b[48;2;39;40;34m           \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m`define_metric` [reference docs here](https://docs.wandb\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m***\u001b[0m\u001b[48;2;39;40;34m                                                                                                                \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mYou will now generate a user question and corresponding answer based on the above document. \u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mFirst, explain the user context and what problems they might be trying to solve. \u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mSecond, generate user question. \u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mThird, provide the accurate and concise answer in markdown format to the user question using the documentation. \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mYou'll be evaluated on:\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m- how realistic is that this question will come from a real user one day? \u001b[0m\u001b[48;2;39;40;34m                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m- is this question about W&B? \u001b[0m\u001b[48;2;39;40;34m                                                                                     \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m- can the question be answered using the W&B document fragment above? \u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34m- how accurate is the answer?\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mRemember that users have different styles and can be imprecise. You are very good at impersonating them!\u001b[0m\u001b[48;2;39;40;34m           \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mUse the following format:\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mCONTEXT: \u001b[0m\u001b[48;2;39;40;34m                                                                                                          \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mQUESTION: \u001b[0m\u001b[48;2;39;40;34m                                                                                                         \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mANSWER: \u001b[0m\u001b[48;2;39;40;34m                                                                                                           \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mLet's start!\u001b[0m\u001b[48;2;39;40;34m                                                                                                       \u001b[0m\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(documents, n_questions=3, n_generations=5):\n",
    "    questions = []\n",
    "    for _, document in documents:\n",
    "        chunk = extract_random_chunk(document)\n",
    "        user_prompt = generate_context_prompt(chunk, n_questions)\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        response = completion_with_backoff(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            n = n_generations,\n",
    "            )\n",
    "        questions.extend([response.choices[i].message.content for i in range(n_generations)])\n",
    "    return questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A Note about the `system` role: For GPT4 based pipelines you probably want to move some part of the context prompt to the `system` context. As we are using `gpt3.5-turbo` here, you can put the instruction on the user prompt, you can read more about this on [OpenAI docs here](https://platform.openai.com/docs/guides/chat/instructing-chat-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse model generation and extract CONTEXT, QUESTION and ANSWER\n",
    "def parse_generation(generation):\n",
    "    lines = generation.split(\"\\n\")\n",
    "    context = []\n",
    "    question = []\n",
    "    answer = []\n",
    "    flag = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"CONTEXT:\" in line:\n",
    "            flag = \"context\"\n",
    "            line = line.replace(\"CONTEXT:\", \"\").strip()\n",
    "        elif \"QUESTION:\" in line:\n",
    "            flag = \"question\"\n",
    "            line = line.replace(\"QUESTION:\", \"\").strip()\n",
    "        elif \"ANSWER:\" in line:\n",
    "            flag = \"answer\"\n",
    "            line = line.replace(\"ANSWER:\", \"\").strip()\n",
    "\n",
    "        if flag == \"context\":\n",
    "            context.append(line)\n",
    "        elif flag == \"question\":\n",
    "            question.append(line)\n",
    "        elif flag == \"answer\":\n",
    "            answer.append(line)\n",
    "\n",
    "    context = \"\\n\".join(context)\n",
    "    question = \"\\n\".join(question)\n",
    "    answer = \"\\n\".join(answer)\n",
    "    return context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"A user is using Weights & Biases integration with PyTorch Lightning and wants to log their model's gradients, parameter histograms, and model topology during training.\\n\",\n",
       " 'How do I log the gradients, parameter histograms, and topology of my model while training it with PyTorch Lightning and W&B?\\n',\n",
       " \"To log your model's gradients, parameter histograms, and topology while training with PyTorch Lightning and W&B, you can pass your model object to `wandb_logger.watch()` function. Here's how you can do it:\\n\\n1. Instantiate the `WandbLogger` and pass it to Lightning's `Trainer`:\\n\\n```python\\nwandb_logger = WandbLogger()\\ntrainer = Trainer(logger=wandb_logger)\\n```\\n\\n2. Pass your model object to `wandb_logger.watch()`:\\n\\n```python\\nwandb_logger.watch(model)\\n```\\n\\nFor more details, check the PyTorch Lightning [`WandbLogger` documentation](https://pytorch-lightning.readthedocs.io/en/latest/extensions/generated/pytorch_lightning.loggers.WandbLogger.html?highlight=wandblogger).\")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = generate_questions([documents[0]], n_questions=3, n_generations=5)\n",
    "parse_generation(generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x12fd4a3e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_generations = []\n",
    "generations = generate_questions(documents, n_questions=3, n_generations=5)\n",
    "for generation in generations:\n",
    "    context, question, answer = parse_generation(generation)\n",
    "    parsed_generations.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "# let's convert parsed_generations to a pandas dataframe and save it locally\n",
    "df = pd.DataFrame(parsed_generations)\n",
    "df.to_csv('generated_examples.csv', index=False)\n",
    "\n",
    "# log df as a table to W&B for interactive exploration\n",
    "wandb.log({\"generated_examples\": wandb.Table(dataframe=df)})\n",
    "\n",
    "# log csv file as an artifact to W&B for later use\n",
    "artifact = wandb.Artifact(\"generated_examples\", type=\"dataset\")\n",
    "artifact.add_file(\"generated_examples.csv\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earnest-bush-30</strong> at: <a href='https://wandb.ai/darek/llmapps/runs/6478i8zh' target=\"_blank\">https://wandb.ai/darek/llmapps/runs/6478i8zh</a><br/>Synced 7 W&B file(s), 1 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230601_134554-6478i8zh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
