{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Retrieval Question Answering\n",
    "\n",
    "## Parsing documents\n",
    "\n",
    "We will use a small sample of markdown documents in this notebook. Let's find them and make sure we can stuff them into the prompt. That means they may need to be chunked and not exceed some number of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# This function is used to find all the markdown files in a directory  \n",
    "def find_md_files(directory):\n",
    "    md_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".md\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    md_files.append(Document(page_content=f.read(), metadata={\"source\": file_path}))\n",
    "    return md_files\n",
    "\n",
    "documents = find_md_files('docs_sample')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cl100k_base'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# We will need to coiunt tokens in the documents, and for that we need to know the encoding\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "encoding_name = encoding.name\n",
    "encoding_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4179, 365, 1206, 2596, 2940, 537, 956, 803, 1644, 2529, 2093]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to count the number of tokens in each document\n",
    "def count_tokens(documents):\n",
    "    token_counts = [len(encoding.encode(document.page_content)) for document in documents]\n",
    "    return token_counts\n",
    "\n",
    "count_tokens(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 987)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# Let's use Langchain markdown splitter to split the documents into sections\n",
    "md_text_splitter = MarkdownTextSplitter()\n",
    "document_sections = md_text_splitter.split_documents(documents)\n",
    "len(document_sections), max(count_tokens(document_sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 512)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# We need to create more granular chunks of text, so we will use the TokenTextSplitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    encoding_name='cl100k_base',\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=0,\n",
    "    allowed_special={\"<|endoftext|>\"},\n",
    ")\n",
    "document_sections = token_splitter.split_documents(documents)\n",
    "len(document_sections), max(count_tokens(document_sections))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Let's now use embeddings with a vector database retriever to find relevant documents for a query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs_sample/collaborate-on-reports.md\n",
      "docs_sample/teams.md\n",
      "docs_sample/teams.md\n",
      "docs_sample/teams.md\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# We will use the OpenAIEmbeddings to embed the text, and Chroma to store the vectors\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(document_sections, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "query = \"How can I share my W&B report with my team members in a public W&B project?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Let's see the results\n",
    "for doc in docs:\n",
    "    print(doc.metadata[\"source\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff Prompt\n",
    "\n",
    "We'll now take the content of the retrieved documents, stuff them into prompt template along with the query, and pass into an LLM to obtain the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "prompt = PROMPT.format(context=context, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' To share a report, select the **Share** button on the upper right hand '\n",
      " 'corner. You can either provide an email account or copy the magic link. '\n",
      " 'Users invited by email will need to log into Weights & Biases to view the '\n",
      " 'report. Users who are given a magic link to not need to log into Weights & '\n",
      " 'Biases to view the report. Shared reports are view-only.')\n"
     ]
    }
   ],
   "source": [
    "# Use langchain to call openai chat API with the question\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()\n",
    "response = llm.predict(prompt)\n",
    "pprint(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langchain\n",
    "\n",
    "Langchain gives us tools to do this efficiently in few lines of code. Let's do the same using `RetrievalQA` chain. We will also pass a W&B Tracer callback into the chain so we can analyze and debug it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/darek/llmapps/runs/ga1z0jg1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' You can share your report in a public W&B project by selecting the \"Share\" '\n",
      " 'button on the upper right hand corner. You can then provide an email account '\n",
      " 'or copy the magic link. Users invited by email will need to log into Weights '\n",
      " '& Biases to view the report. Users who are given a magic link do not need to '\n",
      " 'log into Weights & Biases to view the report. Shared reports are view-only.')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from wandb.integration.langchain import WandbTracer\n",
    "\n",
    "wandb_config = {\"project\": \"llmapps\"}\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "result = qa.run(query, callbacks=[WandbTracer(wandb_config)])\n",
    "pprint(result)\n",
    "\n",
    "WandbTracer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
