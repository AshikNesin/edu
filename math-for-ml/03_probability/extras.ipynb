{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extras.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQCJ14kPZrLX"
      },
      "source": [
        "# Math4ML Part III: Probability - Extras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg3cotCfruZP"
      },
      "source": [
        "# Setup Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LQYrNA2ZrLZ"
      },
      "source": [
        "# allows interactive plotting\n",
        "%matplotlib widget\n",
        "\n",
        "# importing from standard library\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# importing libraries\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !git clone --branch \"math4ml/reorg\" \"https://github.com/wandb/edu.git\"\n",
        "    %cd \"edu/math-for-ml/03_probability\"\n",
        "else:\n",
        "    pass\n",
        "\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "\n",
        "# importing course-specific modules\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hHh1EmrZrLZ"
      },
      "source": [
        "# Section 1. Visualizing and Minimizing Surprises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4cLpaaJZrLa"
      },
      "source": [
        "In machine learning,\n",
        "we often devise complex models that implement high-dimensional\n",
        "and/or highly non-linear functions of the inputs.\n",
        "\n",
        "While this brings a lot of power,\n",
        "it often obscures the central concepts,\n",
        "like the surprise,\n",
        "aka negative log probabilty.\n",
        "\n",
        "In these two examples, we'll consider a very simple model:\n",
        "one that takes no inputs and produces a single value.\n",
        "\n",
        "Informally, we implicitly make these sorts of simple models\n",
        "without realizing it all the time:\n",
        "for example, whenever we summarize polling data by saying\n",
        "something like \"53% +/- 1 of those surveyed approved\".\n",
        "\n",
        "Similarly, any time we calculate the mean $\\mu$\n",
        "and standard deviation, $\\sigma$, of a dataset,\n",
        "and summarize the result as $\\mu+/-\\sigma$,\n",
        "we've formed a model,\n",
        "and we've actually minimized a surprise,\n",
        "or maximized a likelihood.\n",
        "\n",
        "Below, we'll examine that relationship."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_jjB4qwZrLa"
      },
      "source": [
        "## Artisanal, Hand-Tuned, Small-Batch Data Science"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h7WuCQhZrLa"
      },
      "source": [
        "Typically, we minimize the surprises of our models with algorithms,\n",
        "like gradient descent.\n",
        "As a special case, this includes calculating the best model parameters directly.\n",
        "\n",
        "But there's nothing to say we couldn't do it ourselves!\n",
        "\n",
        "The cell below generates an interactive plot showing a histogram of some data\n",
        "and a tuneable Gaussian density.\n",
        "By adjusting the sliders,\n",
        "you can move the center of this distribution and change its spread.\n",
        "Underneath, the surprise is printed.\n",
        "\n",
        "It is calculated by taking the average of the logarithm of the Gaussian density,\n",
        "evaluated at each of the values in `data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fo2VIdwZrLb"
      },
      "source": [
        "true_mu = -1.; true_sigma = 0.5; N = 20;\n",
        "data = true_sigma * np.random.standard_normal(size=N) + true_mu\n",
        "\n",
        "\n",
        "fitter = utils.mle.make_gauss_fitter(data, true_mu, true_sigma)\n",
        "utils.mle.make_interactor(fitter, mu_lims=[-10, 10], sigma_lims=[1e-3, 10]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGOF5A0RZrLb"
      },
      "source": [
        "To begin, make the surprise as low as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Geb_ALZrLb"
      },
      "source": [
        "You might check in which direction the surprise goes down,\n",
        "for each parameter, and move one step in that direction.\n",
        "This is the by-hand version of the algorithm\n",
        "[coordinate descent](https://en.wikipedia.org/wiki/Coordinate_descent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNtOPswvZrLb"
      },
      "source": [
        "#### Q Spatially, how do the probability density and the data relate at this point? Is the density high or low  where the histogram indicates the presence of more data points?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV5OJrnAZrLb"
      },
      "source": [
        "By default, the values of $\\mu$ and $\\sigma$ used to generate\n",
        "the data are `0` and `0.5`,\n",
        "respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dMPlVfMZrLc"
      },
      "source": [
        "#### Q Is the value you landed on equal to these \"ground truth\" values? Do you find this surprising?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHJ6xEX3ZrLc"
      },
      "source": [
        "The cell below computes the mean and standard deviation of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBWRE3ziZrLc"
      },
      "source": [
        "np.mean(data), np.std(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XplYKTJZrLc"
      },
      "source": [
        "Input these numbers as values for $\\mu$ and $\\sigma$\n",
        "by clicking the values next to the sliders and typing (or copy-pasting) them in.\n",
        "\n",
        "#### Q Is the surprise higher or lower than the surprise you found by hand? Relate this finding to the statement above, that every time we summarize data by the mean and standard deviation, we've minimized the surprise of a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7NvXwxqZrLc"
      },
      "source": [
        "Increase the value of `N`,\n",
        "which sets the total number of values in `data`,\n",
        "by at least an order of magnitude, e.g. to `200`.\n",
        "\n",
        "Then fit the model again, by minimizing the surprise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48G-r8GJZrLd"
      },
      "source": [
        "#### Q Are the values you found the same? Are they closer to or further from the \"true\" values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW5MmDaqZrLd"
      },
      "source": [
        "Increase the value of `N` by another order of magnitude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2J-kgUYZrLd"
      },
      "source": [
        "#### Q Are the values of $\\mu$ and $\\sigma$ now closer or further from the values used to generate the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z9ylXsMZrLd"
      },
      "source": [
        "#### Q What happens to the surprise if you reduce $\\sigma$ to extremely values, e.g. 1e-3?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J6FKPxLZrLd"
      },
      "source": [
        "## Visualizing the Surprise as a Function of the Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYZ8lobpZrLd"
      },
      "source": [
        "The section above visualized the surprise as a function of the data values:\n",
        "$$\n",
        "s(x; \\mu, \\sigma) = -\\log(p(x; \\mu, \\sigma))\n",
        "$$\n",
        "where $p(\\cdot; \\mu, \\sigma)$\n",
        "is the probability density associated with the parameters\n",
        "$\\mu$ and $\\sigma$,\n",
        "which assigns a probability to all possible observations.\n",
        "\n",
        "But when we are working with models,\n",
        "we aren't concerned directly with the \n",
        "probabilities of observing data points --\n",
        "the data has already been observed.\n",
        "\n",
        "We are instead interested in which parameters\n",
        "to use to help us model the data,\n",
        "and so are interested in $s$ as a function of $\\mu$ and $\\sigma$ instead.\n",
        "\n",
        "In the jargon of machine learning, an object like that,\n",
        "which is a probability density when thought of as a function over data values\n",
        "but which we're thinking of as a function over parameter values,\n",
        "is called a _likelihood_.\n",
        "\n",
        "The cell below visualizes the surprise of the same, simple model\n",
        "as function of a single parameter, $\\mu$,\n",
        "that is, as a negative logarithm of the likelihood.\n",
        "\n",
        "This is the quantity which we are minimizing when we git models.\n",
        "\n",
        "Before proceeding, return to the data generation cell above\n",
        "and change `N` back to `20`.\n",
        "\n",
        "Then execute the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-XlrmrOZrLe"
      },
      "source": [
        "utils.mle.make_plot(data, num_gaussians=5, true_mu=true_mu);\n",
        "\n",
        "print(\"The surprise-minimizing estimate for µ is \"+str(np.mean(data)))\n",
        "print(\"The value of µ used to generate the data is \"+str(true_mu))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml1VlPf-ZrLe"
      },
      "source": [
        "#### Q What is type of function is the model surprise in the top panel (e.g. trigonometric,  exponential, linear)? Can you explain why it has that shape?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CQFFMvYZrLe"
      },
      "source": [
        "The surprise-minimizing value of $\\mu$ is indicated by a star."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJJhn7O4ZrLe"
      },
      "source": [
        "#### Q Relate the fact that this value minimizes the surprise to the features of the graph of the surprise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9orWKmuZrLf"
      },
      "source": [
        "If you are familiar with gradient descent, answer the question below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxQeQMBqZrLf"
      },
      "source": [
        "#### Q Explain why a gradient descent-based method for minimizing the surprise is attracted to this point and why it does not leave once it reaches it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qVCvA82ZrLf"
      },
      "source": [
        "Re-run this cell after increasing the value of `N` by at least an order of magnitude.\n",
        "If the answer is unclear, consider increasing `N` even more.\n",
        "\n",
        "#### Q Does the general shape of the surprise change with the value of `N`? What, if anything, does change about it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdiL1MkRZrLf"
      },
      "source": [
        "# Section 2. The Central Limit Theorem In Action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwoRM1bmZrLf"
      },
      "source": [
        "The previous example used, as do many machine learning models,\n",
        "a Gaussian density for the model surprise,\n",
        "which corresponds to using the squared error for the loss function.\n",
        "\n",
        "This choice is convenient, but is it justified?\n",
        "\n",
        "The classic justification for it is as follows:\n",
        "the things we aren't measuring and including in our model are\n",
        "- **numerous** -- any model is a massive simplification of the real world\n",
        "- **independent or weakly dependent** -- otherwise we might summarize them into a single measurement and include that in our model\n",
        "- **of roughly equal magnitude** -- if there are a small number with larger magntiude (variance), we also might include them in our model\n",
        "- **combine additively** -- lots of physical processes behave this way\n",
        "\n",
        "When all of these things hold,\n",
        "the errors introduced by the things we aren't measuring\n",
        "will have an appxroximately Gaussian distribution.\n",
        "This is the Limit Theorem so important it gets to be called\n",
        "the _Central Limit Theorem_.\n",
        "\n",
        "The cells below demonstrate this theorem in action for a few different probability distributions.\n",
        "\n",
        "Specifically, they show that if we look at the distribution obtained by adding\n",
        "a whole colection of random variables, each of which has the same distribution,\n",
        "we will see a Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUUKrZYUZrLf"
      },
      "source": [
        "coin_flip = [0.5, 0.5] # heads, tails. \"adding up\" means counting how many tails you get\n",
        "d6_roll = [0] + [1/6] * 6 # a six-sided die.\n",
        "d20_roll = [0] + [1/20] * 20 # roll for initiative!\n",
        "wonky_pmf = [1/2, 1/8] + [1/100] * 10 + [3/8 - 1/10]  # even very uneven distributions work\n",
        "your_pmf_here = [] # write your own! must sum to ~1, all values >=0 and < 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4gTiQ25ZrLg"
      },
      "source": [
        "This cell selects a distribution\n",
        "(more specifically, a `p`robability `m`ass `f`unction)\n",
        "and number of times it should be added.\n",
        "\n",
        "Values higher than `30` should be avoided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnXJYe0zZrLg"
      },
      "source": [
        "pmf, iters = coin_flip, 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGW8aZPdZrLg"
      },
      "source": [
        "This cell visualizes the distribution.\n",
        "\n",
        "Note that none are shaped anything like a Gaussian!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiHxtoyeZrLg"
      },
      "source": [
        "plt.figure(); plt.bar(range(len(pmf)), pmf)\n",
        "plt.title(\"Probability Distribution\")\n",
        "plt.ylabel(\"Probability\"); plt.xlabel(\"Value\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx7XCAkAZrLg"
      },
      "source": [
        "This final cell produces a short animation showing the evolution of the distribution\n",
        "as more and more variables are added together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ISVvwZZrLg"
      },
      "source": [
        "anim_html = utils.clt.setup_and_run_animation(pmf, iters)\n",
        "\n",
        "IPython.display.HTML(anim_html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BhAVzR8ZrLh"
      },
      "source": [
        "#### Q How many iterations would you say it takes for each example to \"converge\" to the limit and appear approximately Gaussian?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36SGvoV3ZrLh"
      },
      "source": [
        "Though all get there in the end, some take longer than others.\n",
        "\n",
        "#### Q Of the four provided `pmf`s, which takes the longest to resemble a Gaussian? Can you articulate what about it seems to cause the issue?"
      ]
    }
  ]
}