{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math4ML Part I: Linear Algebra - Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nF_N9U9pXRN"
   },
   "source": [
    "# Setup Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFEk1hwsvjh-"
   },
   "source": [
    "This section includes setup code for the remaining sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --user -qq wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OGWHqVCZGuGk",
    "outputId": "4b3030f0-dff8-49de-bbb1-c3006a008206"
   },
   "outputs": [],
   "source": [
    "# importing from standard library\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# importing libraries\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# handle \n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !git clone --branch \"math4ml\" \"https://github.com/wandb/edu.git\"\n",
    "    %cd \"math-for-ml/01_linearalgebra\"\n",
    "else:\n",
    "    pass\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "# importing course-specific modules\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9_uzirotQfB"
   },
   "source": [
    "# Section 1. Visualizing Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iEueBO2Aj7OX"
   },
   "source": [
    "In two or three dimensions, vectors and linear transformations of vectors\n",
    "can be easily visualized:\n",
    "we can draw a bunch of vectors as points on a two-dimensional screen\n",
    "or in a three-dimensional environment and then _animate_ the transformation.\n",
    "\n",
    "This section does just that,\n",
    "with the aim of building intuition for a bunch of concepts from linear algebra:\n",
    "diagonal matrices, rank, and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftCT7rjsuxLE"
   },
   "source": [
    "See [this YouTube series by 3blue1brown](https://www.youtube.com/watch?v=F3lG9_SxCXk)\n",
    "for more visualizations of linear algebraic concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vS901_tlOsl"
   },
   "source": [
    "The animations are set up by the `setup_plot` function in the `utils.animate` module.\n",
    "\n",
    "Run the cell below to see the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNWHx-DPfGHX"
   },
   "outputs": [],
   "source": [
    "utils.animate.setup_plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHT_SNmveatz"
   },
   "source": [
    "## A Collection of Interesting Linear Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5-z3ZuDlZc9"
   },
   "source": [
    "This cell defines several linear transformations that you might want to visualize.\n",
    "\n",
    "You can also define your own linear transformations,\n",
    "so long as they are represented by two-by-two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUxPYaGpVG8I"
   },
   "outputs": [],
   "source": [
    "eye = [[1, 0],\n",
    "       [0, 1]]\n",
    "\n",
    "scaling1 = [[3, 0],\n",
    "            [0, 3]]\n",
    "\n",
    "scaling2 = [[1/2, 0],\n",
    "            [0, 1/2]]\n",
    "\n",
    "shear = [[1, 2],\n",
    "         [0, 1]]\n",
    "\n",
    "simple_reflection = [[0, 1],\n",
    "                     [1, 0]]\n",
    "\n",
    "negative_reflection = [[0, -1],\n",
    "                       [-1 ,0]]\n",
    "\n",
    "rank0 = [[0, 0],\n",
    "         [0, 0]]\n",
    "\n",
    "rank1 = [[1, 2],\n",
    "         [1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyNkycl7i5BG"
   },
   "source": [
    "## Animating the Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates the animations. See the comments for instructions on use.\n",
    "\n",
    "Some guiding questions appear below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ct_eu0sEboWh"
   },
   "outputs": [],
   "source": [
    "# This line selects the transformation to visualize.\n",
    "transform = scaling1\n",
    "\n",
    "# This line places a mesh of points to animate:\n",
    "#  either centered at 0 or in the top-right quadrant\n",
    "mesh = utils.animate.all_quadrants_mesh\n",
    "# mesh = utils.animate.unit_square_mesh\n",
    "\n",
    "# These lines execute the animation code.\n",
    "\n",
    "f, ax, animate, n_frames = utils.animate.setup_plot(\n",
    "    transform,\n",
    "    mesh_properties=mesh,\n",
    "    animate_basis=True)\n",
    "\n",
    "anim = animation.FuncAnimation(f, animate, frames=n_frames)\n",
    "HTML(anim.to_jshtml(fps=24, default_mode=\"reflect\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Transformations and Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the transformations `scaling1` and `scaling2`.\n",
    "\n",
    "A `scaling` matrix just changes the lengths of vectors, all by the same amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q How can we tell a matrix is a scaling matrix just by looking at its entries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis vectors are the columns of an identity matrix (like `eye`).\n",
    "\n",
    "They determine the bottom and left sides of a square with area 1. We call this the \"unit\" square.\n",
    "\n",
    "After the basis vectors are transformed, they still map out a four-sided figure: always a parallelogram, sometimes a rectangle, and sometimes a square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What four-sided shape do the basis vectors define when they're transformed by a scaling matrix? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important property of a matrix is its _determinant_.\n",
    "\n",
    "Determinants tell us, among other things, the ratio of the area of the parallelogram to the area .\n",
    "\n",
    "Use the `unit_square_mesh` to visualize this parallelogram directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Based on this geometric information, what's the determinant of a scaling matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very special \"scaling\" matrix above, called `eye`.\n",
    "\n",
    "Animate its transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What's so special about `eye`?\n",
    "\n",
    "One way of viewing the entries of a matrix is that the columns tell you where your basis vectors go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Explain why `eye` is special from this point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate the matrices `rank1` and `rank0`.\n",
    "\n",
    "These matrices have lower _rank_ than the other transformations.\n",
    "\n",
    "This makes them \"non-invertible\" -- there is no way to \"undo\" the transformation they perform.\n",
    "\n",
    "#### Q Based on the animation, can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, calculate the determinants of the low-rank matrices (with `np.linalg.det`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply np.linalg.det here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q What do you get? Give an explanation for this number in terms of the \"area of parallelograms\" interpretation of the determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotations and Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call some transformations \"reflections\" because they act like mirrors --\n",
    "they transform points on one side of some axis (aka vector) to their mirror images on the other side.\n",
    "\n",
    "#### What are the axes around which the `negative_reflection` and `simple_reflection` reflect points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function `utils.animate.make_rotation` with argument `theta` will make a matrix that rotates the input by an angle `theta`.\n",
    "\n",
    "This works in radians, not degress, so if you want to make a 90 degree rotation, you should input ${\\pi \\over 2}$.\n",
    "In Python, that would be written `np.pi / 2`.\n",
    "\n",
    "Make a 180 degree rotation matrix, name it `rot180`, and visualize its transformation.\n",
    "\n",
    "Note that `rot180` and `negative_reflection` both send the `unit_square_mesh` to the bottom-left quadrant.\n",
    "\n",
    "These transformations are not the same, however.\n",
    "\n",
    "#### Q Compare the animations. How do the transformations `rot180` and `negative_reflection` differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine two transformations into one using matrix multiplication, `np.matmul`.\n",
    "\n",
    "That is, from two transformations, we produce a single transformation, the _composition_ of the two components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose a rotation matrix and a scaling matrix together and observe the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying two complex numbers, or numbers that can have both real and imaginary parts,\n",
    "can be implemented using vectors and matrices.\n",
    "\n",
    "One complex number is represented by a 2-dimensional vector, whose entries represent the real and imaginary components, respectively.\n",
    "\n",
    "The other is represented by a 2x2 matrix that is the composition of a rotation and a scaling matrix.\n",
    "\n",
    "The rotation angle $\\theta$ and scaling value $r$ for creating this matrix come from the polar representation of the complex number: $r\\mathrm{e}^{i\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4j53wEQC0zr"
   },
   "source": [
    "# Section 2. The Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _singular value decomposition_ or SVD is used to break a matrix $M$ up\n",
    "into three canonical components:\n",
    "\n",
    "- $U$, a \"tall\" matrix: one that is at least as tall (number of rows) as it is wide (number of columns).\n",
    "- $S$, a square matrix with zeros everywhere except on the diagonal.\n",
    "- $V^T$, a \"wide\" matrix: one that is at least as wide (number of columns) as it is tall (number of rows). It is thought of as the transpose of a matrix $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which are constructed to satisfy the expression below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "U S V^T = M\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "among some others, which we will shortly investigate.\n",
    "\n",
    "This decomposition works as follows:\n",
    "\n",
    "- First, $V^T$ \"throws out\" every vector that $M$ maps to $0$ (these vectors are the _kernel_ of $M$). The result is that every different output of $V^T$ results in a different output at the end of the pipeline. $V^T$, then, contains all the aspects of $M$ that are \"many-to-one\". It also performs an orthogonal operation: a composition of rotations and reflections.\n",
    "- Then, $S$ applies a scaling transformation. It is also often denoted $\\Sigma$. It is called the _singular value matrix_ and its entries the <i>singular values</i>. Its inputs are always the same size as its outputs, and it only maps $0$ to $0$. Therefore, it is a square matrix of full rank and so is invertible.\n",
    "- Finally, $U$ takes the output of $S$ and makes it into an array of the right shape. In general, the output of $S$ can be smaller than the output of $M$, and so $U$ has more rows (i.e. slots in its output) than columns (i.e. slots in its input). $U$ also includes an orthogonal operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine these pieces using the function `utils.svd.compact`.\n",
    "\n",
    "This specialized version of the SVD was written for this course.\n",
    "For details on using the version in `numpy`, `np.linalg.svd`,\n",
    "see the notes at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate a few matrices to work with: two square matrices with `full_rank` and `low_rank` plus a `wide` and a `tall` matrix.\n",
    "\n",
    "These matrices are all random: the entries are draw by a random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 25\n",
    "rank = 4\n",
    "\n",
    "full_rank = utils.random_matrix.SymmetricWigner(size).M\n",
    "low_rank = utils.random_matrix.Wishart(size, rank).M\n",
    "\n",
    "wide_matrix = np.squeeze(\n",
    "    [utils.random_matrix.generate_random_unit_vector(dim=size) for _ in range(rank)])\n",
    "tall_matrix = np.squeeze(\n",
    "    [utils.random_matrix.generate_random_unit_vector(dim=size) for _ in range(rank)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility `utils.svd.show_matrix` is also included, so you can visualize the matrices, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_matrix(full_rank); utils.svd.show_matrix(low_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_matrix(wide_matrix); utils.svd.show_matrix(tall_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's confirm that the SVD is working as intended.\n",
    "\n",
    "First, we run it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V_T = utils.svd.compact(full_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then, we want to check where $USV^T = M$.\n",
    "\n",
    "The cell below uses `array_equal`,\n",
    "which checks whether all of the entries in a pair of arrays are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before running this cell below, think about what you expect it to return.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(U @ S @ V_T, full_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q Was this surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of the SVD promised equality, but `numpy` is telling us we don't have it.\n",
    "What gives?\n",
    "\n",
    "First, computations with floating point numbers are inexact,\n",
    "and rounding errors abound.\n",
    "When working with floats, we should be very cautious\n",
    "about applying any mathematical claims.\n",
    "\n",
    "And second, the SVD is actually computed using an optimization procedure,\n",
    "and so the answer is inexact.\n",
    "\n",
    "So instead of asking whether two arrays are `equal`,\n",
    "we ask if their entries are `close`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(U @ S @ V_T, full_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'd be great if our mathematical results could give us simple tests to run, the way we can define and run tests for other kinds of code, but unfortunately there's typically a large gap between the math in a textbook and what's happening inside our computer.\n",
    "\n",
    "These sorts of issues make numerical computing, including linear algebra, even harder to learn than other kinds of programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the components of the SVD on some examples.\n",
    "\n",
    "They are displayed in order, from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Low-Rank Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_matrix(low_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_svd(*utils.svd.compact(low_rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SVD is a \"typical case\" for the definition given above.\n",
    "`V_T` is nice and wide, `S` is much smaller than `M`,\n",
    "and `U` is very tall.\n",
    "\n",
    "It is also a typical case for the SVD in practice in ML.\n",
    "In common uses of the SVD,\n",
    "the rank is (approximately) very low,\n",
    "and so the picture above is an accurate mental model.\n",
    "\n",
    "Compare `U` and `V_T` in the graphic above.\n",
    "Though within a matrix there is generally no pattern\n",
    "(because `low_rank` is a random matrix),\n",
    "you might notice a pattern in the entries\n",
    "if you look at both simultaneously.\n",
    "(hint: compare rows of `U` to columns of `V_T`).\n",
    "\n",
    "The cells below confirm that this pattern holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_low, S_low, V_T_low = utils.svd.compact(low_rank)\n",
    "\n",
    "V_low = V_T_low.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(U_low, V_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full-Rank Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the rank is not low,\n",
    "the SVD looks rather different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_matrix(full_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_svd(U, S, V_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $U$ and $V^T$ are both square.\n",
    "\n",
    "Review the definition of tall and wide matrices given above,\n",
    "in the description of the SVD.\n",
    "Under these definitions, a square matrix is actually both tall and wide!\n",
    "These kinds of definitional subtleties are typical in mathematics, so watch out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for a full-rank square matrix,\n",
    "$U$ and $V^T$ are both square,\n",
    "for a low-rank square matrix,\n",
    "$U$ and $V^T$ are both non-square.\n",
    "\n",
    "We can also obtain results between these two extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wide Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_matrix(wide_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_svd(*utils.svd.compact(wide_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tall Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_matrix(tall_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.svd.show_svd(*utils.svd.compact(tall_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the wide matrix, `V_T` is wide but `U` is square.\n",
    "\n",
    "The situation is reversed for the tall matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On Computing the SVD in `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute the singular value decomposition, e.g. in `numpy`,\n",
    "there are a few important things to keep in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, there are many different \"flavors\" of SVD, and the flavor implemented by a library might not be the one you're thinking of.\n",
    "\n",
    "For example, by default, `np.linalg.svd` implements something called a \"full\" SVD,\n",
    "in which `U` and `V_T` are square instead of tall and wide.\n",
    "The same decomposition principle is being applied,\n",
    "but the details are slightly different.\n",
    "For example, `U @ S @ V_T` is not directly defined.\n",
    "\n",
    "If the keyword argument `full_matrices`\n",
    "is provided to `np.linalg.svd` with the value `False`,\n",
    "then the flavor of the SVD changes.\n",
    "Now, it is a form of [reduced SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs),\n",
    "but still not the compact SVD.\n",
    "\n",
    "Second, `numpy` actually returns a vector of singular values,\n",
    "which I'll denote `s`,\n",
    "rather than a matrix of singular values.\n",
    "For the full SVD,\n",
    "this means that while `U @ s @ V_T` is undefined,\n",
    "the expression `U @ np.diag(s) @ V_T` is,\n",
    "where `np.diag` converts between a vector and\n",
    "the matrix that has that vectors for its diagonal entries.\n",
    "`np.diag(s)` is the same as the `S` used above.\n",
    "\n",
    "In general, there can be a large gap between the algorithms discussed in classes and math textbooks and those implemented in production-grade libraries like `numpy`.\n",
    "Make sure to read the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a91oDtxhHgfk"
   },
   "source": [
    "# Section 3. The Matrix Revolution\n",
    "\n",
    "## (almost) Everything is Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7h_DMfMnYxMp"
   },
   "source": [
    "Almost all of the most important operations in linear algebra are based on matrix multiplication.\n",
    "\n",
    "The type signature of matrix multiplication appears below,\n",
    "along with the typical mathematical notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xn8fkxwvd3TE"
   },
   "source": [
    "$$\\mathrm{matmul(x, y)} \\colon \\mathbb{R}^{n \\times m} \\times \\mathbb{R}^{m \\times k} \\rightarrow \\mathbb{R}^{n \\times k}\\\\\n",
    "\\mathrm{matmul}(x, y) = xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fAgOQ9sZDKH"
   },
   "source": [
    "Each section below demonstrates how a key operation in linear algebra is a form of matrix multiplication.\n",
    "\n",
    "The sections also include implementations in pure Python and in several common data science/machine learning libraries.\n",
    "Some sections also include examples of where these operations show up in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TGAGS-PQo58"
   },
   "source": [
    "### Vector-Vector Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stSyOTYfQyJE"
   },
   "source": [
    "#### Dot Product aka Scalar Product aka Inner Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MbZuHDl5LpJ1"
   },
   "source": [
    "$$\n",
    "\\mathrm{dot} \\colon \\mathbb{R}^{n\\times 1} \\times \\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{1\\times 1}  \\\\\n",
    "\\mathrm{dot}(x, y) = \\sum_i x_i \\cdot y_i \\\\ \n",
    "\\mathrm{dot}(x, y) = \\mathrm{matmul}(x^\\top, y)= x^\\top y \\\\ \n",
    "\\mathrm{dot}(x, y) = \\langle x , y \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cpySTJzgIRak"
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "vector1 = [random.normalvariate(0, 1) for _ in range(N)]\n",
    "vector2 = [random.normalvariate(0, 1) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltcCuhZbRGHG"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7tNHzqWHuCU"
   },
   "outputs": [],
   "source": [
    "def inner_product(vector1, vector2):\n",
    "    result = 0\n",
    "    for element1, element2 in zip(vector1, vector2):\n",
    "        result += element1 * element2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkTrg2J2RP56"
   },
   "source": [
    "##### Numpy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWMfbncQKfSC"
   },
   "outputs": [],
   "source": [
    "def inner_product_numpy(vector1, vector2):\n",
    "    return np.sum(np.multiply(vector1, vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7L-9HFwK8qV"
   },
   "outputs": [],
   "source": [
    "def inner_product_numpy_matmul(vector1, vector2):\n",
    "    vector1 = np.atleast_2d(vector1)\n",
    "    vector2 = np.atleast_2d(vector2)\n",
    "\n",
    "    return vector1 @ vector2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPtXV2i0dsB3"
   },
   "outputs": [],
   "source": [
    "def inner_product_numpy_dot(vector1, vector2):\n",
    "    return np.dot(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cHVKLyiRVxd"
   },
   "source": [
    "##### TensorFlow and PyTorch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmuPbqXMKl7U"
   },
   "outputs": [],
   "source": [
    "def inner_product_tf(vector1, vector2):\n",
    "    return tf.reduce_sum(tf.multiply(vector1, vector2)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-7Qow03I44U"
   },
   "outputs": [],
   "source": [
    "def inner_product_tf_tensordot(vector1, vector2):\n",
    "    return tf.tensordot(vector1, vector2, axes=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffJdohPlSvwX"
   },
   "outputs": [],
   "source": [
    "def inner_product_torch(vector1, vector2):\n",
    "    vector1 = torch.Tensor(vector1)\n",
    "    vector2 = torch.Tensor(vector2)\n",
    "\n",
    "    return torch.sum(torch.mul(vector1, vector2)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoYtq2UFRZzd"
   },
   "source": [
    "##### ML Model Examples: Linear Regression, `keras.Dense`, `torch.nn.Linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WiPO_PfvO5Df"
   },
   "outputs": [],
   "source": [
    "def inner_product_sklearn(vector1, vector2):\n",
    "    regression = sklearn.linear_model.LinearRegression(\n",
    "      fit_intercept=False)\n",
    "\n",
    "    regression.coef_ = np.array(vector1)\n",
    "    regression.intercept_ = 0.\n",
    "\n",
    "    return regression.predict(np.atleast_2d(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9ea0NuCNf2S"
   },
   "outputs": [],
   "source": [
    "def inner_product_keras(vector1, vector2):\n",
    "    N = len(vector1)\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "      keras.layers.Dense(\n",
    "          1, activation=None, use_bias=False, input_shape=(N,),\n",
    "          kernel_initializer=keras.initializers.Constant(vector1)))\n",
    "\n",
    "    return model.predict([vector2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0vfG9ljU802"
   },
   "outputs": [],
   "source": [
    "def inner_product_torchnn(vector1, vector2):\n",
    "    N = len(vector1)\n",
    "    layer = torch.nn.Linear(N, 1, bias=False)\n",
    "    layer.weight = torch.nn.Parameter(torch.Tensor(vector2))\n",
    "\n",
    "    return layer(torch.Tensor(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rzuDHIeXipj"
   },
   "source": [
    "##### ML Calculation Examples: Norms and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkyJtxUIY-Te"
   },
   "source": [
    "$\\|x\\|_2 = \\sqrt{\\mathrm{dot}(x, x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mS_6uQpQXqzO"
   },
   "outputs": [],
   "source": [
    "def norm(vector):\n",
    "    return np.sqrt(inner_product(vector, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeC0C8leXzKe"
   },
   "outputs": [],
   "source": [
    "def normalize(vector):\n",
    "    vector_norm = norm(vector)\n",
    "    return [element / vector_norm for element in vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TPK5ORXaBYL"
   },
   "source": [
    "$$\\cos(\\angle_{x, y}) = \\frac{\\mathrm{dot}(x, y)}{\\|x\\|_2\\cdot\\|y\\|_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a15Tpk9wXmyG"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    return inner_product(normalize(vector1), normalize(vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mP9Zv0IabFTX"
   },
   "source": [
    "#### Outer Product aka Tensor Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgJ73_psbSBA"
   },
   "source": [
    "$$\n",
    "\\mathrm{outer} \\colon \\mathbb{R}^{n \\times 1} \\times \\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{n \\times n}  \\\\\n",
    "\\mathrm{outer}(x, y)_{i, j} = x_i \\cdot y_j \\\\ \n",
    "\\mathrm{outer}(x, y) = \\mathrm{matmul}(x, y^\\top) = x y^\\top \\\\\n",
    "\\mathrm{outer}(x, y) = x \\otimes y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVHNppB0cxvX"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkkTXlUwbJYl"
   },
   "outputs": [],
   "source": [
    "def outer_product(vector1, vector2):\n",
    "    result = []\n",
    "    for ii, element1 in enumerate(vector1):\n",
    "        subresult = []\n",
    "    for jj, element2 in enumerate(vector2):\n",
    "        subresult.append(element1 * element2)\n",
    "    result.append(subresult)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qL6lk8_cbMr"
   },
   "outputs": [],
   "source": [
    "outer_product(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqjH-cNDciug"
   },
   "source": [
    "##### Numpy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6Mlrmn-c6z_"
   },
   "outputs": [],
   "source": [
    "def outer_product_numpy(vector1, vector2):\n",
    "    return np.outer(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JLeYkL6XdFfb"
   },
   "outputs": [],
   "source": [
    "def outer_product_numpy_matmul(vector1, vector2):\n",
    "    vector1 = np.atleast_2d(vector1)\n",
    "    vector2 = np.atleast_2d(vector2)\n",
    "\n",
    "    return vector1.T @ vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jlDMb6WQdZ63"
   },
   "outputs": [],
   "source": [
    "outer_product_numpy_matmul(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQvtXRy4ikzd"
   },
   "source": [
    "### Matrix-Vector Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8tj7fcAU0d3"
   },
   "source": [
    "#### Matrix-Vector Multiplication aka Applying Matrices as Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HadJusUWTzfN"
   },
   "source": [
    "Usually, we think of a matrix as a piece of _data_,\n",
    "but we can also think of it as a _function_.\n",
    "\n",
    "When we think of it as a piece of _data_,\n",
    "we often write it mathematically as a\n",
    "\"member of $\\mathbb{R}^{m\\times n}$, the set of arrays\n",
    "with $m$ rows and $n$ columns\", like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0xaG9YGfVci"
   },
   "source": [
    "$$X \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "As a function, we would write it as \"a function that takes in arrays with $n$ entries and returns arrays with $m$ entries\", or:\n",
    "\n",
    "$$X \\colon \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FET9lA0hfuN5"
   },
   "source": [
    "It's useful to think of some corner cases:\n",
    "what if $m == n == 1$?\n",
    "\n",
    "Then we might have\n",
    "$$ 2 \\in \\mathbb{R}$$\n",
    "or\n",
    "$$ 2 \\colon \\mathbb{R} \\rightarrow \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Qzf-LDFgdIh"
   },
   "source": [
    "where $2(x) = 2x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HlPXEEGUeUi"
   },
   "source": [
    "As indicated by this example,\n",
    "the right notion of what it means to \"apply\" a matrix as a function\n",
    "to a vector must be somehow like mutliplication.\n",
    "\n",
    "In fact, the right method isn't exactly multiplication as we are used to it,\n",
    "because the inputs and outputs aren't the same shape,\n",
    "but it's close enough that it goes by the same name:\n",
    "matrix-vector multiplication.\n",
    "\n",
    "Its type signature and definition are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCt7ys1Di_Eg"
   },
   "source": [
    "$$\\mathrm{matvec}(X, y) \\colon \\mathbb{R}^{m \\times n} \\times \\mathbb{R}^{n \\times 1} \\rightarrow \\mathbb{R}^{m \\times 1}\\\\\n",
    "\\mathrm{matvec}(X, y) = \\mathrm{matmul}(X, y) = Xy$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19KemrmGjmmV"
   },
   "source": [
    "$$\n",
    "\\mathrm{matvec}(X, y)_i = \\sum_j X_{i,j} y_j\\\\\n",
    "\\mathrm{matvec}(X, y)_i = {X_{i, \\colon}} y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEsfNSGdPR0T"
   },
   "source": [
    "$$\n",
    "\\mathrm{matvec}(X, y) = \\sum_j {X_{\\colon, j}} y_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvLeej95irfN"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UahTVtGkdH8"
   },
   "outputs": [],
   "source": [
    "def matvec(matrix, vector):\n",
    "    result = []\n",
    "    for row in matrix:\n",
    "        result.append(inner_product(row, vector))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEmpnnKDMGL4"
   },
   "outputs": [],
   "source": [
    "def matvec_sum(matrix, vector):\n",
    "    result = []\n",
    "    for ii, row in enumerate(matrix):\n",
    "        result_ii = 0\n",
    "        for jj, element in enumerate(vector):\n",
    "            result_ii += row[jj] * element \n",
    "        result.append(result_ii)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBD7469gkulZ"
   },
   "outputs": [],
   "source": [
    "M = 3\n",
    "matrix = [[random.normalvariate(0, 1) for _ in range(N)] for _ in range(M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcD9Udz3ktM4"
   },
   "outputs": [],
   "source": [
    "matvec(matrix, vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1YcRnXyNB-Z"
   },
   "outputs": [],
   "source": [
    "matvec_sum(matrix, vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "93OAhM_Ok9sA"
   },
   "outputs": [],
   "source": [
    "np.dot(matrix, vector1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Esyf8eKJLd3n"
   },
   "source": [
    "##### Numpy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OeN9W2mNfZg"
   },
   "outputs": [],
   "source": [
    "def matvec_numpy(matrix, vector):\n",
    "    result = np.zeros_like(np.array(matrix)[:, 0])\n",
    "    for ii, row in enumerate(matrix):\n",
    "        result[ii] = np.sum(np.multiply(row, vector))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNSlz-LtOwze"
   },
   "outputs": [],
   "source": [
    "def matvec_numpy_alternate(matrix, vector):\n",
    "    matrix = np.array(matrix)\n",
    "    result = np.zeros_like(matrix[:, 0])\n",
    "\n",
    "    for jj, column in enumerate(matrix.T):\n",
    "        result += vector[jj] * column\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViH6AUi3lBPA"
   },
   "outputs": [],
   "source": [
    "def matvec_numpy_matmul(matrix, vector):\n",
    "    return np.array(matrix) @ np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOkWFWvwXSvQ"
   },
   "source": [
    "##### Tensorflow and Torch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6FioM6qUXhh5"
   },
   "outputs": [],
   "source": [
    "def matvec_tf(matrix, vector):\n",
    "    return tf.reduce_sum(tf.multiply(matrix, vector), axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RY6zZjuKYFT5"
   },
   "outputs": [],
   "source": [
    "def matvec_tf_tensordot(matrix, vector):\n",
    "    return tf.tensordot(matrix, vector, axes=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwzzQnjLhlU5"
   },
   "source": [
    "Notice that this `tensordot` based implementation is identical to the `inner_product_tf` implementation with `tensordot`. That's because both are just implementing `matmul`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6mf-i-AY9Eq"
   },
   "outputs": [],
   "source": [
    "def matvec_torch(matrix, vector):\n",
    "    matrix = torch.Tensor(matrix)\n",
    "    vector = torch.Tensor(vector)\n",
    "\n",
    "    return torch.sum(torch.mul(matrix, vector), dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G91rcKr8a9TJ"
   },
   "source": [
    "##### ML Model Examples: Multiple Linear Regression, `keras.Dense`, `torch.nn.Linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekUPBPo3heQi"
   },
   "source": [
    "Note similarity to dot products above.\n",
    "Not a coincidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoDOCdJsaekh"
   },
   "outputs": [],
   "source": [
    "def matvec_sklearn(matrix, vector):\n",
    "    regression = sklearn.linear_model.LinearRegression(\n",
    "      fit_intercept=False)\n",
    "\n",
    "    regression.coef_ = np.array(matrix)\n",
    "    regression.intercept_ = 0.\n",
    "\n",
    "    return regression.predict(np.atleast_2d(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "As09KdXkbMqy"
   },
   "outputs": [],
   "source": [
    "def matvec_keras(matrix, vector):\n",
    "    N, M = len(matrix), len(matrix[0])\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "      keras.layers.Dense(\n",
    "          N, activation=None, use_bias=False, input_shape=(M,),\n",
    "          kernel_initializer=keras.initializers.Constant(np.array(matrix).T)))\n",
    "\n",
    "    return model.predict([vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDIAbUjfg-6p"
   },
   "source": [
    "Gotcha: `Keras` does all of its matrix multiplications \"from the right\",\n",
    "as in $y^\\top X$ rather than $Xy$,\n",
    "so we need to use the transpose here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJGg2vk3bPRP"
   },
   "outputs": [],
   "source": [
    "def matvec_torchnn(matrix, vector):\n",
    "    N, M = len(matrix), len(matrix[0])\n",
    "    layer = torch.nn.Linear(N, M, bias=False)\n",
    "    layer.weight = torch.nn.Parameter(torch.Tensor(matrix))\n",
    "\n",
    "    return layer(torch.Tensor(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUH035ymisyY"
   },
   "source": [
    "### Matrix-Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_3uZoUqwMiE"
   },
   "source": [
    "#### Matrix-Matrix Multiplication aka Composing Matrices as Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jP9bmX2ZOVdu"
   },
   "source": [
    "One of the most important features of functions is that they can be **composed**:\n",
    "one function can be applied after another, the output of one used as the input of the next.\n",
    "\n",
    "To compose two functions $f$ and $g$,\n",
    "where\n",
    "\n",
    "$$\n",
    "f\\colon A \\rightarrow B\n",
    "$$\n",
    "and\n",
    "$$\n",
    "g\\colon B \\rightarrow C\n",
    "$$\n",
    "we match the output of one to the inputs of the other,\n",
    "resulting in a new single function,\n",
    "denoted\n",
    "$$\n",
    "g \\circ f \\colon A \\rightarrow C\n",
    "$$\n",
    "where\n",
    "$$\n",
    "g \\circ f(x) = g\\left(f(x)\\right)\n",
    "$$\n",
    "\n",
    "This might be pronounced \"$g$ after $f$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv8RJ-QziyQZ"
   },
   "source": [
    "Similarly, when we combine, or compose, two matrices $X$ and $Y$,\n",
    "the result is a new matrix, $XY$,\n",
    "which we obtain by multiplying the two matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HiLsQqsxLHdo"
   },
   "source": [
    "$$\\mathrm{matmul(X, Y)} \\colon \\mathbb{R}^{n \\times m} \\times \\mathbb{R}^{m \\times k} \\rightarrow \\mathbb{R}^{n \\times k}\\\\\n",
    "\\mathrm{matmul}(X, Y) = XY = X \\circ Y$$\n",
    "$$\n",
    "\\mathrm{matmul}(X, Y)_{ij} = \\mathrm{matmul}(X_{i, :}, Y_{:, j})\n",
    "$$\n",
    "$$\n",
    "\\mathrm{matmul}(X, Y)_{ij} = \\sum_r X_{i, r} Y_{r, j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OikYDHfqyr3U"
   },
   "outputs": [],
   "source": [
    "N, M, K = 2, 3, 4\n",
    "matrix1 = [[random.normalvariate(0, 1) for _ in range(M)] for _ in range(N)]\n",
    "matrix2 = [[random.normalvariate(0, 1) for _ in range(K)] for _ in range(M)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Y62sVtpv7f0"
   },
   "source": [
    "##### Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSLLDJ9iivxp"
   },
   "outputs": [],
   "source": [
    "def matmul(matrix1, matrix2):\n",
    "    result_array = []\n",
    "    for ii, row in enumerate(matrix1):\n",
    "        result_row = []\n",
    "        for jj in range(len(matrix2[0])):\n",
    "            result = 0\n",
    "            for kk in range(len(matrix2)):\n",
    "                result += matrix1[ii][kk] * matrix2[kk][jj]\n",
    "            result_row.append(result)\n",
    "        result_array.append(result_row)\n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBFpRDvRn96S"
   },
   "outputs": [],
   "source": [
    "matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6XqzwF6wZ_T"
   },
   "source": [
    "##### Numpy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cTWE6aHlwcuS"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy(matrix1, matrix2):\n",
    "    return np.array(matrix1) @ np.array(matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly-5EiDnwhZ8"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy_dot(matrix1, matrix2):\n",
    "    return np.dot(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBKr6Mjwwmwb"
   },
   "outputs": [],
   "source": [
    "def matmul_numpy_matmul(matrix1, matrix2):\n",
    "    return np.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9MxeNjqhw0wo"
   },
   "outputs": [],
   "source": [
    "matmul_numpy(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-26dpoNx1ux"
   },
   "outputs": [],
   "source": [
    "matmul_numpy_dot(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0affHd0Lx3s7"
   },
   "outputs": [],
   "source": [
    "matmul_numpy_matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCfStwcPy43U"
   },
   "source": [
    "##### Tensorflow and PyTorch Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9749vRtP0RHS"
   },
   "outputs": [],
   "source": [
    "def matmul_tf(matrix1, matrix2):\n",
    "    return tf.matmul(matrix1, matrix2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yMPWF13zHOk"
   },
   "outputs": [],
   "source": [
    "def matmul_tf_tensordot(matrix1, matrix2):\n",
    "    return tf.tensordot(matrix1, matrix2, axes=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skWXzh7QzLFL"
   },
   "outputs": [],
   "source": [
    "matmul_tf_tensordot(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLlbzgb83Mr8"
   },
   "outputs": [],
   "source": [
    "def matmul_torch(matrix1, matrix2):\n",
    "    matrix1 = torch.Tensor(matrix1)\n",
    "    matrix2 = torch.Tensor(matrix2)\n",
    "\n",
    "    return torch.matmul(matrix1, matrix2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VD8oFk9v0fA4"
   },
   "outputs": [],
   "source": [
    "def matmul_torch_tensordot(matrix1, matrix2):\n",
    "    matrix1 = torch.Tensor(matrix1)\n",
    "    matrix2 = torch.Tensor(matrix2)\n",
    "\n",
    "    return torch.tensordot(matrix1, matrix2, dims=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TzC1e2A0t3M"
   },
   "outputs": [],
   "source": [
    "matmul_torch(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVvAQWST4RJO"
   },
   "outputs": [],
   "source": [
    "matmul_torch_tensordot(matrix1, matrix2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qOvkwqdHHKQ2",
    "K4j53wEQC0zr",
    "K9_uzirotQfB",
    "SHT_SNmveatz",
    "uyNkycl7i5BG",
    "a91oDtxhHgfk",
    "5TGAGS-PQo58",
    "bQvtXRy4ikzd",
    "G8tj7fcAU0d3",
    "LvLeej95irfN",
    "Esyf8eKJLd3n",
    "uOkWFWvwXSvQ",
    "G91rcKr8a9TJ",
    "BUH035ymisyY",
    "m_3uZoUqwMiE",
    "5Y62sVtpv7f0",
    "o6XqzwF6wZ_T"
   ],
   "name": "Colab - Math4ML I: Linear Algebra",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
