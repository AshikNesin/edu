{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercises.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLA3P7vvT5S0"
      },
      "source": [
        "# Math4ML Part II: Calculus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg3cotCfruZP"
      },
      "source": [
        "# Setup Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZUFT7sjT5S2"
      },
      "source": [
        "This section includes setup code for the remaining sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yfZuFH5T5S3"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade -qq wandb okpy==1.15.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q681SPoKT5S3"
      },
      "source": [
        "# importing from standard library\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# importing libraries\n",
        "import autograd\n",
        "import autograd.numpy as np  # trick for automatic differentiation with numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !git clone --branch \"math4ml/reorg\" \"https://github.com/wandb/edu.git\"\n",
        "    %cd \"edu/math-for-ml/02_calculus\"\n",
        "else:\n",
        "    pass\n",
        "\n",
        "if \"../\" not in sys.path:\n",
        "    sys.path.append(\"../\")\n",
        "\n",
        "# importing course-specific modules\n",
        "import autograder\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKSRMd-lrVUG"
      },
      "source": [
        "## Section 1. How does Gradient Descent Behave?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raKbpC3RT5S4"
      },
      "source": [
        "In general, machine learning models depend on more than one parameter,\n",
        "and so to understand the behavior of gradient descent,\n",
        "we need to consider loss functions with multiple input dimensions,\n",
        "also known as loss *surfaces*.\n",
        "\n",
        "To draw a loss surface, we plot the value of the loss function at each combination of values for the inputs. Because we're able to, at most, make things in 3 dimensions, we'll have two input dimensions and leave the third for the loss.\n",
        "\n",
        "A surface is one way to generalize the familiar old idea of the graph of a function to functions that take more than one input.\n",
        "\n",
        "We live on a surface of this type, the surface of the Earth. If you want to think of it as a loss function, you could think of it as the loss function you'd have if you didn't like being at a high altitude, as a function of your latitude and longitude. Consider: what point on Earth would optimize this loss function?\n",
        "\n",
        "Let's visualize some surfaces that are closer to what you might see if you plotted the loss surface for a machine learning model you were optimizing. A list of loss functions is defined in the cells below, first in Python and then, for some, in mathematical notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9OVnXo9T5S4"
      },
      "source": [
        "scale = 1\n",
        "\n",
        "losses = [lambda x,y: np.square(x) + np.square(y),\n",
        "          lambda x,y: np.square(x) + 0.1 * np.square(y),\n",
        "          lambda x,y: 3 * np.square(x) + 0.1 * np.square(y),\n",
        "          lambda x,y: np.cos(3 * x) + 0.5 * (np.square(x) + np.square(y) + x),\n",
        "          lambda x,y: np.cos(3 * x) + np.square(x)+np.square(y),\n",
        "          lambda x,y: np.where(np.abs(x)+np.abs(y)<0.75,0,np.abs(x)+np.abs(y)-0.75),\n",
        "          lambda x,y: np.where(x>y+0.25,1.25,y)+np.square(x)+np.square(y),\n",
        "          lambda x,y: 0.1 * np.random.standard_normal(size=x.shape),\n",
        "          lambda x,y: utils.surfaces.gauss_random_field(x, y, scale)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUvrbd2qT5S5"
      },
      "source": [
        "$$\n",
        "\\begin{align}\n",
        "    l_0(x, y) &= x^2 + y^2\\\\\n",
        "    l_1(x, y) &= x^2 + 0.1 \\cdot y^2\\\\\n",
        "    l_2(x, y) &= 3x^2+ 0.1 \\cdot y^2\\\\\n",
        "    l_3(x, y) &= \\cos(3x) + 0.5 \\cdot (x^2 + y^2 + x)\\\\\n",
        "    l_4(x, y) &= \\cos(3x) + x^2 + y^2\\\\\n",
        "    l_5(x, y) &= \\left\\{\\begin{array}{rl}\n",
        "            \\|x\\|+ \\|y\\| - 0.75, & \\text{if } \\|x\\|+ \\|y\\| > 0.75\\\\\n",
        "            0, & \\text{otherwise }\n",
        "            \\end{array}\\right.\\\\\n",
        "    l_6(x, y) &= x^2 +y^2 + \\left\\{\\begin{array}{rl}\n",
        "            1.25, & \\text{if } x > y + 0.25\\\\\n",
        "            y, & \\text{otherwise }\n",
        "            \\end{array}\\right.\\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXQctKXYT5S5"
      },
      "source": [
        "The next cell produces a 3-D plot of a single loss surface, chosen by indexing into the `losses` list.\n",
        "\n",
        "The plots are interactive, to the extent that you can change the perspective. You can rotate with by clicking and dragging the left mouse button and zoom by doing the same with the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Ue938sT5S5"
      },
      "source": [
        "loss = losses[0]\n",
        "\n",
        "N = 50\n",
        "\n",
        "mesh_extent = 1.5\n",
        "\n",
        "utils.surfaces.plot_loss_surface(loss, N, mesh_extent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0iQYNdT5S5"
      },
      "source": [
        "The following questions will ask you to visualize these loss surfaces and answer questions about them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umt3KJxxT5S6"
      },
      "source": [
        "For certain problems, gradient descent performs nicely. View `losses[0]`.\n",
        "\n",
        "#### Q Pick a few different starting points and follow the direction of steepest descent. Where do you end up?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9FdEtIrT5S6"
      },
      "source": [
        "#### Q What's nice about this loss surface?\n",
        "*This question might be easier to answer once you've seen some of the other loss surfaces.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcFjXhET5S6"
      },
      "source": [
        "When we follow gradients numerically, using a computer, we have to pick a scale for the \"size\" of steps we take. This can cause problems we might not anticipate with a view of gradient descent based on physical intuition.\n",
        "\n",
        "View `losses[1]` and then `losses[2]`.\n",
        "\n",
        "#### Q Why might picking a size of step cause issues with the surface `losses[1]` or `losses[2]`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wANH1-gLT5S6"
      },
      "source": [
        "Other problems can't be solved by gradient descent effectively. Select `losses[3]`.\n",
        "\n",
        "#### Q Again, select multiple different starting points and follow the direction of steepest descent. What's different in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4AEa6ztT5S7"
      },
      "source": [
        "#### Q Why might this change in the behavior of gradient descent be a bad thing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFqkVgXT5S7"
      },
      "source": [
        "There are several similar cases to the above issue that are of interest. View `losses[4]` and then `losses[5]`.\n",
        "\n",
        "#### Q Compare and contrast loss surfaces `3`, `4`, and `5`. Which ones cause issues for optimization? Explain your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpm4EwvxT5S7"
      },
      "source": [
        "Some issues are more theoretical than practical. View `losses[6]`.\n",
        "\n",
        "#### Q Can we still do gradient descent on this loss surface? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZnnVx5iT5S8"
      },
      "source": [
        "For some loss functions, the right method of minimization can be hard to decide. View `losses[7]`.\n",
        "\n",
        "On this function, the value at each point is random and independent of the value at all other points.\n",
        "\n",
        "#### Q What strategies might you use to minimize this loss function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_jWOZWvT5S8"
      },
      "source": [
        "The loss surfaces for things like \"a neural network that maps [pictures of horses to almost identical pictures of zebras](https://github.com/junyanz/CycleGAN)\" are expected to be much more complicated than the ones we've looked at so far. The next class of loss functions is a simplified model of a neural network loss.\n",
        "\n",
        "View `losses[8]` with the parameters `N = 100`, `mesh_extent = 10`, and `scale = 1` in the plotting and loss definition cells (original values are `50`, `1.5`, and `1`, for reference).\n",
        "\n",
        "You can also attempt to increase these values to `N = 150`, `mesh_extent = 25`, and `scale = 2`. With these settings, the plot will take a bit of time to render on most machines and the plot will lag when interacted with. On some machines, this may consume too much memory and cause the plot to not render. If the plot successfully renders and you'd like to see more, increase the parameters to `N = 250`, `mesh_extent = 50`, and `scale = 3`.\n",
        "\n",
        "#### Q Do these loss surfaces look like promising candidates for gradient descent? Why or why not?"
      ]
    }
  ]
}