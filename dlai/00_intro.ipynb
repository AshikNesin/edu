{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfb9335",
   "metadata": {},
   "source": [
    "# Introduction to W&B\n",
    "\n",
    "We will add `wandb` to sprite classification model training, so that we can track and visualize important metrics, gain insights into our model's behavior and make informed decisions for model improvements. We will also see how to compare and analyze different experiments, collaborate with team members, and reproduce results effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba792c-2baa-4c19-a132-2ed82a759e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bfcc9",
   "metadata": {},
   "source": [
    "### Sprite classification\n",
    "\n",
    "We will build a simple model to classify sprites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 * 16 * 16\n",
    "OUTPUT_SIZE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_WORKERS = 2\n",
    "CLASSES = [\"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\"]\n",
    "DATA_DIR = Path('./data/')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "\n",
    "def get_model(dropout):\n",
    "    \"Simple MLP with Dropout\"\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "        nn.BatchNorm1d(HIDDEN_SIZE),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f739c-d7ef-4954-ae87-d5bdd6bf25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a config object to store our hyperparameters\n",
    "config = SimpleNamespace(\n",
    "    epochs = 2,\n",
    "    batch_size = 128,\n",
    "    lr = 1e-5,\n",
    "    dropout = 0.5,\n",
    "    slice_size = 10_000,\n",
    "    valid_pct = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5492ebb-2dfa-44ce-af6c-24655e45a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"Train a model with a given config\"\n",
    "    # Start a wandb run\n",
    "    wandb.init(\n",
    "        project=\"dlai-intro\",\n",
    "        config=config,\n",
    "        anonymous=\"allow\",\n",
    "    )\n",
    "    # Get the data\n",
    "    train_dl, valid_dl = get_dataloaders(DATA_DIR, \n",
    "                                         config.batch_size, \n",
    "                                         config.slice_size, \n",
    "                                         config.valid_pct)\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "\n",
    "    # A simple MLP model\n",
    "    model = get_model(config.dropout)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    example_ct = 0\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs), total=config.epochs):\n",
    "        model.train()\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_dl):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            train_loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct += len(images)\n",
    "            metrics = {\n",
    "                \"train/train_loss\": train_loss,\n",
    "                \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch))/n_steps_per_epoch,\n",
    "                \"train/example_ct\": example_ct\n",
    "            }\n",
    "            # Log train metrics to wandb\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "        # Compute validation metrics, log images on last epoch\n",
    "        val_loss, accuracy = validate_model(model, valid_dl, loss_func, \n",
    "                                            log_images=(epoch == (config.epochs - 1)))\n",
    "\n",
    "        # Log train and validation metrics to wandb\n",
    "        val_metrics = {\n",
    "            \"val/val_loss\": val_loss,\n",
    "            \"val/val_accuracy\": accuracy\n",
    "        }\n",
    "        # Log validation metrics to wandb\n",
    "        wandb.log(val_metrics)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):\n",
    "    \"Compute the performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (images, labels) in enumerate(valid_dl):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            val_loss += loss_func(outputs, labels) * labels.size(0)\n",
    "\n",
    "            # Compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Log one batch of images to the dashboard, always same batch_idx.\n",
    "            if i == batch_idx and log_images:\n",
    "                log_image_predictions_table(images, predicted, labels, outputs.softmax(dim=1))\n",
    "\n",
    "    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
    "\n",
    "def log_image_predictions_table(images, predicted, labels, probs):\n",
    "    \"Create a wandb Table to log images, labels, and predictions\"\n",
    "    columns = [\"image\", \"pred\", \"target\"] + [f\"score_{i}\" for i in range(OUTPUT_SIZE)]\n",
    "    table = wandb.Table(columns=columns)\n",
    "    \n",
    "    for img, pred, targ, prob in zip(images.cpu(), predicted.cpu(), labels.cpu(), probs.cpu()):\n",
    "        table.add_data(wandb.Image(img), CLASSES[pred], CLASSES[targ], *prob.numpy())\n",
    "    \n",
    "    wandb.log({\"predictions_table\": table}, commit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cac7d2",
   "metadata": {},
   "source": [
    "### W&B account\n",
    "[Sign up](https://wandb.ai/site) for a free account at https://wandb.ai/site and then login to your wandb account to store the results of your experiments and use advanced W&B features. You can also continue to learn in anonymous mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c37e2-7ff5-46a6-afb7-b80cb69f7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.login() # uncomment if you want to login to wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df2485",
   "metadata": {},
   "source": [
    "### Train model\n",
    "Let's train the model with default config and check how it's doing in W&B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423c964-f7e3-4d3b-8a24-e70f7f4414c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8af6b3-fdec-4f46-90b4-28585257e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.epochs = 3\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecf01d",
   "metadata": {},
   "source": [
    "Let's try other values of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f7b8d-216c-4b51-a389-eaae195e5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.epochs = 1\n",
    "config.lr = 1e-4\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83ea0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
