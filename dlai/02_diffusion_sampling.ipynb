{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958524a2-cb56-439e-850e-032dd10478f2",
   "metadata": {},
   "source": [
    "# Sampling from a diffusion model\n",
    "In this notebooks we will sampled from the previously trained diffusion model.\n",
    "- We are going to compare the samples from DDPM and DDIM samplers\n",
    "- Visualize mixing samples with conditional diffusion models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700e687c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "# Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c3a942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wandb Params\n",
    "project = \"debug_dlai\"\n",
    "entity = \"capecape\"\n",
    "\n",
    "# ddpm sampler hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "num_samples = 32\n",
    "height = 16\n",
    "ddim_n = 25\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "# we are storing the parameters in a dictionary to be logged to wandb\n",
    "config = dict(\n",
    "    timesteps=timesteps,\n",
    "    beta1=beta1,\n",
    "    beta2=beta2,\n",
    "    num_samples=num_samples,\n",
    "    height=height,\n",
    "    ddim_n=ddim_n,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb43f98f",
   "metadata": {},
   "source": [
    "We will load the model from a wandb.Artifact and set up the sampling loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ab66255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_artifact_name):\n",
    "    \"Load the model from wandb artifacts\"\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(model_artifact_name, type=\"model\")\n",
    "    model_path = artifact.download()\n",
    "\n",
    "    # recover model info from the registry\n",
    "    producer_run = artifact.logged_by()\n",
    "\n",
    "    # load the weights dictionary\n",
    "    model_weights = torch.load(model_path + f\"/context_model_31.pth\", map_location=\"cpu\")\n",
    "\n",
    "    # create the model\n",
    "    model = ContextUnet(in_channels=3, \n",
    "                        n_feat=producer_run.config[\"n_feat\"], \n",
    "                        n_cfeat=producer_run.config[\"n_cfeat\"], \n",
    "                        height=producer_run.config[\"height\"])\n",
    "    \n",
    "    # load the weights into the model\n",
    "    model.load_state_dict(model_weights)\n",
    "\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47633e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "nn_model = load_model('capecape/dlai_diffusion/w1r7jpji_context_model:v8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe8eb277",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d92c52-8a11-450c-bc78-ffa221af2fa3",
   "metadata": {},
   "source": [
    "We will sample and log the generated samples to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f479d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b0f5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16085a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with context using standard algorithm\n",
    "# we make a change to the original algorithm to allow for context and passing a noise tensor (samples)\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_context(samples, context, save_rate=20):\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t, ctx)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate==0 or i==timesteps or i<8:\n",
    "            print(f'sampling timestep {i:3d}', end='\\r')\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples.clip(-1, 1), intermediate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00b9ef16-1848-476d-a9dd-09175b8f0e3c",
   "metadata": {},
   "source": [
    "Let's define a set of noises and a context vector to condition on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88afdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noise vector\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "noises = torch.randn(num_samples, 3, height, height).to(device)  \n",
    "\n",
    "# A fixed context vector to sample from\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4]), 5).to(device=device).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cbf9ef8-619a-4052-a138-a88c0f0f8b0b",
   "metadata": {},
   "source": [
    "Let's bring that faster DDIM sampler from the diffusion course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e12affd6-0caa-4e00-8499-c5a7495bc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sampling function for DDIM   \n",
    "# removes the noise using ddim\n",
    "def denoise_ddim(x, t, t_prev, pred_noise):\n",
    "    ab = ab_t[t]\n",
    "    ab_prev = ab_t[t_prev]\n",
    "    \n",
    "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
    "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
    "\n",
    "    return x0_pred + dir_xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdfc048-47f0-43b5-983e-da715e1ed562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fast sampling algorithm with context\n",
    "@torch.no_grad()\n",
    "def sample_ddim_context(samples, context, n=25): \n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    step_size = timesteps // n\n",
    "    for i in range(timesteps, 0, -step_size):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
    "        intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples.clip(-1, 1), intermediate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aee10774-ff79-4df7-9b2d-1908561c23e5",
   "metadata": {},
   "source": [
    "Let's create a `wandb.Table` to store our generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d20ffa-552e-4836-8c98-7655ca92cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"input_noise\", \"ddpm\", \"ddim\", \"class\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90b838be-8fa1-4c12-9c4f-e40dfacc08e1",
   "metadata": {},
   "source": [
    "let's compute ddpm samples as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e24210-4885-4559-92e1-db10566ef5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep   1\r"
     ]
    }
   ],
   "source": [
    "ddpm_samples, _ = sample_ddpm_context(noises, ctx_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "836584a1-26b5-45b1-98c9-0c45d639c8f9",
   "metadata": {},
   "source": [
    "For DDIM we can control the step size by the `n` param:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b07c26-0ac2-428a-8351-34f8b7228074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep  20\r"
     ]
    }
   ],
   "source": [
    "ddim_samples, _ = sample_ddim_context(noises, ctx_vector, n=ddim_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af33d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_to_classes(ctx_vector):\n",
    "    classes = \"hero,non-hero,food,spell,side-facing\".split(\",\")\n",
    "    return [classes[i] for i in [ctx_vector[i].argmax().item() for i in range(ctx_vector.shape[0])]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daea8275-0356-452e-a9f9-2824ef53f1ea",
   "metadata": {},
   "source": [
    "Let's keep track of the sampling params on a dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85be303d-0f0b-4df4-8c87-bd1bfb6145a2",
   "metadata": {},
   "source": [
    "We can add the rows to the table one by one, we also cast images to `wandb.Image` so we can render them correctly in the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "481afea1-ae53-4b5b-a3db-1d49be0733a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise, ddpm_sample, ddim_sample, c in zip(noises, ddpm_samples, ddim_samples, ctx_to_classes(ctx_vector)):\n",
    "    table.add_data(wandb.Image(noise), \n",
    "                   wandb.Image(ddpm_sample), \n",
    "                   wandb.Image(ddim_sample),\n",
    "                   c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "987cee86-2db1-4a2a-9d14-f70c6248ecb9",
   "metadata": {},
   "source": [
    "we log the table to W&B, we can also use `wandb.init` as a context manager, this way we ensure that the run is finished when exiting the manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc7a2ca-ae05-4462-9ae3-82eb1a6dbc27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/dlai/wandb_diffusion/wandb/run-20230704_142746-xfz2uh0q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/debug_dlai/runs/xfz2uh0q' target=\"_blank\">winter-feather-1</a></strong> to <a href='https://wandb.ai/capecape/debug_dlai' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/debug_dlai' target=\"_blank\">https://wandb.ai/capecape/debug_dlai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/debug_dlai/runs/xfz2uh0q' target=\"_blank\">https://wandb.ai/capecape/debug_dlai/runs/xfz2uh0q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-feather-1</strong> at: <a href='https://wandb.ai/capecape/debug_dlai/runs/xfz2uh0q' target=\"_blank\">https://wandb.ai/capecape/debug_dlai/runs/xfz2uh0q</a><br/>Synced 6 W&B file(s), 1 media file(s), 97 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230704_142746-xfz2uh0q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=project, entity=entity, job_type=\"samplers_battle\", config=config):\n",
    "    wandb.log({\"samplers_tables\":table})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a974258a-55fb-43ef-9136-985ec85bc3fc",
   "metadata": {},
   "source": [
    "## Mixing classes during sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68987e12-22d1-4c40-b0d2-b33f6397c34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling timestep   1\r"
     ]
    }
   ],
   "source": [
    "ctx = torch.tensor([\n",
    "    # hero, non-hero, food, spell, side-facing\n",
    "    [1,0,0,0,0],      #human\n",
    "    [1,0,0.6,0,0],    \n",
    "    [0,0,0.6,0.4,0],  \n",
    "    [1,0,0,0,1],  \n",
    "    [1,1,0,0,0],\n",
    "    [1,0,0,1,0]\n",
    "]).float().to(device)\n",
    "\n",
    "# let's pass the same noise everytime\n",
    "samples = torch.cat([torch.randn(1, 3, height, height)]*6, axis=0).to(device)  \n",
    "ddpm_samples, _ = sample_ddpm_context(samples, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "626ef616-dae4-4417-9219-d67ef0794e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "hero_table = wandb.Table(columns=[\"generation\", \"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbeb2d18-feb9-4452-b368-3f7f03b1715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, c in zip(ddpm_samples, ctx.cpu().numpy().tolist()):\n",
    "    hero_table.add_data(wandb.Image(s), *c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e196f2b-4d13-4cc2-a380-2f23530bee1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tcapelle/work/dlai/wandb_diffusion/wandb/run-20230704_142806-sv5fvps1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/debug_dlai/runs/sv5fvps1' target=\"_blank\">summer-spaceship-2</a></strong> to <a href='https://wandb.ai/capecape/debug_dlai' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/debug_dlai' target=\"_blank\">https://wandb.ai/capecape/debug_dlai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/debug_dlai/runs/sv5fvps1' target=\"_blank\">https://wandb.ai/capecape/debug_dlai/runs/sv5fvps1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-spaceship-2</strong> at: <a href='https://wandb.ai/capecape/debug_dlai/runs/sv5fvps1' target=\"_blank\">https://wandb.ai/capecape/debug_dlai/runs/sv5fvps1</a><br/>Synced 6 W&B file(s), 1 media file(s), 7 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230704_142806-sv5fvps1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=project, entity=entity, job_type=\"sampling_mix\", config=config):\n",
    "    wandb.log({\"hero_table\":hero_table})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
