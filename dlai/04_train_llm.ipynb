{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0ed738-872b-4952-baf8-b3f7c92214a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64e095c-d28b-44aa-a122-4121c5c66a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd80268-c4a1-4e1a-aed3-cd5c3ab4d48f",
   "metadata": {},
   "source": [
    "Load a dataset from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7535b8b-d220-44e8-a56c-97e250c36596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/tcapelle/.cache/huggingface/datasets/MohamedRashad___parquet/MohamedRashad--characters_backstories-6398ba4bb1a6e421/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899f1c4acc1a40d19459e9323bc75960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('MohamedRashad/characters_backstories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78879ff2-7eca-4b57-83f8-00b203f9e65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'target'],\n",
       "        num_rows: 2322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58c980-50ce-4d57-8d58-13a4bbda7249",
   "metadata": {},
   "source": [
    "As this dataset has no validation split, we will create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dae9106-8015-43da-a6d9-1124dee4bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2286ae41-213d-480d-a4ba-8c4e2e1c4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roneneldan/TinyStories-33M\"  # distilgpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26dfa0b7-8d9f-44f3-9e09-bc12bcb5ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7a79d-9519-4133-a8cd-0a2bc59ee97b",
   "metadata": {},
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the map method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea05869-8ece-4a82-b9d4-3a62a84b6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413ebca-019b-49dc-b042-cf3cb20bf26c",
   "metadata": {},
   "source": [
    "Then we apply it to all the splits in our `datasets` object, using `batched=True` and 4 processes to speed up the preprocessing. We won't need the `text` column afterward, so we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ce65c5f-8227-4c41-9e96-dfc80de611be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Generate Backstory based on following information\\nCharacter Name: Vaskir Tempus\\nCharacter Race: Yuan-ti abomination\\nCharacter Class: Paladin of treachery/ goo bladelock\\n\\nOutput:\\n',\n",
       " 'target': 'Vaskir is an exiled yuan ti who forsook his religion to worship the great old one dendar. he is a master swordsman who wields a greatsword in tandem with a longsword, effectively dualwielding the huge blade. He is chaotic evil, believing that government and law holds back all of humanity from their goals, keeping them oppressed and subjugated under the foot of the highest ruler'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][232]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0f6c6-8b87-4dc7-b138-f3c9d6cac163",
   "metadata": {},
   "source": [
    "we want to grab the characters backstories in the `target` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22368c91-ddf8-4b08-848e-f732ff155494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2264 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2812 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2573 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2952 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2661 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4725 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2464 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3121 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = ds.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59cc8a9-5f87-4eb7-abbc-f4fc18fea51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][1][\"input_ids\"][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70bc12ae-52dc-47ad-b9ef-1e5b8af829e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85c6ad00-3825-4f12-be49-8ff336d5d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a73750-5e38-4236-a5c3-b356d8041dc3",
   "metadata": {},
   "source": [
    "First note that we duplicate the inputs for our labels. This is because the model of the ðŸ¤— Transformers library apply the shifting to the right, so we don't need to do it manually.\n",
    "\n",
    "Also note that by default, the `map` method will send a batch of 1,000 examples to be treated by the preprocessing function. So here, we will drop the remainder to make the concatenated tokenized texts a multiple of `block_size` every 1,000 examples. You can adjust this behavior by passing a higher batch size (which will also be processed slower). You can also speed-up the preprocessing by using multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "572f29c8-84d3-45b9-b8df-26de8c22bc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/465 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4f131eb-979e-40f6-9e28-19756beaa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7345ab23-8d12-4d4c-a39d-bb2202bff218",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"tiny-stories-characters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d74ee155-3c30-4ef2-9c4d-fd8ee222c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-characters-backstories\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af62105f-a478-436f-88a2-5c1d78b9d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01958a56-c22a-4a27-bc71-41c59fc97f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m (\u001b[33mdeeplearning-ai-temp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/work/edu/dlai/wandb/run-20230706_130647-l0pa7ivo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeplearning-ai-temp/tiny-stories-characters/runs/l0pa7ivo' target=\"_blank\">dulcet-cherry-3</a></strong> to <a href='https://wandb.ai/deeplearning-ai-temp/tiny-stories-characters' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeplearning-ai-temp/tiny-stories-characters' target=\"_blank\">https://wandb.ai/deeplearning-ai-temp/tiny-stories-characters</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeplearning-ai-temp/tiny-stories-characters/runs/l0pa7ivo' target=\"_blank\">https://wandb.ai/deeplearning-ai-temp/tiny-stories-characters/runs/l0pa7ivo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='816' max='816' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [816/816 01:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.795800</td>\n",
       "      <td>5.142995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.887100</td>\n",
       "      <td>5.009582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.886900</td>\n",
       "      <td>4.998519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=816, training_loss=5.0707503650702686, metrics={'train_runtime': 92.5014, 'train_samples_per_second': 70.572, 'train_steps_per_second': 8.821, 'total_flos': 284203589566464.0, 'train_loss': 5.0707503650702686, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a247e01-35d0-454f-8b7e-5f24cdf66f33",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f16d43d-445f-4df5-8734-85584f95792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model\n",
    "device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7911e43f-f4ce-4855-9f68-662438af8d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The hero was half human and cat, his strenghts were\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "844802b9-0ffc-466e-bedb-d7b7c6f337de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  4293,   373,  2063,  1692,   290,  3797,    11,   465, 43071,\n",
       "           456,   912,   547]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0883650-ab62-49c9-88d8-7f8c4fdfb0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hero was half human and cat, his strenghts were the first to be. He was the only one who had a lot of power, and he was the only one who had a lot of power. He was a great wizard, and he was the only one who could do it. He was a great wizard, and he was the greatest wizard in the world. He was a great wizard, and he was the greatest wizard in the world. He was a great wizard, and he was the greatest wizard in the world. He was a great wizard, and he was the greatest wizard in the world. He was a great\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_length = 128, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
