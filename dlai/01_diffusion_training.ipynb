{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958524a2-cb56-439e-850e-032dd10478f2",
   "metadata": {},
   "source": [
    "# Training a Diffusion Model with W&B\n",
    "\n",
    "In this notebooks we will instrument the training of a diffusion model with W&B. We will use the Lab3 notebook and add:\n",
    "- Logging of the training loss\n",
    "- Sampling from the model during training and logging the samples to W&B\n",
    "- Saving the model checkpoints to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700e687c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "# Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c3a942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_samples = 32\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "# device = \"mps\"\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "data_dir = './data/'\n",
    "save_dir = './data/weights/'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3\n",
    "\n",
    "# we are storing the parameters in a dictionary to be logged to wandb\n",
    "config = dict(\n",
    "    num_samples=num_samples,\n",
    "    timesteps=timesteps,\n",
    "    beta1=beta1,\n",
    "    beta2=beta2,\n",
    "    device=device,\n",
    "    n_feat=n_feat,\n",
    "    n_cfeat=n_cfeat,\n",
    "    height=height,\n",
    "    save_dir=save_dir,\n",
    "    batch_size=batch_size,\n",
    "    n_epoch=n_epoch,\n",
    "    lrate=lrate,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb43f98f",
   "metadata": {},
   "source": [
    "All this is the same as the previous notebook, except for the addition of the context vector size n_cfeat. We will use this to condition the diffusion model on a context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a705d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    \n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc9001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c63b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprite shape: (89400, 16, 16, 3)\n",
      "labels shape: (89400, 5)\n"
     ]
    }
   ],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset.from_np(data_dir + \"sprites_1788_16x16.npy\", data_dir + \"sprite_labels_nc_1788_16x16.npy\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: perturbs an image to a specified noise level\n",
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe8eb277",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d92c52-8a11-450c-bc78-ffa221af2fa3",
   "metadata": {},
   "source": [
    "We will need to instrument the sampler to have telemetry on the generated images while training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f5bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16085a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with context using standard algorithm\n",
    "# we make a change to the original algorithm to allow for context explicitely (the noises)\n",
    "@torch.no_grad()\n",
    "def sample_ddpm_context(samples, context, save_rate=20):\n",
    "    # array to keep track of generated steps for plotting\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t, c=context)    # predict noise e_(x_t,t, ctx)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples.clip(-1, 1), intermediate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9ed46d7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00b9ef16-1848-476d-a9dd-09175b8f0e3c",
   "metadata": {},
   "source": [
    "we choose a fixed context vector with 6 of each class, this way we know what to expect on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88afdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noise vector\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "noises = torch.randn(num_samples, 3, height, height).to(device)  \n",
    "\n",
    "# A fixed context vector to sample from\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4]), 5).to(device=device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same code as before, added comments on the extra W&B instrumentation lines\n",
    "# create a wandb run\n",
    "run = wandb.init(project=\"dlai_diffusion\", job_type=\"train_conditional\", config=config)\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    # set into train mode\n",
    "    nn_model.train()\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, leave=False)\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        c = c.to(x)   \n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.8).to(device)\n",
    "        c = c * context_mask.unsqueeze(-1)        \n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)      \n",
    "        pred_noise = nn_model(x_pert, t / timesteps, c=c)      \n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()    \n",
    "        optim.step()\n",
    "\n",
    "        # we log the relevant metrics to the workspace\n",
    "        wandb.log({\"loss\": loss.item(),\n",
    "                   \"lr\": optim.param_groups[0]['lr'],\n",
    "                   \"epoch\": ep})\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        nn_model.eval()\n",
    "        ckpt_file = save_dir + f\"context_model_{ep}.pth\"\n",
    "        torch.save(nn_model.state_dict(), ckpt_file)\n",
    "\n",
    "        # save model to wandb as an Artifact\n",
    "        artifact_name = f\"{wandb.run.id}_context_model\"\n",
    "        at = wandb.Artifact(artifact_name, type=\"model\", \n",
    "                            metadata={\"loss\":loss.item(), \"epoch\":ep})\n",
    "        at.add_file(ckpt_file)\n",
    "        wandb.log_artifact(at, aliases=[f\"epoch_{ep}\"])\n",
    "\n",
    "        # sample the model and log the images to W&B\n",
    "        samples, _ = sample_ddpm_context(noises, ctx_vector[:num_samples])\n",
    "        wandb.log({\"train_samples\": [wandb.Image(img) for img in samples.split(1)]})\n",
    "\n",
    "# finish W&B run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
