{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958524a2-cb56-439e-850e-032dd10478f2",
   "metadata": {},
   "source": [
    "# Training a Diffusion Model with W&B\n",
    "\n",
    "In this notebooks we will instrument the training of a diffusion model with W&B. We will use the Lab3 notebook and add:\n",
    "- Logging of the training loss\n",
    "- Sampling from the model during training and logging the samples to W&B\n",
    "- Saving the model checkpoints to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e687c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(anonymous=\"allow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c0d229a",
   "metadata": {},
   "source": [
    "## Setting Things Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d76c167-7122-4f88-9c9f-5ded96684fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_samples = 30\n",
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = get_device()\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "data_dir = Path('./data/')\n",
    "save_dir = Path('./data/weights/')\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3\n",
    "\n",
    "# we are storing the parameters to be logged to wandb\n",
    "config = SimpleNamespace(\n",
    "    num_samples=num_samples,\n",
    "    timesteps=timesteps,\n",
    "    beta1=beta1,\n",
    "    beta2=beta2,\n",
    "    device=device,\n",
    "    n_feat=n_feat,\n",
    "    n_cfeat=n_cfeat,\n",
    "    height=height,\n",
    "    save_dir=save_dir,\n",
    "    batch_size=batch_size,\n",
    "    n_epoch=n_epoch,\n",
    "    lrate=lrate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99dea4",
   "metadata": {},
   "source": [
    "setup DDPM noise scheduler and sampler (same as in the generative Ai course). \n",
    "- perturb_input: Adds noise to the input image at the corresponding timestep on the schedule\n",
    "- sample_ddpm_context: Samples from the model using the DDPM sampler, we will use this function during training to sample from the model regularly and see how our training is progressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c642e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_input, sample_ddpm_context = setup_ddpm(beta1, beta2, timesteps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9001e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c63b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset.from_np(data_dir/\"sprites_1788_16x16.npy\", data_dir/\"sprite_labels_nc_1788_16x16.npy\")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate, eps=1e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9ed46d7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00b9ef16-1848-476d-a9dd-09175b8f0e3c",
   "metadata": {},
   "source": [
    "we choose a fixed context vector with 6 of each class, this way we know what to expect on the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88afdba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noise vector\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "noises = torch.randn(num_samples, 3, height, height).to(device)  \n",
    "\n",
    "# A fixed context vector to sample from\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,\n",
    "                                     1,1,1,1,1,1,\n",
    "                                     2,2,2,2,2,2,\n",
    "                                     3,3,3,3,3,3,\n",
    "                                     4,4,4,4,4,4]), \n",
    "                       5).to(device=device).float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26765a7e-4ddc-449e-95c3-54c58a564738",
   "metadata": {},
   "source": [
    "The following training cell takes very long to run on CPU, we have already trained the model for you on a GPU equipped machine.\n",
    "\n",
    "### You can visit the result of this >> [training here](https://wandb.ai/deeplearning-ai-temp/sprite_diffusion/runs/lqf74fua) <<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same code as before, added comments on the extra W&B instrumentation lines\n",
    "# create a wandb run\n",
    "run = wandb.init(project=\"sprite_diffusion\", \n",
    "                 job_type=\"train\", \n",
    "                 anonymous=\"allow\", \n",
    "                 config=config)\n",
    "\n",
    "for ep in tqdm(range(n_epoch), leave=True, total=n_epoch):\n",
    "    # set into train mode\n",
    "    nn_model.train()\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, leave=False)\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        c = c.to(x)   \n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.8).to(device)\n",
    "        c = c * context_mask.unsqueeze(-1)        \n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)      \n",
    "        pred_noise = nn_model(x_pert, t / timesteps, c=c)      \n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()    \n",
    "        optim.step()\n",
    "\n",
    "        # we log the relevant metrics to the workspace\n",
    "        wandb.log({\"loss\": loss.item(),\n",
    "                   \"lr\": optim.param_groups[0]['lr'],\n",
    "                   \"epoch\": ep})\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        nn_model.eval()\n",
    "        ckpt_file = save_dir/f\"context_model.pth\"\n",
    "        torch.save(nn_model.state_dict(), ckpt_file)\n",
    "\n",
    "        # save model to wandb as an Artifact\n",
    "        artifact_name = f\"{wandb.run.id}_context_model\"\n",
    "        at = wandb.Artifact(artifact_name, type=\"model\", \n",
    "                            metadata={\"loss\":loss.item(), \"epoch\":ep})\n",
    "        at.add_file(ckpt_file)\n",
    "        wandb.log_artifact(at, aliases=[f\"epoch_{ep}\"])\n",
    "\n",
    "        # sample the model and log the images to W&B\n",
    "        samples, _ = sample_ddpm_context(nn_model, noises, ctx_vector[:num_samples])\n",
    "        wandb.log({\"train_samples\": [wandb.Image(img) for img in samples.split(1)]})\n",
    "\n",
    "# finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676315f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
