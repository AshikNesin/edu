{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import os\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import markdown\n",
    "import markdownify\n",
    "import frontmatter\n",
    "import html\n",
    "import regex as re\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "docs_dir = \"../data/wandb_docs\"\n",
    "docs_dir = pathlib.Path(docs_dir)\n",
    "docs_files = sorted(docs_dir.rglob(\"*.md\"))\n",
    "\n",
    "print(f\"Number of files: {len(docs_files)}\\n\")\n",
    "print(\"First 5 files:\\n{files}\".format(files=\"\\n\".join(map(str, docs_files[:5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [{\"content\": file.read_text(), \"source\": str(file.relative_to(docs_dir))} for file in docs_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "\n",
    "def convert_contents_to_soup(contents: str) -> BeautifulSoup:\n",
    "    _, content = frontmatter.parse(contents)\n",
    "    # use some extensions to convert the markdown to html\n",
    "    markdown_document = markdown.markdown(\n",
    "        content,\n",
    "        extensions=[\n",
    "            \"toc\",\n",
    "            \"pymdownx.extra\",\n",
    "            \"pymdownx.blocks.admonition\",\n",
    "            \"pymdownx.magiclink\",\n",
    "            \"pymdownx.blocks.tab\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.saneheaders\",\n",
    "            \"pymdownx.striphtml\",\n",
    "            \"pymdownx.highlight\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.escapeall\"\n",
    "        ],\n",
    "    )\n",
    "    soup = BeautifulSoup(markdown_document, \"html.parser\")\n",
    "    def remove_urls_a_tags_hrefs(soup):\n",
    "        # For hyperlinks, keep the text but remove the link\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.replace_with(a_tag.text)\n",
    "        \n",
    "        # Remove all images\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.decompose()\n",
    "        \n",
    "        return soup\n",
    "\n",
    "    # Use the function as before\n",
    "    soup = remove_urls_a_tags_hrefs(soup)\n",
    "\n",
    "    def remove_javascript_import_statements(soup):\n",
    "        for p in soup.find_all('p'):\n",
    "            if p.text.strip().startswith('import') and ';' in p.text:\n",
    "                p.decompose()\n",
    "        return soup\n",
    "    soup = remove_javascript_import_statements(soup)\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def clean_soup(soup: BeautifulSoup) -> BeautifulSoup:\n",
    "    \"\"\"Cleans the BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "        soup: The BeautifulSoup object to clean.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    for img_tag in soup.find_all(\"img\", src=True):\n",
    "        img_tag.extract()\n",
    "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    for p_tag in soup.find_all(\"p\"):\n",
    "        if not p_tag.text.strip():\n",
    "            p_tag.decompose()\n",
    "    return soup\n",
    "\n",
    "\n",
    "def clean_contents(contents: str) -> str:\n",
    "    \"\"\"Cleans the contents.\n",
    "\n",
    "    Args:\n",
    "        contents: The contents to clean.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned contents.\n",
    "    \"\"\"\n",
    "    soup = convert_contents_to_soup(contents)\n",
    "    soup = clean_soup(soup)\n",
    "    cleaned_document = markdownify.MarkdownConverter(\n",
    "        heading_style=\"ATX\"\n",
    "    ).convert_soup(soup)\n",
    "    # Regular expression pattern to match import lines\n",
    "    js_import_pattern = r\"import .* from [‘’']@theme/.*[‘’'];\\s*\\n*\"\n",
    "    cleaned_document = re.sub(js_import_pattern, \"\", cleaned_document)\n",
    "    cleaned_document = cleaned_document.replace(\"![]()\", \"\\n\")\n",
    "    cleaned_document = re.sub(r\"\\[([^]]+)\\]\\([^)]+\\)\", r\"\\1\", cleaned_document)\n",
    "    cleaned_document = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned_document)\n",
    "    cleaned_document = frontmatter.loads(cleaned_document).content\n",
    "    return cleaned_document\n",
    "\n",
    "\n",
    "def extract_frontmatter(file_path: pathlib.Path) -> Dict[str, Any]:\n",
    "    \"\"\"Extracts the frontmatter from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        The extracted frontmatter.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents = frontmatter.load(f)\n",
    "        return {k: contents[k] for k in contents.keys()}\n",
    "\n",
    "\n",
    "def strip_markdown_content(file_content):\n",
    "    soup = convert_contents_to_soup(file_content)\n",
    "\n",
    "    # Format headers with custom style and ensure they are not successive\n",
    "    for header in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        header_text = header.get_text()\n",
    "        formatted_header = f\"\\n\\n---\\n\\n{header_text}\\n\\n\\n\"\n",
    "        header.replace_with(formatted_header)\n",
    "\n",
    "    # Replace <br> tags with newline characters\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\\n\\n\")\n",
    "\n",
    "    # Append a newline after each paragraph\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        p.append(\"\\n\\n\\n\")\n",
    "\n",
    "    # Handle multiline code blocks enclosed in <pre> tags\n",
    "    for pre in soup.find_all(\"pre\"):\n",
    "        code_text = pre.get_text()\n",
    "        cleaned_code_text = code_text.strip(\"\\n\")\n",
    "        # Ensure the code block is separated by newlines and enclosed in triple backticks\n",
    "        formatted_code = f\"\\n\\n\\n```\\n{cleaned_code_text}\\n\\n```\\n\\n\\n\"\n",
    "        pre.replace_with(formatted_code)\n",
    "\n",
    "    # Handle inline code blocks\n",
    "    for code in soup.find_all(\"code\"):\n",
    "        if (\n",
    "            code.parent.name != \"pre\"\n",
    "        ):  # This checks if the <code> tag is not inside a <pre> tag\n",
    "            inline_code_text = code.get_text()\n",
    "            formatted_inline_code = f\"`{inline_code_text}`\"\n",
    "            code.replace_with(formatted_inline_code)\n",
    "\n",
    "    # Extract and unescape the HTML to plain text\n",
    "    text = soup.get_text()\n",
    "    unescaped_text = html.unescape(text)\n",
    "\n",
    "    # Clean up escaped underscores and backticks\n",
    "    clean_text = re.sub(r\"\\\\_\", \"_\", unescaped_text)\n",
    "    clean_text = re.sub(r\"\\\\`\", \"`\", clean_text)\n",
    "\n",
    "    # # Normalize double newlines to newlines\n",
    "    clean_text = re.sub(r\"\\n\\n\", \"\\n\", clean_text)\n",
    "\n",
    "    # # Normalize triple or more newlines to double newlines\n",
    "    clean_text = re.sub(r\"\\n{3,}\", \"\\n\\n\", clean_text)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def clean_markdown_chunk(chunk):\n",
    "    chunk = chunk.replace(\"---\", \"\\n\\n\")\n",
    "    chunk = chunk.replace(\"```\", \"\\n\\n\")\n",
    "    chunk = re.sub(r\"\\n+\", \"\\n\", chunk)\n",
    "    chunk = chunk.strip()\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text, allowed_special=\"all\")\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(\"gpt-4o\")\n",
    "\n",
    "def length_function(content: str) -> int:\n",
    "    \n",
    "    return len(tokenizer.encode(content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\n",
    "                \"\\n---\\n\",\n",
    "                \"\\n```\\n\",\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "            ],\n",
    "            is_separator_regex=True,\n",
    "            chunk_size=512,\n",
    "            chunk_overlap=0,\n",
    "            keep_separator=False,\n",
    "            length_function=length_function,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df = pd.DataFrame(docs_df[\"content\"].map(strip_markdown_content).map(text_splitter.split_text).explode().map(clean_markdown_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df[\"source\"] = docs_df[\"source\"]\n",
    "chunks_df = chunks_df.loc[chunks_df['content'].map(length_function) >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "client = AsyncOpenAI()\n",
    "import numpy as np\n",
    "\n",
    "async def embed_batch(texts: List[str]) -> np.array:\n",
    "    embeddings = await client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=\"text-embedding-3-small\",\n",
    "    )\n",
    "    return np.array([embedding.embedding for embedding in embeddings.data])\n",
    "\n",
    "async def embed_data(texts: List[str], batch_size: int = 100) -> np.array:\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        embeddings.append(await embed_batch(batch))\n",
    "    return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = asyncio.run(embed_data(chunks_df[\"content\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_df['embedding'] = embeddings.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "co_client = cohere.AsyncClient(api_key=os.getenv(\"CO_API_KEY\"))\n",
    "async def rerank_context(query: str, context_df: pd.DataFrame, top_n: int=10) -> pd.DataFrame:\n",
    "    reranked = await co_client.rerank(\n",
    "        query=query,\n",
    "        documents=context_df[\"content\"].tolist(),\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        top_n=top_n,\n",
    "        return_documents=False\n",
    "    )\n",
    "    idxs = [result.index for result in reranked.results]\n",
    "    scores = [result.relevance_score for result in reranked.results]\n",
    "    return pd.DataFrame({\"content\": context_df.iloc[idxs][\"content\"], \"source\": context_df.iloc[idxs][\"source\"], \"score\": scores})\n",
    "    \n",
    "\n",
    "async def get_context_docs(df: pd.DataFrame, query: str, answer: str, top_k: int = 10) -> pd.DataFrame:\n",
    "    embedding = await embed_batch([query, answer])\n",
    "    sims = cosine_similarity(embedding, np.array(df['embedding'].tolist()))\n",
    "    qidxs = sims[0].argsort()[::-1][:top_k]\n",
    "    aidxs = sims[1].argsort()[::-1][:top_k]\n",
    "    idxs = np.unique(np.concatenate([qidxs, aidxs]))\n",
    "    context_df = pd.DataFrame({\"content\": df.iloc[idxs][\"content\"], \"source\": df.iloc[idxs][\"source\"]})\n",
    "    reranked = await rerank_context(query, context_df, top_k)\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = pd.read_json(\"../data/wandbot_sample_questions_answers.jsonl\", lines=True, orient=\"records\")\n",
    "# sample = sample_questions.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instructor.utils import disable_pydantic_error_url\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field, model_validator\n",
    "from typing import Dict, List\n",
    "disable_pydantic_error_url()\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"The final scores and relevance of the documents\"\"\"\n",
    "    final_scores: Dict[str, int] = Field(..., description=\"The final scores for each document based on the criteria\")\n",
    "    relevance: List[int] = Field(..., description=\"The ranked order of relevance of the documents\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    @classmethod\n",
    "    def validate_relevance(cls, data: Any) -> Any:\n",
    "        if len(data.relevance) != len(set(data.relevance)):\n",
    "            raise ValueError(\"The relevance list must be unique.\")\n",
    "        if len(data.relevance) != len(data.final_scores):\n",
    "            raise ValueError(\"The relevance list must be the same length as the final scores.\")\n",
    "        return data\n",
    "    \n",
    "\n",
    "ins_client = instructor.from_openai(AsyncOpenAI(),  mode=instructor.Mode.JSON)\n",
    "\n",
    "async def create_message(question: str, answer: str, context_df: pd.DataFrame) -> dict:\n",
    "    documents = \"\"\n",
    "    for idx, row in context_df.reset_index().iterrows():\n",
    "        documents += f\"<doc_{idx}>\\n{row['content']}\\n</doc_{idx}>\\n\"\n",
    "\n",
    "    query_prompt = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    <answer>\n",
    "    {answer}\n",
    "    </answer>\n",
    "    {documents}\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"role\": \"user\", \"content\": query_prompt}\n",
    "\n",
    "import copy\n",
    "\n",
    "async def process_sample(question: str, answer: str, chunks_df: pd.DataFrame) -> dict:\n",
    "    messages = copy.deepcopy(json.load(open(\"prompts/retrieval_generation_prompt.json\")))\n",
    "    \n",
    "    context_df = await get_context_docs(chunks_df, question, answer)\n",
    "    message = await create_message(question, answer, context_df)\n",
    "    messages.append(message)\n",
    "\n",
    "    relevance_response = await ins_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        response_model=Response,\n",
    "        max_retries=5,\n",
    "        max_tokens=250,\n",
    "    )\n",
    "    relevance_results = relevance_response.model_dump()\n",
    "    \n",
    "    relevance_scores = list(relevance_results[\"final_scores\"].values())\n",
    "    context_df[\"relevance\"] = relevance_scores\n",
    "    context_df = context_df.iloc[relevance_results[\"relevance\"]]\n",
    "    contexts = context_df.to_dict(orient=\"records\")\n",
    "    \n",
    "    return {\"question\": question, \"answer\": answer, \"contexts\": contexts}\n",
    "\n",
    "async def get_test_set(sample_questions: pd.DataFrame, chunks_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tasks = [\n",
    "        process_sample(row[\"question\"], row[\"answer\"], chunks_df)\n",
    "        for _, row in sample_questions.iterrows()\n",
    "    ]\n",
    "    \n",
    "    test_set = await asyncio.gather(*tasks)\n",
    "    return pd.DataFrame(test_set)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "undone_questions = sample_questions[sample_questions[\"question\"].map(lambda x: x not in sample_test_set[\"question\"].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "undone_sample_test_set = asyncio.run(get_test_set(undone_questions, chunks_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample_test_set = pd.concat([sample_test_set, undone_sample_test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sample_test_set.to_json(\"../data/eval/full_test_dataset.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_sample_test_set = full_sample_test_set[full_sample_test_set[\"contexts\"].map(lambda x: set([i.get(\"relevance\") for i in x])).map(len) == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts = complete_sample_test_set[\"contexts\"].map(lambda x: pd.Series([i.get('relevance') for i in x]).value_counts().to_dict())\n",
    "subset_sample = complete_sample_test_set[sample_counts.map(lambda x: x[2] >= 2) & sample_counts.map(lambda x: x[1] >= 2) & sample_counts.map(lambda x: x[0] >= 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sample_counts = subset_sample[\"contexts\"].map(lambda x: pd.Series([i.get('relevance') for i in x]).value_counts().to_dict())\n",
    "final_test_set = subset_sample[subset_sample_counts.map(lambda x: x[2] >=x[1]) | subset_sample_counts.map(lambda x: x[1] >=x[0])]\n",
    "final_test_set = final_test_set.drop_duplicates(subset=[\"question\"])\n",
    "final_test_set = final_test_set.sample(100, replace=False)\n",
    "\n",
    "\n",
    "final_test_set.to_json(\"../data/eval/final_test_dataset.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_set = subset_sample[subset_sample.question.map(lambda x: x not in final_test_set.question.tolist())].sample(50, replace=False)\n",
    "eval_set = eval_set.drop_duplicates(subset=[\"question\"])\n",
    "eval_set.to_json(\"../data/eval/final_eval_dataset.jsonl\", lines=True, orient=\"records\")\n",
    "eval_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
