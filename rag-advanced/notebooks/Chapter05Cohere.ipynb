{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 (Cohere)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/rag-irl/rag-advanced/notebooks/Chapter05Cohere.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-cohere} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq markdown pymdown-extensions beautifulsoup4 wandb tiktoken blingfire numpy cohere python-dotenv scipy pandas weave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import wandb\n",
    "\n",
    "WANDB_ENTITY = \"wandbot\"\n",
    "WANDB_PROJECT = \"advanced_rag\"\n",
    "\n",
    "wandb.require(\"core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"data_loading\", group=\"ingestion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = pathlib.Path(\"../data/wandb_docs_06_24\")\n",
    "docs_files = sorted(docs_dir.rglob(\"*.md\"))\n",
    "\n",
    "print(f\"Number of files: {len(docs_files)}\\n\")\n",
    "print(\"First 5 files:\\n{files}\".format(files='\\n'.join(map(str, docs_files[:5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in docs_files:\n",
    "    content = file.read_text()\n",
    "    data.append({\n",
    "        \"content\": content,\n",
    "        \"metadata\": {\n",
    "            \"source\": str(file.relative_to(docs_dir)),\n",
    "            \"raw_tokens\": len(tokenizer.encode(content))\n",
    "        }})\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], data))\n",
    "print(f\"Total Tokens in dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_artifact = wandb.Artifact(name=\"raw_data\", type=\"dataset\",\n",
    "description=\"Wandb documentation\", metadata={\n",
    "    \"total_files\": len(docs_files),\n",
    "    \"date_downloaded\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"total_tokens\": total_tokens\n",
    "    })\n",
    "with raw_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(raw_artifact)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing and pre-processing\n",
    "\n",
    "1. Load and parse the markdown document with [pymdownx](https://facelessuser.github.io/pymdown-extensions/) library and convert them to html.\n",
    "2. Convert the html to text using [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/).\n",
    "3. Remove unnecessary characters and extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"data_processing\", group=\"ingestion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_contents_to_text(contents: str) -> str:\n",
    "    markdown_document = markdown.markdown(\n",
    "        contents,\n",
    "        extensions=[\n",
    "            \"toc\",\n",
    "            \"pymdownx.extra\",\n",
    "            \"pymdownx.blocks.admonition\",\n",
    "            \"pymdownx.magiclink\",\n",
    "            \"pymdownx.blocks.tab\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.saneheaders\",\n",
    "            \"pymdownx.striphtml\",\n",
    "        ],\n",
    "    )\n",
    "    soup = BeautifulSoup(markdown_document, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def make_text_tokenization_safe(content: str) -> str:\n",
    "    \n",
    "    # Normalize whitespace including Space, Tab, Newline, Carriage return, Form feed, Vertical tab\n",
    "    content = re.sub(r'\\s+', ' ', content, flags=re.UNICODE)\n",
    "\n",
    "    special_tokens_set = tokenizer.special_tokens_set\n",
    "\n",
    "    def remove_special_tokens(text: str) -> str:\n",
    "        \"\"\"Removes special tokens from the given text.\n",
    "\n",
    "        Args:\n",
    "            text: A string representing the text.\n",
    "\n",
    "        Returns:\n",
    "            The text with special tokens removed.\n",
    "        \"\"\"\n",
    "        for token in special_tokens_set:\n",
    "            text = text.replace(token, \"\")\n",
    "        return text\n",
    "\n",
    "    cleaned_content = remove_special_tokens(content)\n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_artifact = run.use_artifact(f'{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest', type='dataset')\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = []\n",
    "\n",
    "for doc in raw_data:\n",
    "    parsed_doc = doc.copy()\n",
    "    parsed_doc[\"parsed_content\"]=convert_contents_to_text(doc[\"content\"])\n",
    "    parsed_doc[\"metadata\"][\"parsed_tokens\"] = len(tokenizer.encode(parsed_doc[\"parsed_content\"]))\n",
    "    parsed_data.append(parsed_doc)\n",
    "parsed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parsed_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], parsed_data))\n",
    "\n",
    "preprocessed_artifact = wandb.Artifact(name=\"preprocessed_data\", type=\"dataset\",\n",
    "description=\"Preprocessed wandb documentation\", metadata={\n",
    "    \"total_files\": len(parsed_data),\n",
    "    \"date_preprocessed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"total_parsed_tokens\": total_parsed_tokens,\n",
    "    }\n",
    ")\n",
    "with preprocessed_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in parsed_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(preprocessed_artifact)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Chunking\n",
    "\n",
    "1. First we split the text into sentences using [BlingFire](https://github.com/microsoft/BlingFire) library.\n",
    "2. Then we split the sentences into chunks of a maximum number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blingfire import text_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"data_chunking\", group=\"ingestion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_artifact = run.use_artifact(f'{WANDB_ENTITY}/{WANDB_PROJECT}/preprocessed_data:latest', type='dataset')\n",
    "artifact_dir = preprocessed_artifact.download()\n",
    "preprocessed_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "preprocessed_data = list(map(json.loads, preprocessed_data_file.read_text().splitlines()))\n",
    "preprocessed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://platform.openai.com/docs/tutorials/web-qa-embeddings\n",
    "\n",
    "CHUNK_SIZE=500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "\n",
    "\n",
    "def split_into_chunks(text, max_tokens = CHUNK_SIZE):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text_to_sentences(text).split(\"\\n\")\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "\n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "for doc in preprocessed_data:\n",
    "    chunks = split_into_chunks(doc[\"parsed_content\"])\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            {\n",
    "                \"parsed_content\" : chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"parsed_tokens\": len(tokenizer.encode(chunk))\n",
    "            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for doc in chunked_data:\n",
    "    cleaned_doc = doc.copy()\n",
    "    cleaned_doc[\"embeddable_content\"] = make_text_tokenization_safe(doc[\"parsed_content\"])\n",
    "    cleaned_doc[\"metadata\"][\"embeddable_tokens\"] = len(tokenizer.encode(cleaned_doc[\"embeddable_content\"]))\n",
    "    cleaned_data.append(cleaned_doc)\n",
    "\n",
    "cleaned_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parsed_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], cleaned_data))\n",
    "total_embeddable_tokens = sum(map(lambda x: x[\"metadata\"][\"embeddable_tokens\"], cleaned_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(name=\"chunked_data\", type=\"dataset\",\n",
    "description=\"Chunked wandb documentation\", metadata={\n",
    "    \"total_files\": len(cleaned_data),\n",
    "    \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"total_parsed_tokens\": total_parsed_tokens,\n",
    "    \"total_embeddable_tokens\": total_embeddable_tokens, \n",
    "    \"chunk_size\": CHUNK_SIZE\n",
    "    }\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in cleaned_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Embedding\n",
    "\n",
    "1. We use [Cohere](https://cohere.ai/) to embed the chunks of text.\n",
    "2. We preprocess the text to remove any special characters and whitespace.\n",
    "3. We then embed the text in batches using the cohere `embed-english-v3.0` model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"data_embedding\", group=\"ingestion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_artifact = run.use_artifact(f'{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest', type='dataset')\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.AsyncClient(api_key=os.getenv(\"CO_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def embed_batch(texts: List[str]) -> List[float]:\n",
    "    response = await co.embed(\n",
    "        texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
    "    )\n",
    "    return response.embeddings\n",
    "\n",
    "\n",
    "async def embed_texts(texts: List[str], batch_size=50) -> List[List[float]]:\n",
    "    tasks = [embed_batch(texts[i:i+batch_size]) for i in range(0, len(texts), batch_size)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return [item for sublist in results for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = asyncio.run(embed_texts(list(map(lambda x: x[\"embeddable_content\"], chunked_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_data = []\n",
    "for document, embedding in zip(chunked_data, embeddings):\n",
    "    embedded_document = document.copy()\n",
    "    embedded_document[\"embedding\"] = embedding\n",
    "    embedded_data.append(embedded_document)\n",
    "\n",
    "embedded_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_artifact = wandb.Artifact(name=\"embedded_data\", type=\"dataset\",\n",
    "description=\"Embedded wandb documentation\", metadata={\n",
    "    \"total_files\": len(embedded_data),\n",
    "    \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"embedding_model\": \"cohere-embed-english-v3.0\",\n",
    "    \"embedding_dim\": 1024,\n",
    "    \"chunk_size\": CHUNK_SIZE,\n",
    "    }\n",
    ")\n",
    "with embedded_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in embedded_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(embedded_artifact)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval\n",
    "\n",
    "1. We embed the query with the [Cohere](https://cohere.ai/) `embed-english-v3.0` model.\n",
    "2. We use the cosine distance to find the most relevant chunks of text from the embedding.\n",
    "3. We return the top-k chunks of text along with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import re\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import markdown\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import weave\n",
    "from IPython.display import Markdown\n",
    "from scipy import spatial\n",
    "\n",
    "import wandb\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"parambharat\"\n",
    "WANDB_PROJECT = \"advanced_rag\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(entity=WANDB_ENTITY, project=WANDB_PROJECT, job_type=\"data_retrieval\", group=\"retrieval\")\n",
    "_ = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_artifact = run.use_artifact(f'{WANDB_ENTITY}/{WANDB_PROJECT}/embedded_data:latest', type='dataset')\n",
    "artifact_dir = embedded_artifact.download()\n",
    "embedded_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "embedded_data = list(map(json.loads, embedded_data_file.read_text().splitlines()))\n",
    "\n",
    "embedded_df = pd.DataFrame(embedded_data)\n",
    "embedded_df[\"embedding\"] = embedded_df[\"embedding\"].map(np.array)\n",
    "embedded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.AsyncClient(api_key=os.getenv(\"CO_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def retieve_context(question: str, context: List[Dict[str, Any]], top_k: int=10) -> List[Dict[str, Any]]:\n",
    "    context_df = pd.DataFrame(context)\n",
    "    q_embeddings = await co.embed(texts=[question], model=\"embed-english-v3.0\", input_type=\"search_query\")\n",
    "    q_embeddings = np.array(q_embeddings.embeddings)\n",
    "\n",
    "    document_embeddings = np.vstack(context_df.embedding.tolist())\n",
    "    # Get the distances from the embeddings\n",
    "    context_df[\"distances\"] = spatial.distance.cdist(q_embeddings, document_embeddings, metric='cosine')[0]\n",
    "    output = context_df.sort_values(\"distances\").head(top_k).to_dict(orient=\"records\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-rank\n",
    "\n",
    "1. We rerank the chunks of text using the [Cohere](https://cohere.ai/) `rerank-english-v3.0` model.\n",
    "2. We return the top-k reranked chunks of text along with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def rerank_context(question: str, context: List[Dict[str, Any]], top_k: int=5) -> List[Dict[str, Any]]: \n",
    "    response = await co.rerank(query=question, documents=[item[\"embeddable_content\"] for item in context], model=\"rerank-english-v3.0\", top_n=top_k)\n",
    "    reranked_indices = [item.index for item in response.results]\n",
    "    return [context[index] for index in reranked_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def build_context(question: str, context: List[Dict[str, Any]], max_len: int=4096) -> List[Dict[str, Any]]:\n",
    "    retrieved_context = await retieve_context(question, context, top_k=20)\n",
    "    reranked_context = await rerank_context(question, retrieved_context, top_k=10)\n",
    "    \n",
    "    outputs = []\n",
    "    cur_len = 0\n",
    "\n",
    "    for row in reranked_context:\n",
    "        cur_len += row[\"metadata\"]['parsed_tokens'] + 4\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        outputs.append(row)\n",
    "\n",
    "    # Return the context\n",
    "    outputs = [{\"text\": item[\"parsed_content\"], \"source\": item[\"metadata\"][\"source\"]} for item in outputs]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Synthesis\n",
    "\n",
    "1. We use the [Cohere](https://cohere.ai/) `command-r-plus` model to generate an answer to the query.\n",
    "2. We return the answer in Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are Wandbot, a support expert for Weights & Biases, wandb, and weave.\n",
    "Your goal is to help users with questions related to the Weights & Biases Platform, providing accurate and helpful responses based solely on the given context.\n",
    "\n",
    "You will be provided the context you should use to answer the user's question\n",
    "\n",
    "First, ensure you understand the question and the relevant information in the context. If the question is unclear, prepare to ask for clarification.\n",
    "\n",
    "\n",
    "Process the question and context as follows:\n",
    "1. Identify the main topic and any subtopics in the question.\n",
    "2. Locate relevant information in the context.\n",
    "3. Formulate a clear, concise answer based only on the provided context.\n",
    "4. If code snippets are needed, ensure they are derived only from the context and are syntactically correct and functional.\n",
    "5. Prepare to cite your sources for each piece of information you use.\n",
    "\n",
    "Format your response in Markdown using MLA but without using headers. Structure your answer as follows:\n",
    "1. Direct answer to the question\n",
    "2. Explanation or steps (if applicable)\n",
    "3. Code snippet (if relevant)\n",
    "4. Additional information or tips (if appropriate)\n",
    "\n",
    "For each piece of information you use, add a citation.\n",
    "\n",
    "If the context doesn't provide sufficient information to answer the question fully or accurately, admit your uncertainty and suggest contacting Weights & Biases support at support@wandb.com or visiting the community forums at https://wandb.me/community.\n",
    "\n",
    "Remember, you must always provide both an answer (or an admission of uncertainty) and citations in your response. Do not refer to the context directly in your answer; instead, provide the information and cite the source.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def generate_response(question: str, context: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "\n",
    "    response = await co.chat(\n",
    "        preamble=SYSTEM_PROMPT,\n",
    "        message=question,\n",
    "        model=\"command-r-plus\",\n",
    "        documents=context,\n",
    "        temperature=0.1,\n",
    "        max_tokens=2000\n",
    "        \n",
    "        )\n",
    "    \n",
    "    return response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def render_html_response(question: str, response: Dict[str, Any])-> str:\n",
    "    text = response['text']\n",
    "    citations = sorted(response['citations'], key=lambda x: x['start'], reverse=True)\n",
    "    documents = {item['id']: item for item in response['documents']}\n",
    "    \n",
    "    # Create a dictionary to store unique sources\n",
    "    sources_dict = {}\n",
    "    \n",
    "    for i, citation in enumerate(citations):\n",
    "        # Create a list of source numbers for this citation\n",
    "        source_numbers = []\n",
    "        for doc_id in citation['document_ids']:\n",
    "            source = documents[doc_id]['source']\n",
    "            if source not in sources_dict:\n",
    "                sources_dict[source] = len(sources_dict) + 1\n",
    "            source_numbers.append(str(sources_dict[source]))\n",
    "        \n",
    "        # Join the source numbers for the hover text\n",
    "        hover_text = f\"Sources: [{', '.join(source_numbers)}]\"\n",
    "        cited_text = text[citation['start']:citation['end']]\n",
    "        html_span = f'<span class=\"citation\" data-tooltip=\"{hover_text}\">{cited_text}</span>'\n",
    "        text = text[:citation['start']] + html_span + text[citation['end']:]\n",
    "\n",
    "    # Convert markdown to HTML after processing citations\n",
    "    text = markdown.markdown(text, extensions=[\n",
    "            \"toc\",\n",
    "            \"pymdownx.extra\",\n",
    "            \"pymdownx.blocks.admonition\",\n",
    "            \"pymdownx.magiclink\",\n",
    "            \"pymdownx.blocks.tab\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.saneheaders\",\n",
    "            \"pymdownx.striphtml\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Create the footer with numbered sources\n",
    "    footer = \"<ol>\"\n",
    "    for source, number in sorted(sources_dict.items(), key=lambda x: x[1]):\n",
    "        footer += f\"<li>{source}</li>\"\n",
    "    footer += \"</ol>\"\n",
    "\n",
    "    html = f\"\"\"\n",
    "    <div class=\"response\">\n",
    "        <h3>Question:</h3>\n",
    "        <p>{markdown.markdown(question)}</p>\n",
    "        <h3>Answer:</h3>\n",
    "        {text}\n",
    "        <hr>\n",
    "        <h4>Sources:</h4>\n",
    "        {footer}\n",
    "    </div>\n",
    "\n",
    "    <style>\n",
    "        .citation {{\n",
    "            text-decoration: underline;\n",
    "            cursor: pointer;\n",
    "            position: relative;\n",
    "        }}\n",
    "        .citation::after {{\n",
    "            content: attr(data-tooltip);\n",
    "            position: absolute;\n",
    "            bottom: 100%;\n",
    "            left: 50%;\n",
    "            transform: translateX(-50%);\n",
    "            background-color: #333;\n",
    "            color: white;\n",
    "            padding: 5px;\n",
    "            border-radius: 3px;\n",
    "            opacity: 0;\n",
    "            transition: opacity 0.3s;\n",
    "            white-space: nowrap;\n",
    "            pointer-events: none;\n",
    "        }}\n",
    "        .citation:hover::after {{\n",
    "            opacity: 1;\n",
    "        }}\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def render_markdown_with_footnotes(question: str, response: Dict[str, Any]) -> str:\n",
    "    text = response['text']\n",
    "    citations = sorted(response['citations'], key=lambda x: x['start'], reverse=True)\n",
    "    documents = {item['id']: item for item in response['documents']}\n",
    "    \n",
    "    sources_dict = {}\n",
    "    \n",
    "    code_blocks = re.finditer(r'(`{1,3})[\\s\\S]*?\\1', text)\n",
    "    code_block_ranges = [(m.start(), m.end(), m.group(1)) for m in code_blocks]\n",
    "\n",
    "    for citation in citations:\n",
    "        source_numbers = []\n",
    "        for doc_id in citation['document_ids']:\n",
    "            source = documents[doc_id]['source']\n",
    "            if source not in sources_dict:\n",
    "                sources_dict[source] = len(sources_dict) + 1\n",
    "            source_numbers.append(str(sources_dict[source]))\n",
    "        \n",
    "        footnote = f'<sup>[{\", \".join(source_numbers)}]</sup>'\n",
    "        \n",
    "        in_code_block = False\n",
    "        for start, end, delim in code_block_ranges:\n",
    "            if start <= citation['start'] < end:\n",
    "                in_code_block = True\n",
    "                if len(delim) == 3:  # Multiline code block\n",
    "                    footnote = f'\\n{footnote}'\n",
    "                break\n",
    "        \n",
    "        if in_code_block:\n",
    "            text = text[:end] + footnote + text[end:]\n",
    "        else:\n",
    "            cited_text = text[citation['start']:citation['end']]\n",
    "            underlined_text = f'<ins>{cited_text}</ins>{footnote}'\n",
    "            text = text[:citation['start']] + underlined_text + text[citation['end']:]\n",
    "\n",
    "    footer = []\n",
    "    for source, number in sorted(sources_dict.items(), key=lambda x: x[1]):\n",
    "        footer.append(f'<a name=\"fn{number}\"></a>[{number}] {source}')\n",
    "    footer = '\\n - '.join(footer)\n",
    "    markdown = f\"\"\"## Question\\n**{question}**\\n\\n---\\n\\n## Answer\\n{text}\\n\\n---\\n\\n## Sources\\n\\n - {footer}\"\"\"\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def answer_question(question: str, context: List[Dict[str, Any]]):\n",
    "    context = await build_context(question, context)\n",
    "    response = await generate_response(question, context)\n",
    "    markdown = render_markdown_with_footnotes(question, response)\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query=\"How can I resume my accidentally stopped sweeps?\"\n",
    "markdown_output = asyncio.run(answer_question(sample_query, embedded_df.to_dict(orient=\"records\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(markdown_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
