{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG with Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool use allows for greater flexibility in accessing and utilizing data sources, thus unlocking new use cases not possible with a standard RAG approach.\n",
    "\n",
    "In an enterprise setting where data sources are diverse with non-homogeneous formats (structured/semi-structured/unstructured), this approach becomes even more important.\n",
    "\n",
    "In this notebook, we'll look at how we can implement an agentic RAG system using a tool use approach. We'll do this by building a Weights & Biases assistant. The assistant can search for information about how to use the product, retrieve information from the internet, search code examples, and even perform data analysis.\n",
    "\n",
    "Concretely, we'll cover the following use cases:\n",
    "1. Tool routing\n",
    "2. Parallel tool use\n",
    "3. Multi-step tool use\n",
    "4. Self-correction\n",
    "5. Structured queries\n",
    "6. Structured data queries\n",
    "7. Action (plotting charts)\n",
    "\n",
    "We'll give the assistant access to the following tools:\n",
    "- `search_developer_docs`: Searches the Weights & Biases developer documentation\n",
    "- `search_internet`: Searches the internet for general queries\n",
    "- `search_code_examples`: Searches code examples and tutorials on using Weights & Biases\n",
    "- `analyze_evaluation_results`: Analyzes a table containing results from evaluating an LLM application\n",
    "\n",
    "Note that for simplicity, we are not implementing a full-fledge search. Instead, we'll use a mock datasets containing small, pre-defined data for each tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U cohere pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cohere\n",
    "\n",
    "from tool_def_v2 import (\n",
    "    search_developer_docs,\n",
    "    search_internet,\n",
    "    search_code_examples,\n",
    "    analyze_evaluation_results,\n",
    "    tools,\n",
    ")\n",
    "\n",
    "co = cohere.ClientV2(api_key=os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"search_developer_docs\": search_developer_docs,\n",
    "    \"search_internet\": search_internet,\n",
    "    \"search_code_examples\": search_code_examples,\n",
    "    \"analyze_evaluation_results\": analyze_evaluation_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"\"\"## Task and Context\n",
    "You are an assistant who helps developers use Weights & Biases. The company is also referred to as Wandb or W&B for short. You are equipped with a number of tools that can provide different types of information. If you can't find the information you need from one tool, you should try other tools if there is a possibility that they could provide the information you need. Use the internet to search for information not available in the sources provided by Weights & Biases\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(query: str, messages: list[dict] | None = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Run the agent with the given query and messages.\n",
    "    \"\"\"\n",
    "    if messages is None:\n",
    "        messages = []\n",
    "\n",
    "    if 'system' not in {m.get('role') for m in messages}:\n",
    "        messages = [{'role': 'system', 'content': system_message}]\n",
    "    \n",
    "    # Step 1: get user message\n",
    "    print(f\"USER MESSAGE:\\n{query}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    messages.append({'role': 'user','content': query})\n",
    "\n",
    "    # 2 - Model generates tool calls, if any\n",
    "    model = \"command-r-plus-08-2024\"\n",
    "    res = co.chat(model=model,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                temperature=0.2)\n",
    "    \n",
    "    # Keep invoking tools as long as the model generates tool calls\n",
    "    while res.message.tool_calls:\n",
    "        # Tool plan and tool calls\n",
    "        print(\"\\nTOOL PLAN:\")\n",
    "        print(res.message.tool_plan)\n",
    "        \n",
    "        print(\"\\nTOOL CALLS:\")\n",
    "        for tc in res.message.tool_calls:\n",
    "            print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
    "        \n",
    "        messages.append({'role': 'assistant',\n",
    "                        'tool_calls': res.message.tool_calls,\n",
    "                        'tool_plan': res.message.tool_plan})\n",
    "        \n",
    "        # 3 - Execute tools based on the tool calls generated by the model\n",
    "        print(\"\\nTOOL RESULTS:\")\n",
    "        for tc in res.message.tool_calls:\n",
    "            tool_result = functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
    "            tool_content = [json.dumps(tool_result)]\n",
    "            print(tool_result, \"\\n\")\n",
    "            \n",
    "            messages.append({'role': 'tool',\n",
    "                            'tool_call_id': tc.id,\n",
    "                            'tool_content': tool_content}) \n",
    "        \n",
    "        # 4 - Model either generates more tool calls or returns a response\n",
    "        res = co.chat(model=model,\n",
    "                    messages=messages,\n",
    "                    tools=tools,\n",
    "                    temperature=0.2)\n",
    "        \n",
    "    print(\"\\nRESPONSE:\")\n",
    "    print(res.message.content[0].text)\n",
    "\n",
    "    if res.message.citations:\n",
    "        print(\"\\nCITATIONS:\")\n",
    "        for citation in res.message.citations:\n",
    "            print(f\"Start: {citation.start} | End: {citation.end} | Text: '{citation.text}'\")\n",
    "            print(\"Sources:\")\n",
    "            if citation.sources:\n",
    "                for source in citation.sources:\n",
    "                    print(source.id)\n",
    "            print(\"-\"*50)\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Tool routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tool routing, the agent decides which tool(s) to use based on the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "Where can I find the output of a run\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will search for \"Where can I find the output of a run?\"\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"Where can I find the output of a run?\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'developer_docs': [{'text': 'What is a W&B Run?\\nA W&B run is a single unit of computation logged by W&B, representing an atomic element of your project.'}, {'text': \"How do I view a specific run?\\nTo view a run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\"}, {'text': 'What are Artifact Outputs?\\nArtifact Outputs refer to any artifacts produced by a run.'}, {'text': 'How can I do hyperparameter search quickly?\\nUse W&B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.'}]} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "You can find the output of a run by navigating to the W&B App UI, selecting the relevant project, and then choosing the run from the 'Runs' table.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 36 | End: 146 | Text: 'navigating to the W&B App UI, selecting the relevant project, and then choosing the run from the 'Runs' table.'\n",
      "Sources:\n",
      "search_developer_docs_wv3yvwbjv57k:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Where can I find the output of a run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "How much storage does the Wandb teams pricing offer?\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will search for \"Wandb teams pricing storage\".\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_internet | Parameters: {\"query\":\"Wandb teams pricing storage\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'documents': [{'url': 'https://wandb.ai/site/pricing', 'content': 'The free academic license comes with all the product features included on Teams, 100GB of storage, unlimited tracked hours, and up to 100 seats. Additional storage can be purchased for $0.03 per GB, billed monthly. Weights & Biases MLOps and LLMOps pricing plans. Always free for academics.'}, {'url': 'https://docs.wandb.ai/guides/app/settings-page/team-settings', 'content': 'The Usage section describes the total memory usage the team has consumed on the Weights and Biases servers. The default storage plan is 100GB. For more information about storage and pricing, see the Pricing page. Storage The Storage section describes the'}, {'url': 'https://docs.wandb.ai/guides/app/features/teams', 'content': 'Teams. Use W&B Teams as a central workspace for your ML team to build better models faster. Track all the experiments your team has tried so you never duplicate work.; Save and reproduce previously trained models.; Share progress and results with your boss and collaborators.; Catch regressions and immediately get alerted when performance drops.; Benchmark model performance and compare model ...'}, {'url': 'https://community.wandb.ai/t/some-clarification-about-w-b-starter-plan-pricing/3227', 'content': \"I'm currently evaluating W&B as experiment tracking solution for the company I'm currently working. So far, we found that W&B cover all our basic needs so we would like to buy a starter plan. However before to move on, I need some clarifications about the pricing that are not so clear from the info available on the website: Tracked Hours ...\"}, {'url': 'https://wandb.ai/site/for-enterprise/wb-for-teams', 'content': \"A single shared source of truth. Unify everything from models and pipelines to experiments and datasets in a single system of record for your ML team. Weights & Biases lets you log and track what's important for your unique use case so your team can be confident about every step of your model development pipeline. Get started.\"}]} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "The default storage plan for Weights & Biases (W&B) Teams is 100GB. Additional storage can be purchased for $0.03 per GB, billed monthly.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 4 | End: 24 | Text: 'default storage plan'\n",
      "Sources:\n",
      "search_internet_wwr64h2h98bp:0\n",
      "--------------------------------------------------\n",
      "Start: 61 | End: 67 | Text: '100GB.'\n",
      "Sources:\n",
      "search_internet_wwr64h2h98bp:0\n",
      "--------------------------------------------------\n",
      "Start: 68 | End: 137 | Text: 'Additional storage can be purchased for $0.03 per GB, billed monthly.'\n",
      "Sources:\n",
      "search_internet_wwr64h2h98bp:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"How much storage does the Wandb teams pricing offer?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Parallel tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can call multiple tools in parallel. In this example, given that the user is asking about two different things in a single message, the agent generates two parallel tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "Explain what is a W&B Run and how do I view a specific run\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will search for \"what is a W&B Run\" and \"how to view a specific run\" in the Weights & Biases developer documentation.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"what is a W\\u0026B Run\"}\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"how to view a specific run\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'developer_docs': [{'text': 'What is a W&B Run?\\nA W&B run is a single unit of computation logged by W&B, representing an atomic element of your project.'}, {'text': \"How do I view a specific run?\\nTo view a run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\"}, {'text': 'What are Artifact Outputs?\\nArtifact Outputs refer to any artifacts produced by a run.'}, {'text': 'How can I do hyperparameter search quickly?\\nUse W&B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.'}]} \n",
      "\n",
      "{'developer_docs': [{'text': 'What is a W&B Run?\\nA W&B run is a single unit of computation logged by W&B, representing an atomic element of your project.'}, {'text': \"How do I view a specific run?\\nTo view a run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\"}, {'text': 'What are Artifact Outputs?\\nArtifact Outputs refer to any artifacts produced by a run.'}, {'text': 'How can I do hyperparameter search quickly?\\nUse W&B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.'}]} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "A W&B Run is a single unit of computation logged by W&B, representing an atomic element of your project.\n",
      "\n",
      "To view a specific run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 15 | End: 55 | Text: 'single unit of computation logged by W&B'\n",
      "Sources:\n",
      "search_developer_docs_5e5nq87c7jz1:0\n",
      "search_developer_docs_n36fnq5wgz19:0\n",
      "--------------------------------------------------\n",
      "Start: 57 | End: 104 | Text: 'representing an atomic element of your project.'\n",
      "Sources:\n",
      "search_developer_docs_5e5nq87c7jz1:0\n",
      "search_developer_docs_n36fnq5wgz19:0\n",
      "--------------------------------------------------\n",
      "Start: 130 | End: 156 | Text: 'navigate to the W&B App UI'\n",
      "Sources:\n",
      "search_developer_docs_5e5nq87c7jz1:0\n",
      "search_developer_docs_n36fnq5wgz19:0\n",
      "--------------------------------------------------\n",
      "Start: 158 | End: 185 | Text: 'select the relevant project'\n",
      "Sources:\n",
      "search_developer_docs_5e5nq87c7jz1:0\n",
      "search_developer_docs_n36fnq5wgz19:0\n",
      "--------------------------------------------------\n",
      "Start: 196 | End: 233 | Text: 'choose the run from the 'Runs' table.'\n",
      "Sources:\n",
      "search_developer_docs_5e5nq87c7jz1:0\n",
      "search_developer_docs_n36fnq5wgz19:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Explain what is a W&B Run and how do I view a specific run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Multi-step tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be scenarios where tool calling needs to happen in a sequence. For example, when the output of one tool call is needed as input for another tool call.\n",
    "\n",
    "In this example, the agent first searches the developer docs for information about how to view a run. Then, it uses the information to search for a code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "What's that feature to view artifacts? Do you have any jupyter notebook examples?\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will search for the feature to view artifacts. If I find it, I will search for a Jupyter notebook example.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"view artifacts\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'developer_docs': [{'text': 'What is a W&B Run?\\nA W&B run is a single unit of computation logged by W&B, representing an atomic element of your project.'}, {'text': \"How do I view a specific run?\\nTo view a run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\"}, {'text': 'What are Artifact Outputs?\\nArtifact Outputs refer to any artifacts produced by a run.'}, {'text': 'How can I do hyperparameter search quickly?\\nUse W&B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.'}]} \n",
      "\n",
      "\n",
      "TOOL PLAN:\n",
      "I found that Artifact Outputs refer to any artifacts produced by a run. I will now search for Jupyter notebook examples of viewing artifacts.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_code_examples | Parameters: {\"file_type\":\"ipynb\",\"language\":\"en\",\"query\":\"view artifacts\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'code_examples': [{'content': 'Interactive W&B Charts Inside Jupyter', 'file_type': 'ipynb', 'language': 'en'}, {'content': 'Model/Data Versioning with Artifacts (PyTorch)', 'file_type': 'ipynb', 'language': 'en'}, {'content': 'Create a hyperparameter search with W&B PyTorch integration', 'file_type': 'ipynb', 'language': 'en'}, {'content': 'Get started with W&B Weave', 'file_type': 'ipynb', 'language': 'en'}]} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "I'm sorry, I couldn't find any information on how to view artifacts. However, I did find a Jupyter notebook example for model/data versioning with artifacts. Would you like to know more about this?\n",
      "\n",
      "CITATIONS:\n",
      "Start: 91 | End: 157 | Text: 'Jupyter notebook example for model/data versioning with artifacts.'\n",
      "Sources:\n",
      "search_code_examples_pn57nen396fm:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's that feature to view artifacts? Do you have any jupyter notebook examples?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Self-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of multi-step tool use can be extended to self-correction. Given the output of the current tool call, the agent may decide to change its plan i.e. self-correct. \n",
    "\n",
    "In this example, the agent doesn't find the information it's looking for in the developer docs. Thus, it generates a new tool call to search the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "What is the weave product from wandb?\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will search for 'weave product wandb' to find the answer to the question.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_developer_docs | Parameters: {\"query\":\"weave product wandb\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'developer_docs': [{'text': 'What is a W&B Run?\\nA W&B run is a single unit of computation logged by W&B, representing an atomic element of your project.'}, {'text': \"How do I view a specific run?\\nTo view a run, navigate to the W&B App UI, select the relevant project, and then choose the run from the 'Runs' table.\"}, {'text': 'What are Artifact Outputs?\\nArtifact Outputs refer to any artifacts produced by a run.'}, {'text': 'How can I do hyperparameter search quickly?\\nUse W&B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking.'}]} \n",
      "\n",
      "\n",
      "TOOL PLAN:\n",
      "I couldn't find any information about the weave product from wandb in the developer documentation. I will now search the internet to see if I can find any information.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_internet | Parameters: {\"query\":\"weave product wandb\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'documents': [{'url': 'https://wandb.ai/site/weave/', 'content': 'Building demos of Generative AI applications is deceptively easy; getting them into production (and maintaining their high quality) is not. W&B Weave is here to help developers build and iterate on their AI applications with confidence.. Create rigorous apples-to-apples evaluations to score the behavior of any aspect of your app. Examine and debug failures by easily inspecting inputs and outputs.'}, {'url': 'https://www.wandb.courses/courses/101-weave', 'content': 'This course will guide you through the entire process of designing, experimenting, and evaluating LLM-based apps. This course introduces Weave by Weights & Biases, a toolkit for developing Generative AI applications. Learn to log, debug, and evaluate language model workflows, ensuring accurate and consistent results across your projects.'}, {'url': 'https://github.com/wandb/weave', 'content': \"You can trace any function using weave.op() - from api calls to OpenAI, Anthropic, Google AI Studio etc to generation calls from Hugging Face and other open source models to any other validation functions or data transformations in your code you'd like to keep track of.. Decorate all the functions you want to trace, this will generate a trace tree of the inputs and outputs of all your functions:\"}]} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "Weights & Biases Weave is a toolkit for developing Generative AI applications. It helps developers build and iterate on their AI applications with confidence. It allows you to create rigorous apples-to-apples evaluations to score the behaviour of any aspect of your app. You can also examine and debug failures by easily inspecting inputs and outputs.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 28 | End: 78 | Text: 'toolkit for developing Generative AI applications.'\n",
      "Sources:\n",
      "search_internet_atwb37eg610j:0\n",
      "--------------------------------------------------\n",
      "Start: 82 | End: 158 | Text: 'helps developers build and iterate on their AI applications with confidence.'\n",
      "Sources:\n",
      "search_internet_atwb37eg610j:0\n",
      "--------------------------------------------------\n",
      "Start: 176 | End: 270 | Text: 'create rigorous apples-to-apples evaluations to score the behaviour of any aspect of your app.'\n",
      "Sources:\n",
      "search_internet_atwb37eg610j:0\n",
      "--------------------------------------------------\n",
      "Start: 284 | End: 351 | Text: 'examine and debug failures by easily inspecting inputs and outputs.'\n",
      "Sources:\n",
      "search_internet_atwb37eg610j:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What is the weave product from wandb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Structured queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool use setup can be leveraged to perform structured queries. For data sources that contain rich metadata, structured queries can be used to perform highly-specific queries, returning more accurate results.\n",
    "\n",
    "In this example, we can take advantage of metadata available in the code examples dataset such as the file type and language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "Any jupyter notebook for Data Versioning with Artifacts?\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will search for a Jupyter notebook for Data Versioning with Artifacts.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: search_code_examples | Parameters: {\"file_type\":\"ipynb\",\"language\":\"en\",\"query\":\"Data Versioning with Artifacts\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'code_examples': [{'content': 'Interactive W&B Charts Inside Jupyter', 'file_type': 'ipynb', 'language': 'en'}, {'content': 'Model/Data Versioning with Artifacts (PyTorch)', 'file_type': 'ipynb', 'language': 'en'}, {'content': 'Create a hyperparameter search with W&B PyTorch integration', 'file_type': 'ipynb', 'language': 'en'}, {'content': 'Get started with W&B Weave', 'file_type': 'ipynb', 'language': 'en'}]} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "Yes, there is a Jupyter notebook for Data Versioning with Artifacts (PyTorch).\n",
      "\n",
      "CITATIONS:\n",
      "Start: 16 | End: 77 | Text: 'Jupyter notebook for Data Versioning with Artifacts (PyTorch)'\n",
      "Sources:\n",
      "search_code_examples_5qb0cm7rp68f:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Any jupyter notebook for Data Versioning with Artifacts?\")\n",
    "# Tool call: Searches search_code_examples with file_type = ipynb\n",
    "# Answer: Returns file - Model/Data Versioning with Artifacts (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Structured data queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can generate queries against structured data sources, such as a CSV file or a database.\n",
    "\n",
    "In this example, we'll use a mock dataset containing LLM application evaluation results for different use cases and settings. Since it's a CSV file, we can create the `analyze_evaluation_results` tool to perform queries on the dataset using the pandas library, executed by a Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "What's the average evaluation score in run A\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will use the 'analyze_evaluation_results' tool to find the average evaluation score in run A.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results | Parameters: {\"code\":\"import pandas as pd\\n\\ndf = pd.read_csv(\\\"evaluation_results.csv\\\")\\n\\n# Filter the dataframe to only include rows where the run is 'A'\\nfiltered_df = df[df['run'] == 'A']\\n\\n# Calculate the average score\\naverage_score = filtered_df['score'].mean()\\n\\nprint(f\\\"The average evaluation score in run A is {average_score}\\\")\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'python_answer': 'The average evaluation score in run A is 0.6333333333333334\\n'} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "The average evaluation score in run A is 0.6333333333333334.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 41 | End: 60 | Text: '0.6333333333333334.'\n",
      "Sources:\n",
      "analyze_evaluation_results_smgt4k1grzcx:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's the average evaluation score in run A\")\n",
    "# Answer: 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "What's the latency of the highest-scoring run for the summarize_article use case?\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will use the 'analyze_evaluation_results' tool to find the latency of the highest-scoring run for the summarize_article use case.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results | Parameters: {\"code\":\"import pandas as pd\\n\\ndf = pd.read_csv(\\\"evaluation_results.csv\\\")\\n\\n# Filter the dataframe to only include rows where the usecase is 'summarize_article'\\nfiltered_df = df[df['usecase'] == 'summarize_article']\\n\\n# Find the highest-scoring run\\nhighest_scoring_run = filtered_df.loc[filtered_df['score'].idxmax()]\\n\\n# Print the latency of the highest-scoring run\\nprint(f\\\"The latency of the highest-scoring run for the summarize_article use case is {highest_scoring_run['latency']} seconds.\\\")\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'python_answer': 'The latency of the highest-scoring run for the summarize_article use case is 4.8 seconds.\\n'} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "The latency of the highest-scoring run for the summarize_article use case is 4.8 seconds.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 77 | End: 89 | Text: '4.8 seconds.'\n",
      "Sources:\n",
      "analyze_evaluation_results_02p2e8se28cz:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"What's the latency of the highest-scoring run for the summarize_article use case?\")\n",
    "# Answer: 4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "Which use case uses the least amount of tokens on average and what's the average token count?\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will use the 'analyze_evaluation_results' tool to find out which use case uses the least amount of tokens on average and what the average token count is.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results | Parameters: {\"code\":\"import pandas as pd\\n\\ndf = pd.read_csv(\\\"evaluation_results.csv\\\")\\n\\n# Calculate the average number of tokens for each use case\\naverage_tokens_per_usecase = df.groupby(\\\"usecase\\\")[\\\"tokens\\\"].mean()\\n\\n# Find the use case with the lowest average number of tokens\\nmin_tokens_usecase = average_tokens_per_usecase.idxmin()\\nmin_tokens_usecase_avg = average_tokens_per_usecase.min()\\n\\nprint(f\\\"Use case with the least average number of tokens: {min_tokens_usecase}\\\")\\nprint(f\\\"Average number of tokens for that use case: {min_tokens_usecase_avg}\\\")\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'python_answer': 'Use case with the least average number of tokens: extract_names\\nAverage number of tokens for that use case: 106.25\\n'} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "The use case with the least average number of tokens is 'extract_names', with an average of 106.25 tokens.\n",
      "\n",
      "CITATIONS:\n",
      "Start: 56 | End: 71 | Text: ''extract_names''\n",
      "Sources:\n",
      "analyze_evaluation_results_j4pactd6xmj4:0\n",
      "--------------------------------------------------\n",
      "Start: 92 | End: 98 | Text: '106.25'\n",
      "Sources:\n",
      "analyze_evaluation_results_j4pactd6xmj4:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = run_agent(\"Which use case uses the least amount of tokens on average and what's the average token count?\")\n",
    "# Answer: extract_names (106.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Action (plotting charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have access to the Python interpreter, we can also use it to perform other tasks such as plotting charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER MESSAGE:\n",
      "Create a plot of the average evaluation score for each temperature setting for the extract_names use case.\n",
      "==================================================\n",
      "\n",
      "TOOL PLAN:\n",
      "I will use the 'analyze_evaluation_results' tool to create a plot of the average evaluation score for each temperature setting for the extract_names use case.\n",
      "\n",
      "TOOL CALLS:\n",
      "Tool name: analyze_evaluation_results | Parameters: {\"code\":\"import pandas as pd\\n\\ndf = pd.read_csv(\\\"evaluation_results.csv\\\")\\n\\n# Filter the dataframe to only include the extract_names use case\\ndf = df[df[\\\"usecase\\\"] == \\\"extract_names\\\"]\\n\\n# Group by temperature and calculate the average score\\navg_score_by_temp = df.groupby(\\\"temperature\\\")[\\\"score\\\"].mean()\\n\\n# Plot the average score by temperature\\navg_score_by_temp.plot(kind=\\\"bar\\\")\"}\n",
      "\n",
      "TOOL RESULTS:\n",
      "{'python_answer': ''} \n",
      "\n",
      "\n",
      "RESPONSE:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(\"evaluation_results.csv\")\n",
      "\n",
      "# Filter the dataframe to only include the extract_names use case\n",
      "df = df[df[\"usecase\"] == \"extract_names\"]\n",
      "\n",
      "# Group by temperature and calculate the average score\n",
      "avg_score_by_temp = df.groupby(\"temperature\")[\"score\"].mean()\n",
      "\n",
      "# Plot the average score by temperature\n",
      "avg_score_by_temp.plot(kind=\"bar\")\n",
      "```\n",
      "\n",
      "CITATIONS:\n",
      "Start: 10 | End: 29 | Text: 'import pandas as pd'\n",
      "Sources:\n",
      "analyze_evaluation_results_099jyrr30eba:0\n",
      "--------------------------------------------------\n",
      "Start: 31 | End: 73 | Text: 'df = pd.read_csv(\"evaluation_results.csv\")'\n",
      "Sources:\n",
      "analyze_evaluation_results_099jyrr30eba:0\n",
      "--------------------------------------------------\n",
      "Start: 141 | End: 181 | Text: 'df = df[df[\"usecase\"] == \"extract_names\"'\n",
      "Sources:\n",
      "analyze_evaluation_results_099jyrr30eba:0\n",
      "--------------------------------------------------\n",
      "Start: 239 | End: 300 | Text: 'avg_score_by_temp = df.groupby(\"temperature\")[\"score\"].mean()'\n",
      "Sources:\n",
      "analyze_evaluation_results_099jyrr30eba:0\n",
      "--------------------------------------------------\n",
      "Start: 342 | End: 375 | Text: 'avg_score_by_temp.plot(kind=\"bar\"'\n",
      "Sources:\n",
      "analyze_evaluation_results_099jyrr30eba:0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = run_agent(\"Create a plot of the average evaluation score for each temperature setting for the extract_names use case.\")\n",
    "# Answer: temp 0.3 (0.46 avg score) vs temp 0.5 (0.7 avg score). And draws a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates how we can implement an agentic RAG system with tool use.\n",
    "\n",
    "We covered the following use cases:\n",
    "1. Tool routing\n",
    "2. Parallel tool use\n",
    "3. Multi-step tool use\n",
    "4. Self-correction\n",
    "5. Structured queries\n",
    "6. Structured data queries\n",
    "7. Action (plotting charts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
