{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with a Tool Use approach\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/main/rag-advanced/notebooks/rag_tooluse/rag_tooluse_v1.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-05-cohere} -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool use allows for greater flexibility in accessing and utilizing data sources, thus unlocking new use cases not possible with a standard RAG approach.\n",
    "\n",
    "In an enterprise setting where data sources are diverse with non-homogeneous formats (structured/semi-structured/unstructured), this approach becomes even more important.\n",
    "\n",
    "In this notebook, we'll look at how we can implement an agentic RAG system using a tool use approach. We'll do this by building a Weights & Biases assistant. The assistant can search for information about how to use the product, retrieve information from the internet, search code examples, and even perform data analysis.\n",
    "\n",
    "Concretely, we'll cover the following use cases:\n",
    "1. Tool routing\n",
    "2. Parallel tool use\n",
    "3. Multi-step tool use\n",
    "4. Self-correction\n",
    "5. Structured queries\n",
    "6. Structured data queries\n",
    "7. Action (plotting charts)\n",
    "\n",
    "We'll give the assistant access to the following tools:\n",
    "- `search_developer_docs`: Searches the Weights & Biases developer documentation\n",
    "- `search_internet`: Searches the internet for general queries\n",
    "- `search_code_examples`: Searches code examples and tutorials on using Weights & Biases\n",
    "- `analyze_evaluation_results`: Analyzes a table containing results from evaluating an LLM application\n",
    "\n",
    "Note that for simplicity, we are not implementing a full-fledge search. Instead, we'll use a mock datasets containing small, pre-defined data for each tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys\"\n",
    "\n",
    "from tool_def_v1 import (\n",
    "    search_developer_docs,\n",
    "    search_internet,\n",
    "    search_code_examples,\n",
    "    analyze_evaluation_results,\n",
    "    search_tools,\n",
    "    analysis_tool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"search_developer_docs\": search_developer_docs,\n",
    "    \"search_internet\": search_internet,\n",
    "    \"search_code_examples\": search_code_examples,\n",
    "    \"analyze_evaluation_results\": analyze_evaluation_results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble=\"\"\"## Task and Context\n",
    "You are an assistant who helps developers use Weights & Biases. The company is also referred to as Wandb or W&B for short. You are equipped with a number of tools that can provide different types of information. If you can't find the information you need from one tool, you should try other tools if there is a possibility that they could provide the information you need. Use the internet to search for information not available in the sources provided by Weights & Biases\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"command-r-plus-08-2024\"\n",
    "\n",
    "def run_agent(message, tools, chat_history=None):\n",
    "    \n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    # Step 1: Get user message\n",
    "    print(f\"Question:\\n{message}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Step 2: Generate tool calls (if any)    \n",
    "    response = co.chat(\n",
    "        message=message,\n",
    "        model=model,\n",
    "        preamble=preamble,\n",
    "        tools= tools,\n",
    "        chat_history=chat_history,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    while response.tool_calls:\n",
    "        tool_calls = response.tool_calls\n",
    "        \n",
    "        if response.text:\n",
    "            print(\"Tool plan:\")\n",
    "            print(response.text,\"\\n\")\n",
    "        print(\"Tool calls:\")\n",
    "        for call in tool_calls:            \n",
    "            ###############\n",
    "            if call.name == \"analyze_evaluation_results\":\n",
    "                print(f\"Tool name: {call.name}\")\n",
    "                print(f\"Code:\\n\")\n",
    "                tool_call_prettified = print(\"\\n\".join(f\"  {line}\" for line_num, line in enumerate(call.parameters[\"code\"].splitlines())))\n",
    "                print(tool_call_prettified)\n",
    "            else:\n",
    "                print(f\"Tool name: {call.name} | Parameters: {call.parameters}\")\n",
    "            ###############\n",
    "            \n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Step 3: Get tool results\n",
    "        tool_results = []\n",
    "        for tc in tool_calls:\n",
    "            tool_call = {\"name\": tc.name, \"parameters\": tc.parameters}\n",
    "            tool_output = functions_map[tc.name](**tc.parameters)\n",
    "            tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]})\n",
    "        \n",
    "        # Step 4: Generate response and citations                \n",
    "        response = co.chat(\n",
    "            message=\"\",\n",
    "            model=model,\n",
    "            preamble=preamble,\n",
    "            tools=tools,\n",
    "            tool_results=tool_results,\n",
    "            chat_history=response.chat_history,\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        # Append the current chat turn to the chat history\n",
    "        chat_history = response.chat_history\n",
    "        \n",
    "    # Print final response\n",
    "    print(\"Final response:\")\n",
    "    print(response.text)\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Print citations (if any)\n",
    "    show_documents = False # set this to True to see the documents used for the response\n",
    "    if response.citations:\n",
    "        print(\"Citations:\")\n",
    "        for citation in response.citations:\n",
    "            print(citation)\n",
    "        if show_documents:\n",
    "            print(\"\\nCited Documents:\")\n",
    "            for document in response.documents:\n",
    "                print(document)\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Tool routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tool routing, the agent decides which tool(s) to use based on the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"Where can I find the output of a run\", search_tools)\n",
    "# chooses search_developer_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"Who are the co authors of the sentence transformer paper?\", search_tools)\n",
    "# chooses search_internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Parallel tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can call multiple tools in parallel. In this example, given that the user is asking about two different things in a single message, the agent generates two parallel tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"Explain what is a W&B Run and how do I view a specific run\", search_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Multi-step tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be scenarios where tool calling needs to happen in a sequence. For example, when the output of one tool call is needed as input for another tool call.\n",
    "\n",
    "In this example, the agent first searches the developer docs for information about how to view a run. Then, it uses the information to search for a code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_agent(\"What's that feature to automate hyperparameter search? Do you have some code tutorials?\", search_tools)\n",
    "# Does two steps of tool use in a sequence\n",
    "# Returns code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4: Self-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of multi-step tool use can be extended to self-correction. Given the output of the current tool call, the agent may decide to change its plan i.e. self-correct. \n",
    "\n",
    "In this example, the agent doesn't find the information it's looking for in the developer docs. Thus, it generates a new tool call to search the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"What is Wandb's weave solution?\", search_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Structured queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool use setup can be leveraged to perform structured queries. For data sources that contain rich metadata, structured queries can be used to perform highly-specific queries, returning more accurate results.\n",
    "\n",
    "In this example, we can take advantage of metadata available in the code examples dataset such as the file type and language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"Any jupyter notebook for Data Versioning with Artifacts?\", search_tools)\n",
    "# Tool call: Searches search_code_examples with file_type = ipynb\n",
    "# Answer: Returns file - Model/Data Versioning with Artifacts (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Structured data queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent can generate queries against structured data sources, such as a CSV file or a database.\n",
    "\n",
    "In this example, we'll use a mock dataset containing LLM application evaluation results for different use cases and settings. Since it's a CSV file, we can create the `analyze_evaluation_results` tool to perform queries on the dataset using the pandas library, executed by a Python interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"What's the average evaluation score in run A\", analysis_tool)\n",
    "# Answer: 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"What's the latency of the highest-scoring run for the summarize_article use case?\", analysis_tool)\n",
    "# Answer: 4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"Which use case uses the least amount of tokens on average and what's the average token count?\", analysis_tool)\n",
    "# Answer: extract_names (106.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Action (plotting charts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have access to the Python interpreter, we can also use it to perform other tasks such as plotting charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = run_agent(\"Show a chart of the average evaluation score for each temperature setting for the extract_names use case.\", analysis_tool)\n",
    "# Answer: temp 0.3 (0.46 avg score) vs temp 0.5 (0.7 avg score). And draws a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates how we can implement an agentic RAG system with tool use.\n",
    "\n",
    "We covered the following use cases:\n",
    "1. Tool routing\n",
    "2. Parallel tool use\n",
    "3. Multi-step tool use\n",
    "4. Self-correction\n",
    "5. Structured queries\n",
    "6. Structured data queries\n",
    "7. Action (plotting charts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
