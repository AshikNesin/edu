{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with a Tool Use approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll look at how we can implement RAG use cases using a tool use approach.\n",
    "\n",
    "Tool use allows for greater flexibility of accessing data sources, thus unlocking new use cases not possible with a standard RAG approach.\n",
    "\n",
    "In an enterprise setting with diverse data sources with non-homogeneous formats (structured/semi-structured/unstructured), this approach becomes even more useful.\n",
    "\n",
    "We'll look at a few example use cases:\n",
    "- Tool selection\n",
    "- Multi-step searches\n",
    "- Structured queries\n",
    "- Structured data queries\n",
    "- Action - plotting charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cohere\n",
    "\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "\n",
    "from tool_def import (analyze_evaluation_results, search_code_examples,\n",
    "                      search_company_information, search_developer_docs, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"search_code_examples\": search_code_examples,\n",
    "    \"search_developer_docs\": search_developer_docs,\n",
    "    \"search_company_information\": search_company_information,\n",
    "    \"analyze_evaluation_results\": analyze_evaluation_results,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"## Task & Context\n",
    "You are an assistant who helps developers use Weights & Biases. The company is also referred to as Wandb or W&B for short. You are equipped with a number of tools that can provide different types of information. If you can't find the information you need from one tool, you should try other tools if there is a possibility that they could provide the information you need.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run the assistant over multiple chat turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"command-r-plus\"\n",
    "\n",
    "\n",
    "def run_assistant(message, chat_history=[], show_documents=False):\n",
    "    # Step 1: Get user message\n",
    "    print(f\"Question:\\n{message}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Step 2: Generate tool calls (if any)\n",
    "    response = co.chat(\n",
    "        message=message,\n",
    "        model=model,\n",
    "        preamble=preamble,\n",
    "        tools=tools,\n",
    "        chat_history=chat_history,\n",
    "    )\n",
    "\n",
    "    while response.tool_calls:\n",
    "        tool_calls = response.tool_calls\n",
    "\n",
    "        if response.text:\n",
    "            print(\"Intermediate response:\")\n",
    "            print(response.text, \"\\n\")\n",
    "        print(\"Tool calls:\")\n",
    "        for call in tool_calls:\n",
    "            print(f\"Tool name: {call.name} | Parameters: {call.parameters}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Step 3: Get tool results\n",
    "        tool_results = []\n",
    "        for tc in tool_calls:\n",
    "            tool_call = {\"name\": tc.name, \"parameters\": tc.parameters}\n",
    "            tool_output = functions_map[tc.name](**tc.parameters)\n",
    "            tool_results.append({\"call\": tool_call, \"outputs\": [tool_output]})\n",
    "\n",
    "        # Step 4: Generate response and citations\n",
    "        response = co.chat(\n",
    "            message=\"\",\n",
    "            model=model,\n",
    "            preamble=preamble,\n",
    "            tools=tools,\n",
    "            tool_results=tool_results,\n",
    "            chat_history=response.chat_history,\n",
    "        )\n",
    "\n",
    "        # Append the current chat turn to the chat history\n",
    "        chat_history = response.chat_history\n",
    "\n",
    "    # Print final response\n",
    "    print(\"Final response:\")\n",
    "    print(response.text)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Print citations (if any)\n",
    "    if response.citations:\n",
    "        print(\"Citations:\")\n",
    "        for citation in response.citations:\n",
    "            print(citation)\n",
    "        if show_documents:\n",
    "            print(\"\\nCited Documents:\")\n",
    "            for document in response.documents:\n",
    "                print(document)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\"Where can I find the output of a run\")\n",
    "# Chooses search_developer_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\"Where are Wandb's offices\")\n",
    "# Chooses search_company_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-step searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\n",
    "    \"What's that feature to automate hyperparameter search? Do you have some code examples?\"\n",
    ")\n",
    "# Does two steps of tool use\n",
    "# Returns two code examples - Selecting Hyperparameters with Sweeps (Keras) & Create a hyperparameter search with W&B PyTorch integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\"Any jupyter notebook for Data Versioning with Artifacts?\")\n",
    "# Searches for file_type = ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\"Any code examples for data versioning with artifacts?\")\n",
    "# Doesn't need to specify file_type as it's a generic question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English: Code examples on how to visualize datasets\n",
    "chat_history = run_assistant(\"データセットを視覚化する方法のコード例?\")\n",
    "# Searches for language = ja\n",
    "# Returns a code example in Japanese about tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured data queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\"What's the average evaluation score in run A\")\n",
    "# Answer: 0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\n",
    "    \"What's the latency of the highest-scoring run for the summarize_article use case?\"\n",
    ")\n",
    "# Answer: 4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\n",
    "    \"Which use case uses the least amount of tokens on average? Show the comparison in a table.\"\n",
    ")\n",
    "# Answer: extract_names (106.25), draft_email (245.75), summarize_article (355.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action - plotting charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = run_assistant(\n",
    "    \"Create a plot of the average evaluation score for each temperature setting for the extract_names use case.\"\n",
    ")\n",
    "# Answer: temp 0.3 (0.46 avg score) vs temp 0.5 (0.7 avg score). And draws a plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
