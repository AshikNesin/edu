{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/6a50f56a22b7610920dec555aa70d6d1bd4759f7/rag-advanced/notebooks/Chapter06.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-advance-chapter-04} -->\n",
    "\n",
    "## Response Synthesis and Prompting\n",
    "\n",
    "Response synthesis is a critical component of RAG systems, responsible for generating coherent and accurate answers based on retrieved information. In this chapter, we'll explore techniques to improve response quality through iterative prompt engineering and model selection.\n",
    "\n",
    "Key concepts we'll cover:\n",
    "1. Baseline prompt evaluation\n",
    "2. Iterative prompt improvement\n",
    "3. Impact of model selection on response quality\n",
    "4. Comparative analysis of different prompting strategies\n",
    "\n",
    "This hands-on experience will deepen your understanding of advanced RAG concepts and prepare you to implement these techniques in your own projects.\n",
    "\n",
    "Let's begin by setting up our environment and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, execute the following cell to clone the repository and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/wandb/edu.git\n",
    "%cd edu\n",
    "!git checkout rag-irl\n",
    "\n",
    "%cd rag-advanced\n",
    "!pip install -qqq -r requirements.txt\n",
    "%cd notebooks\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup complete, we can now proceed with the chapter content.\n",
    "\n",
    "Initial steps:\n",
    "1. Log in to Weights & Biases (W&B)\n",
    "2. Configure environment variables for API access\n",
    "\n",
    "To obtain your Cohere API key, visit the [Cohere API dashboard](https://dashboard.cohere.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Please enter your COHERE_API_KEY\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading\n",
    "We'll start by loading the semantically chunked data from Chapter 3. As a reminder, semantic chunking is an technique that groups related sentences together, preserving context and improving retrieval relevance.\n",
    "\n",
    "This chunked data will serve as the input for the knowledge base for our RAG pipeline, allowing us to compare the effectiveness of our response synthesis techniques against a baseline system.\n",
    "\n",
    "Let's load the data and take a look at the first few chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 3\n",
    "chunked_data = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/chunked_data:JL5vHMXBHyX364FC8gjNukQrCyAYbnBFa4nESjwer8g\"\n",
    ").get()\n",
    "print(chunked_data.rows[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the query enhancer, hybrid retriever, response generator and RAG pipeline from the previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "from scripts.query_enhancer import QueryEnhancer\n",
    "from scripts.rag_pipeline import QueryEnhancedRAGPipeline\n",
    "from scripts.response_generator import QueryEnhanedResponseGenerator\n",
    "from scripts.retriever import HybridRetrieverReranker\n",
    "\n",
    "query_enhancer = QueryEnhancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt iteration\n",
    "\n",
    "Prompt engineering is a crucial skill in developing effective RAG systems. By carefully crafting prompts, we can guide the model to produce more accurate, relevant, and coherent responses. We'll explore several iterations of prompt improvements:\n",
    "\n",
    "1. Baseline prompt\n",
    "2. Adding precise instructions\n",
    "3. Including response format examples\n",
    "4. Incorporating model reasoning\n",
    "\n",
    "For each iteration, we'll evaluate the impact on response quality using our established metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:Qj4IFICc2EbdXu5A5UuhkPiWgxM1GvJMIvXEyv1DYnM\"\n",
    ").get()\n",
    "\n",
    "print(eval_dataset.rows[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import ALL_METRICS as RESPONSE_METRICS\n",
    "\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=RESPONSE_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = HybridRetrieverReranker()\n",
    "hybrid_retriever.index_data(chunked_data.rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Prompt Evaluation\n",
    "\n",
    "We are now ready to evaluate the performance of the RAG pipeline while iterating over different prompt improvemtns.\n",
    "For comparison, let's begin our evaluation of the baseline RAG pipeline.\n",
    "\n",
    "This simple prompt serves as our starting point. It provides basic instructions for the model to answer questions about W&B using only the provided context. However, it lacks specific guidance on response structure, tone, or level of detail. As we iterate, we'll see how more detailed prompts can improve response quality and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\").read()\n",
    "baseline_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=INITIAL_PROMPT, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class BaselineRAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "baseline_rag_pipeline = BaselineRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=baseline_response_generator,\n",
    ")\n",
    "\n",
    "\n",
    "baseline_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(baseline_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_diff': {'mean': 0.03238267667677893},\n",
       " 'compute_levenshtein': {'mean': 0.37464633323010127},\n",
       " 'compute_rouge': {'mean': 0.19270320912651012},\n",
       " 'compute_bleu': {'mean': 0.028841742667389324},\n",
       " 'llm_response_scorer': {'score': {'mean': 0.75},\n",
       "  'correct': {'true_count': 1, 'true_fraction': 0.05}},\n",
       " 'model_latency': {'mean': 98.9712859749794}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_response_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: When designing your initial prompt, aim for clarity and simplicity. However, be prepared to iterate and refine based on the results.\n",
    "\n",
    "**Best Practice**: Always establish a baseline performance to measure improvements against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt V1: Adding Precise Instructions\n",
    "\n",
    "In our first iteration, let's enhance the prompt by providing more detailed instructions to the AI assistant. We'll focus on:\n",
    "1. Defining a clear role for the AI as a W&B specialist\n",
    "2. Incorporating dynamic elements like language and intent recognition\n",
    "3. Outlining a structured approach to formulating responses\n",
    "4. Specifying formatting requirements, including markdown usage\n",
    "5. Addressing edge cases, such as insufficient information or off-topic queries\n",
    "\n",
    "By adding these elements, we aim to guide the model towards generating more coherent, relevant, and well-structured responses. This approach should help maintain accuracy while ensuring proper citation of sources. As we progress, we'll evaluate how these changes impact the quality of the generated answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in answering questions about Weights & Biases (W&B). Your task is to provide accurate, concise, and helpful responses based on retrieved documentation snippets. Follow these instructions carefully:\n",
      "\n",
      "First, review the retrieved documentation snippets related to W&B\n",
      "Then, consider the user's query\n",
      "You should respond to the user in the following language:\n",
      "{language}\n",
      "We have identified the following intents based on the user's query:\n",
      "{intents}\n",
      "\n",
      "To formulate your response:\n",
      "1. Carefully read and understand the content of each retrieved snippet.\n",
      "2. Identify the most relevant information to answer the user's query.\n",
      "3. Pay special attention to code snippets, function names, class names, and method names.\n",
      "4. Provide a concise answer that addresses the user's query and the identified intents.\n",
      "5. Use information from the retrieved snippets to support your response.\n",
      "6. Explain code snippets, functions, classes, and methods when they are relevant to the query.\n",
      "7. Present function, class, and method names exactly as they appear in the retrieved snippets.\n",
      "8. Include relevant citations from the snippets to support your answer.\n",
      "\n",
      "Format your response as follows:\n",
      "- Use markdown formatting for your entire response.\n",
      "- Use appropriate markdown syntax for headings, lists, code blocks, and emphasis.\n",
      "- For code snippets, use triple backticks (```) with the appropriate language specifier (e.g., ```python).\n",
      "- For inline code or function/class/method names, use single backticks (`).\n",
      "- Include citations using square brackets with numbers, e.g., [1], [2], etc.\n",
      "\n",
      "If the retrieved snippets do not contain enough information to fully answer the query, state this clearly in your response and provide the best possible answer with the available information. If the query is unrelated to W&B, politely inform the user that you can only answer questions about Weights & Biases.\n",
      "\n",
      "Remember, your goal is to provide helpful, correct, and concise responses that fully address the user's query and identified intents while maintaining trustworthiness through proper citations and accurate representation of W&B documentation.\n"
     ]
    }
   ],
   "source": [
    "# Can we improve the prompt with mode precise instructions ?\n",
    "\n",
    "IMPROVED_PROMPT_V1 = open(\"prompts/improved_prompt_v1.txt\").read()\n",
    "\n",
    "print(IMPROVED_PROMPT_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Adding specific instructions and defining the AI's role can significantly improve response quality.\n",
    "\n",
    "**Best Practice**: Include guidelines for handling edge cases, such as insufficient information or off-topic queries, in your prompt design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_v1_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V1, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV1RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v1_rag_pipeline = ImprovedV1RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v1_response_generator,\n",
    ")\n",
    "\n",
    "\n",
    "improved_v1_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v1_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_diff': {'mean': 0.024506886532097615},\n",
       " 'compute_levenshtein': {'mean': 0.38012358381656036},\n",
       " 'compute_rouge': {'mean': 0.2118326998558188},\n",
       " 'compute_bleu': {'mean': 0.029722839004748796},\n",
       " 'llm_response_scorer': {'score': {'mean': 0.85},\n",
       "  'correct': {'true_count': 2, 'true_fraction': 0.1}},\n",
       " 'model_latency': {'mean': 90.24110579490662}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_v1_response_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt V2: Including Response Format Examples\n",
    "\n",
    "In this iteration, we further refine our prompt by incorporating a concrete example of a well-structured response. This addition serves several purposes:\n",
    "\n",
    "1. It demonstrates the desired formatting and structure, including proper use of markdown and code blocks.\n",
    "2. It shows how to integrate citations and reference relevant documentation.\n",
    "3. It illustrates the appropriate level of detail and explanation expected in responses.\n",
    "4. It provides a model for balancing technical accuracy with user-friendly explanations.\n",
    "\n",
    "By including this example, we aim to guide the model towards producing more consistent, well-formatted, and informative responses. This approach should help improve the overall quality and usefulness of the generated answers, making them more accessible to users with varying levels of technical expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in answering questions about Weights & Biases (W&B). Your task is to provide accurate, concise, and helpful responses based on the retrieved documentation snippets. Follow these instructions carefully:\n",
      "\n",
      "1. You will receive retrieved documentation snippets related to W&B. These snippets contain relevant information for answering the user's query.\n",
      "2. You will also be given a user query.\n",
      "3. You should respond to the user in the following language:\n",
      "{language}\n",
      "4. We have identified the following intents based on the user's query:\n",
      "{intents}\n",
      "\n",
      "5. Analyze the retrieved snippets:\n",
      "   - Carefully read and understand the content of each snippet.\n",
      "   - Identify the most relevant information to answer the user's query.\n",
      "   - Pay special attention to code snippets, function names, class names, and method names.\n",
      "\n",
      "6. Formulate your response:\n",
      "   - Provide a concise answer that addresses the user's query.\n",
      "   - Use information from the retrieved snippets to support your response.\n",
      "   - Explain code snippets, functions, classes, and methods when they are relevant to the query.\n",
      "   - Present function, class, and method names exactly as they appear in the retrieved snippets.\n",
      "   - Include relevant citations from the snippets to support your answer.\n",
      "\n",
      "7. Format your response:\n",
      "   - Use markdown formatting for your entire response.\n",
      "   - Enclose your final answer within <answer> tags.\n",
      "   - Use appropriate markdown syntax for headings, lists, code blocks, and emphasis.\n",
      "   - For code snippets, use triple backticks (```) with the appropriate language specifier (e.g., ```python).\n",
      "   - For inline code or function/class/method names, use single backticks (`).\n",
      "   - Include citations using square brackets with numbers, e.g., [1], [2], etc.\n",
      "\n",
      "8. Examples of good responses:\n",
      "\n",
      "<answer>\n",
      "# How to Log Metrics in W&B\n",
      "\n",
      "To log metrics in Weights & Biases (W&B), you can use the `wandb.log()` function. This function lets you track various metrics during your model's training process.\n",
      "\n",
      "Here's a basic example of how to use `wandb.log()`:\n",
      "\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "# Initialize a W&B run\n",
      "wandb.init(project=\"my-project\")\n",
      "\n",
      "# Train your model and log metrics\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_epoch()\n",
      "    accuracy = evaluate_model()\n",
      "    \n",
      "    wandb.log({{\n",
      "        \"epoch\": epoch,\n",
      "        \"loss\": loss,\n",
      "        \"accuracy\": accuracy\n",
      "    }})\n",
      "```\n",
      "\n",
      "In this example, we're logging three metrics: the current epoch, the loss, and the accuracy [1]. You can log any number of metrics as key-value pairs in a dictionary.\n",
      "\n",
      "Remember to call `wandb.init()` at the beginning of your script to initialize a new run [2]. This sets up the connection to the W&B servers and creates a new experiment in your project.\n",
      "\n",
      "For more advanced logging, you can also log histograms, images, and other data types. The W&B documentation provides detailed information on these features [3].\n",
      "\n",
      "References:\n",
      "\n",
      "[1] https://docs.wandb.ai/guides/track/about\n",
      "[2] https://docs.wandb.ai/guides/track/visualize\n",
      "[3] https://docs.wandb.ai/guides/track/parameters-and-sweeps/about\n",
      "</answer>\n",
      "\n",
      "9. Handling edge cases:\n",
      "   - If the retrieved snippets do not contain enough information to fully answer the query, state this clearly in your response and provide the best possible answer with the available information.\n",
      "   - If the query is unrelated to W&B, politely inform the user that you can only answer questions about Weights & Biases.\n",
      "\n",
      "Remember, your goal is to provide helpful, correct, and concise responses that fully address the user's query while maintaining trustworthiness through proper citations and accurate representation of W&B documentation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can we improve the prompt with a example of the response format ?\n",
    "\n",
    "IMPROVED_PROMPT_V2 = open(\"prompts/improved_prompt_v2.txt\").read()\n",
    "print(IMPROVED_PROMPT_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Providing concrete examples in your prompt can help guide the model towards the desired output format and structure.\n",
    "\n",
    "**Best Practice**: When including examples, ensure they demonstrate key aspects like proper citation, use of markdown, and appropriate level of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_v2_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V2, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV2RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v2_rag_pipeline = ImprovedV2RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v2_response_generator,\n",
    ")\n",
    "improved_v2_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v2_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_diff': {'mean': 0.023749018565142093},\n",
       " 'compute_levenshtein': {'mean': 0.4096138347821488},\n",
       " 'compute_rouge': {'mean': 0.2088790435692581},\n",
       " 'compute_bleu': {'mean': 0.05202531240850755},\n",
       " 'llm_response_scorer': {'score': {'mean': 0.7},\n",
       "  'correct': {'true_count': 2, 'true_fraction': 0.1}},\n",
       " 'model_latency': {'mean': 90.154217338562}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_v2_response_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Prompt V3: Incorporating Model Reasoning\n",
    "\n",
    "In this iteration, we focus on enhancing the model's reasoning process and transparency:\n",
    "\n",
    "1. We introduce a structured approach to breaking down and addressing complex queries.\n",
    "2. The prompt now explicitly requests the model to explain its thought process for each step.\n",
    "3. We emphasize the importance of providing detailed explanations, including the relevance and functionality of code elements.\n",
    "4. The example response demonstrates a clear, step-by-step structure with explanations at each stage.\n",
    "5. We've added instructions for handling edge cases more comprehensively.\n",
    "\n",
    "By encouraging the model to \"show its work,\" we aim to produce more transparent, logical, and comprehensive responses. This approach can help users better understand the reasoning behind the answers, potentially leading to improved learning outcomes and increased trust in the AI assistant's capabilities. Additionally, this structured reasoning process may help the model catch and correct its own errors, leading to more accurate and reliable responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in Weights & Biases (W&B). Your task is to provide accurate, detailed, and helpful responses using retrieved documentation snippets. Follow these instructions:\n",
      "\n",
      "1. You will receive documentation snippets and a user query.\n",
      "2. Respond in the specified language: {language}\n",
      "3. Identified intents: {intents}\n",
      "\n",
      "### Process:\n",
      "1. **Break Down the Query:** Divide the user's query into smaller steps and explain this breakdown.\n",
      "2. **Analyze Snippets:**\n",
      "   - Read each snippet.\n",
      "   - Identify relevant information and explain its importance.\n",
      "   - For code/functions/classes/methods:\n",
      "     - Explain their purpose and functionality.\n",
      "     - Describe their relevance to the query.\n",
      "     - Provide a step-by-step breakdown if applicable.\n",
      "3. **Formulate Response:**\n",
      "   - Address each query step with detailed explanations.\n",
      "   - Use snippets to support your response.\n",
      "   - Break down code explanations into logical steps.\n",
      "   - Use exact names from snippets for functions/classes/methods.\n",
      "   - Include citations [1], [2], etc.\n",
      "4. **Format Response:**\n",
      "   - Use markdown for headings, lists, code blocks, and emphasis.\n",
      "   - Enclose the final answer in <answer> tags.\n",
      "   - Use triple backticks for code (e.g., ```python).\n",
      "   - Use inline code formatting for function/class/method names.\n",
      "5. **Structure Response:**\n",
      "   - Overview of approach.\n",
      "   - For each step:\n",
      "     - State the step.\n",
      "     - Explain your thought process.\n",
      "     - Provide relevant information.\n",
      "     - Summarize the step's contribution to the overall answer.\n",
      "   - Conclude with a summary.\n",
      "\n",
      "### Example:\n",
      "\n",
      "<answer>\n",
      "# Logging Metrics in W&B\n",
      "\n",
      "### Approach:\n",
      "1. Define metrics.\n",
      "2. Explain basic logging method.\n",
      "3. Provide a code example.\n",
      "4. Discuss advanced features.\n",
      "\n",
      "### 1. Define Metrics\n",
      "Metrics in W&B are numerical values tracked during model training/evaluation, such as loss and accuracy [1].\n",
      "\n",
      "### 2. Basic Logging Method\n",
      "Use `wandb.log()` to log metrics. It takes a dictionary of key-value pairs (metrics) and sends data to W&B servers for visualization [2].\n",
      "\n",
      "### 3. Code Example\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "wandb.init(project=\"my-project\")\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_epoch()\n",
      "    accuracy = evaluate_model()\n",
      "    \n",
      "    wandb.log({{\n",
      "        \"epoch\": epoch,\n",
      "        \"loss\": loss,\n",
      "        \"accuracy\": accuracy\n",
      "    }})\n",
      "```\n",
      "- Initialize W&B with `wandb.init()`.\n",
      "- Log metrics with `wandb.log()` in each epoch [3].\n",
      "\n",
      "### 4. Advanced Features\n",
      "Log histograms, images, audio, and video for richer visualizations [4].\n",
      "```python\n",
      "wandb.log({{\"histogram\": wandb.Histogram(numpy_array)}})\n",
      "wandb.log({{\"image\": wandb.Image(numpy_array)}})\n",
      "```\n",
      "\n",
      "### Conclusion\n",
      "Consistently logging metrics with `wandb.log()` helps track model performance and make data-driven decisions.\n",
      "\n",
      "References:\n",
      "[1] https://docs.wandb.ai/guides/track/about\n",
      "[2] https://docs.wandb.ai/guides/track/visualize\n",
      "[3] https://docs.wandb.ai/guides/track/parameters-and-sweeps/about\n",
      "[4] https://docs.wandb.ai/guides/track/advanced-logging\n",
      "</answer>\n",
      "\n",
      "### Handling Edge Cases:\n",
      "- If snippets lack enough information:\n",
      "  - State this limitation.\n",
      "  - Provide the best partial answer.\n",
      "  - Suggest sources or methods to find missing info.\n",
      "- If the query is unrelated to W&B:\n",
      "  - Inform the user and explain why.\n",
      "  - Suggest how to rephrase the question to relate to W&B.\n",
      "\n",
      "Your goal is to provide helpful, correct, and detailed responses, maintaining trustworthiness through proper citations and accurate representation of W&B documentation. Always show your reasoning process.\n"
     ]
    }
   ],
   "source": [
    "# Can we further improve the prompt to inlcude model reasoning ?\n",
    "\n",
    "\n",
    "IMPROVED_PROMPT_V3 = open(\"prompts/improved_prompt_v3.txt\").read()\n",
    "\n",
    "print(IMPROVED_PROMPT_V3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Encouraging the model to explain its reasoning process can lead to more transparent and logical responses.\n",
    "\n",
    "**Best Practice**: Structure your prompt to guide the model through a step-by-step approach for complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_v3_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V3, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV3RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v3_rag_pipeline = ImprovedV3RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v3_response_generator,\n",
    ")\n",
    "\n",
    "improved_v3_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v3_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Improvement: Leveraging Advanced Language Models\n",
    "\n",
    "After iterating on our prompt engineering, we now take the next step by utilizing a more advanced language model (command-r-plus). This change demonstrates an important principle in RAG system development: the synergy between prompt design and model capability. By combining our refined prompt with a more sophisticated model, we aim to:\n",
    "\n",
    "1. Improve the overall quality and coherence of generated responses\n",
    "2. Enhance the model's ability to understand and follow complex instructions\n",
    "3. Potentially increase the accuracy and depth of domain-specific knowledge\n",
    "4. Better handle nuanced queries and edge cases\n",
    "\n",
    "This step allows us to explore how model selection interacts with prompt engineering to affect response quality. As we evaluate the results, we'll gain insights into the relative impact of prompt refinement versus model capabilities in our RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Don't rely solely on prompt engineering; consider the capabilities of different models in your iterative improvement process.\n",
    "\n",
    "**Best Practice**: Balance the trade-off between response quality and latency based on your specific use-case requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we further imporve by using a better model to generate the response ?\n",
    "\n",
    "improved_v4_response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r-plus\", prompt=IMPROVED_PROMPT_V3, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "class ImprovedV4RAGPipeline(QueryEnhancedRAGPipeline):\n",
    "    pass\n",
    "\n",
    "\n",
    "improved_v4_rag_pipeline = ImprovedV4RAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=improved_v4_response_generator,\n",
    ")\n",
    "\n",
    "improved_v4_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(improved_v4_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_diff': {'mean': 0.02415209754349641},\n",
       " 'compute_levenshtein': {'mean': 0.42665032163531286},\n",
       " 'compute_rouge': {'mean': 0.2142839488379749},\n",
       " 'compute_bleu': {'mean': 0.0703182494040341},\n",
       " 'llm_response_scorer': {'score': {'mean': 0.8},\n",
       "  'correct': {'true_count': 2, 'true_fraction': 0.1}},\n",
       " 'model_latency': {'mean': 117.70810431241989}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_v4_response_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Evaluations\n",
    "\n",
    "Comparing the performance of different RAG pipeline iterations is crucial for understanding the impact of our prompt engineering efforts. By comparing metrics across various versions, we can identify trends, improvements, and potential trade-offs. This comparative analysis helps us make informed decisions about which prompting strategies are most effective for our specific use case. It's important to consider both quantitative metrics (like accuracy scores) and qualitative aspects (such as response relevance) when assessing overall performance improvements.\n",
    "\n",
    "**Tip**: Use multiple evaluation metrics to get a comprehensive view of your system's performance.\n",
    "\n",
    "**Best Practice**: Regularly reassess and refine your prompts as you gather more data on user queries and system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_retriever_responses](../images/06_compare_prompts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing RAG Pipeline Iterations\n",
    "\n",
    "Here are a few key insights from the evaluation of the RAG pipeline iterations:\n",
    "\n",
    "1. **Response Quality Improvement**: The ImprovedV3 pipelines significantly outperformed earlier versions in LLM Response Scorer metrics (0.95 vs 0.75 for baseline), indicating substantial improvements in response quality and correctness.\n",
    "\n",
    "2. **Trade-off Between Quality and Latency**: While the later iterations (V3 and V4) produced higher quality responses, they also exhibited increased latency. This highlights a common trade-off in AI systems between performance and computational efficiency.\n",
    "\n",
    "3. **Incremental Gains**: Each iteration showed improvements in various metrics, demonstrating the value of iterative refinement in prompt engineering and model selection.\n",
    "\n",
    "4. **Metric Variability**: Some metrics (e.g., Levenshtein distance) showed unexpected increases in later iterations, reminding us that different evaluation metrics can capture different aspects of performance.\n",
    "\n",
    "### Learnings\n",
    "\n",
    "1. Prompt engineering can significantly impact response quality without changing the underlying model.\n",
    "2. Combining refined prompts with more advanced models (as in V4) can lead to synergistic improvements.\n",
    "3. The choice of evaluation metrics is crucial; a holistic view using multiple metrics provides a more comprehensive understanding of system performance.\n",
    "4. In real-world applications, the balance between response quality and latency must be carefully considered based on specific use-case requirements.\n",
    "\n",
    "This evaluation underscores the complexity of optimizing RAG systems and the importance of comprehensive, multi-faceted assessment approaches in AI development.\n",
    "\n",
    "\n",
    "**Overall Best Practice**: \"Iterative improvement is key in RAG system development. Continuously analyze results, gather feedback, and refine both prompts and model selection.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. Iterative Prompt Engineering: Systematic refinement of prompts can significantly enhance response quality without changing the underlying model.\n",
    "\n",
    "2. Structured Instructions: Clear, detailed prompts with specific roles, formatting guidelines, and edge case handling improve response coherence and relevance.\n",
    "\n",
    "3. Example Integration: Including well-crafted examples in prompts helps guide the model towards desired output structure and content quality.\n",
    "\n",
    "4. Reasoning Transparency: Prompting the model to explain its thought process leads to more logical, comprehensive, and trustworthy responses.\n",
    "\n",
    "5. Model-Prompt Synergy: Combining refined prompts with more advanced language models can yield synergistic improvements in response quality.\n",
    "\n",
    "6. Performance Trade-offs: Higher quality responses often come at the cost of increased latency. Balance these factors based on specific use-case requirements.\n",
    "\n",
    "7. Multifaceted Evaluation: Use a combination of metrics to comprehensively assess improvements, as different aspects of performance may not all improve uniformly.\n",
    "\n",
    "8. Continuous Optimization: RAG system development is an ongoing process. Regularly reassess and refine prompts based on performance data and user feedback.\n",
    "\n",
    "9. Scalability and Efficiency: As prompt complexity increases, consider the impact on system efficiency and scalability in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
