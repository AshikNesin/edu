{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Synthesis and Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "import wandb\n",
    "import weave\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparambharat\u001b[0m (\u001b[33mrag-course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/edu/rag-advanced/notebooks/wandb/run-20240730_122350-1kus5rxx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rag-course/dev/runs/1kus5rxx' target=\"_blank\">worthy-night-99</a></strong> to <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">https://wandb.ai/rag-course/dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rag-course/dev/runs/1kus5rxx' target=\"_blank\">https://wandb.ai/rag-course/dev/runs/1kus5rxx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: parambharat.\n",
      "View Weave data at https://wandb.ai/rag-course/dev/weave\n"
     ]
    }
   ],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 6\",\n",
    ")\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/30 12:23:55 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/j8uh2i2o/artifact/961260984/wandb_manifest.json?Expires=1722326035&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=K46MULEweoG3AQ7DX%2BD092siUiwU%2BdLSJe7AGvy6caJW4MzCQsGvni5i6UKoC2JgAYwgb8FyXu4Q208Tu%2F1BITZ8Qoxtfut3kmCQdimC39rMJgDru%2Bs45JHQGiZDIhaOlNDl0vXE0WLhGpsqbamJXlQNBHwGoBxG7fio87m7pM%2Fx12wLaxVQWKRHeoHsrN%2FbYtI8oBeuZVlnb7D0jSwDm0nnbh9FAZxkoHg2%2Bb2QnoVJhA%2BwMy6UwAd%2BfW76LagrGTT7%2FuG6t7j3ho8pEPadeeNt7FH1fNga%2FJs4qXzC0A1TGWEzSNT3DMVNLEHcDalQ9DtnLOt1fC%2FIYelCcm1csA%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'cleaned_content': 'Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with wandb.init(anonymous=\"allow\") :::info Publishing a paper? Please cite W&B, and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com.\\n::: How does someone without an account see results? If someone runs your script and you have to set anonymous=\"allow\":  Auto-create temporary account: W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. Log results quickly: The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI.\\nThese unclaimed anonymous runs will be available for 7 days. Claim data when it\\'s useful: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days.  :::caution Anonymous run links are sensitive. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case.\\n::: What happens to users with existing accounts? If you set anonymous=\"allow\" in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run.\\nWhat are features that aren\\'t available to anonymous users? No persistent data: Runs are only saved for 7 days in an anonymous account.\\nUsers can claim anonymous run data by saving it to a real account.',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md', 'parsed_tokens': 468}},\n",
       " {'cleaned_content': \"No artifact logging: Runs will print a warning on the command line that you can't log an artifact to an anonymous run. No profile or settings pages: Certain pages aren't available in the UI, because they're only useful for real accounts.\",\n",
       "  'metadata': {'source': 'guides/app/features/anon.md', 'parsed_tokens': 49}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the data from Chapter 3\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1bbf33734be4dd9b8a154ca9296ec0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.retriever import HybridRetrieverReranker\n",
    "# Using the query enhancer, response generator, and RAG pipeline from the previous chapter\n",
    "\n",
    "import cohere\n",
    "from scripts.query_enhancer import QueryEnhancer\n",
    "from scripts.response_generator import QueryEnhanedResponseGenerator\n",
    "from scripts.rag_pipeline import QueryEnhancedRAGPipeline\n",
    "\n",
    "query_enhancer = QueryEnhancer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in answering questions about Weights & Biases (W&B). Your task is to provide accurate, concise, and helpful responses based on retrieved documentation snippets. Follow these instructions carefully:\n",
      "\n",
      "First, review the retrieved documentation snippets related to W&B\n",
      "Then, consider the user's query\n",
      "You should respond to the user in the following language:\n",
      "{language}\n",
      "We have identified the following intents based on the user's query:\n",
      "{intents}\n",
      "\n",
      "To formulate your response:\n",
      "1. Carefully read and understand the content of each retrieved snippet.\n",
      "2. Identify the most relevant information to answer the user's query.\n",
      "3. Pay special attention to code snippets, function names, class names, and method names.\n",
      "4. Provide a concise answer that addresses the user's query and the identified intents.\n",
      "5. Use information from the retrieved snippets to support your response.\n",
      "6. Explain code snippets, functions, classes, and methods when they are relevant to the query.\n",
      "7. Present function, class, and method names exactly as they appear in the retrieved snippets.\n",
      "8. Include relevant citations from the snippets to support your answer.\n",
      "\n",
      "Format your response as follows:\n",
      "- Use markdown formatting for your entire response.\n",
      "- Use appropriate markdown syntax for headings, lists, code blocks, and emphasis.\n",
      "- For code snippets, use triple backticks (```) with the appropriate language specifier (e.g., ```python).\n",
      "- For inline code or function/class/method names, use single backticks (`).\n",
      "- Include citations using square brackets with numbers, e.g., [1], [2], etc.\n",
      "\n",
      "If the retrieved snippets do not contain enough information to fully answer the query, state this clearly in your response and provide the best possible answer with the available information. If the query is unrelated to W&B, politely inform the user that you can only answer questions about Weights & Biases.\n",
      "\n",
      "Remember, your goal is to provide helpful, correct, and concise responses that fully address the user's query and identified intents while maintaining trustworthiness through proper citations and accurate representation of W&B documentation.\n"
     ]
    }
   ],
   "source": [
    "# lets improve the prompt with mode precise instructions\n",
    "\n",
    "IMPROVED_PROMPT_V1 = open(\"prompts/improved_prompt_v1.txt\").read()\n",
    "\n",
    "print(IMPROVED_PROMPT_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V1, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "hybrid_retriever = HybridRetrieverReranker()\n",
    "\n",
    "hybrid_retriever.index_data(chunked_data)\n",
    "\n",
    "rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=response_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:9O0EmmPINmYjgbXW3kucVrDxlTUQJQs0fVZYJj2mtOk\"\n",
    ").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeaveDict({'question': 'why did wandb stop logging model parameters with a huggingface model.  I have used in the past and it worked.  Maybe I changed an environement or config, how do I reset?', 'answer': 'It sounds like you\\'re experiencing issues with Weights & Biases not logging model parameters when using a Hugging Face model. This could be due to several reasons, such as changes in environment variables or configuration settings. Here are some steps to troubleshoot and reset your configuration:\\n\\n### 1. Check `WANDB_WATCH` Environment Variable\\nEnsure that the `WANDB_WATCH` environment variable is set correctly. This variable controls whether model gradients and parameters are logged. You should set it to `all` to log both gradients and parameters.\\n\\n```python\\nimport os\\nos.environ[\\'WANDB_WATCH\\'] = \\'all\\'  # Log histograms of gradients and parameters\\n```\\n\\nOr, you can set this in your terminal before running your script:\\n```bash\\nexport WANDB_WATCH=all\\n```\\n\\n### 2. Verify Integration in Your Script\\nMake sure that the `report_to` argument in your `TrainingArguments` or script is set to `wandb`. This ensures that the Hugging Face integration with Weights & Biases is active.\\n\\n```python\\nfrom transformers import TrainingArguments\\n\\ntraining_args = TrainingArguments(\\n    report_to=\"wandb\",\\n    ...\\n)\\n```\\n\\n### 3. Check for Any Overrides in Your Code\\nSometimes, specific settings in your code might override the environment variables. Look for any lines in your script that might be setting `WANDB_WATCH` to `false` or modifying other related settings.\\n\\n### 4. Reset W&B Environment Variables\\nIf you suspect that your environment variables are not set correctly, you can reset them. Here\\'s how you can set them to typical values for logging with Hugging Face:\\n\\n```python\\n# Set environment variables\\nos.environ[\\'WANDB_WATCH\\'] = \\'all\\'  # Ensure all parameters and gradients are logged\\nos.environ[\\'WANDB_LOG_MODEL\\'] = \\'true\\'  # Log model as an artifact\\nos.environ[\\'WANDB_PROJECT\\'] = \\'your_project_name\\'  # Specify your project name\\n```\\n\\n### 5. Re-login to W&B\\nSometimes, simply re-authenticating can resolve issues. Make sure you are logged in to W&B:\\n\\n```python\\nimport wandb\\nwandb.login()\\n```\\n\\n### 6. Update W&B and Hugging Face Libraries\\nEnsure that you are using the latest versions of both `wandb` and `transformers` libraries, as updates often fix bugs and improve integration features.\\n\\n```bash\\npip install --upgrade wandb transformers\\n```\\n\\n### 7. Review W&B Dashboard\\nCheck your W&B dashboard to see if there are any error messages or warnings that might give you more context about what\\'s going wrong.\\n\\nBy following these steps, you should be able to resolve the issue with logging model parameters in Weights & Biases when using a Hugging Face model. If the problem persists, consider reaching out to W&B support or checking the [community forums](https://wandb.me/community) for more help.', 'contexts': [{'content': 'Additional W&B settings\\nFurther configuration of what is logged with `Trainer` is possible by setting environment variables. A full list of W&B environment variables can be found here.\\nEnvironment Variable\\nUsage\\n`WANDB_PROJECT`\\nGive your project a name (`huggingface` by default)\\n`WANDB_LOG_MODEL`\\nLog the model checkpoint as a W&B Artifact (`false` by default) \\n`false` (default): No model checkpointing `checkpoint`: A checkpoint will be uploaded every args.save_steps (set in the Trainer\\'s TrainingArguments). `end`: The final model checkpoint will be uploaded at the end of training.\\n`WANDB_WATCH`\\nSet whether you\\'d like to log your models gradients, parameters or neither\\n`false` (default): No gradient or parameter logging `gradients`: Log histograms of the gradients `all`: Log histograms of gradients and parameters\\n`WANDB_DISABLED`\\nSet to `true` to disable logging entirely (`false` by default)\\n`WANDB_SILENT`\\nSet to `true` to silence the output printed by wandb (`false` by default)\\nWANDB_WATCH=all\\nWANDB_SILENT=true\\n%env WANDB_WATCH=all\\n%env WANDB_SILENT=true\\nCustomize wandb.init\\nThe `WandbCallback` that `Trainer` uses will call `wandb.init` under the hood when `Trainer` is initialized. You can alternatively set up your runs manually by calling `wandb.init` before the`Trainer` is initialized. This gives you full control over your W&B run configuration.\\nAn example of what you might want to pass to `init` is below. For more details on how to use `wandb.init`, check out the reference documentation.\\nwandb.init(\\n    project=\"amazon_sentiment_analysis\",\\n    name=\"bert-base-high-lr\",\\n    tags=[\"baseline\", \"high-lr\"],\\n    group=\"bert\",\\n)', 'source': 'guides/integrations/huggingface.md', 'score': 0.96260285, 'relevance': 2}, {'content': '4) Turn on model checkpointing\\nUsing Weights & Biases\\' Artifacts, you can store up to 100GB of models and datasets for free and then use the Weights & Biases Model Registry to register models to prepare them for staging or deployment in your production environment.\\nLogging your Hugging Face model checkpoints to Artifacts can be done by setting the `WANDB_LOG_MODEL` environment variable to one of `end` or `checkpoint` or `false`: \\n`checkpoint`: a checkpoint will be uploaded every `args.save_steps` from the TrainingArguments. \\n`end`:  the model will be uploaded at the end of training. \\nUse `WANDB_LOG_MODEL` along with `load_best_model_at_end` to upload the best model at the end of training.\\nimport os\\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\\nWANDB_LOG_MODEL=\"checkpoint\"\\n%env WANDB_LOG_MODEL=\"checkpoint\"\\nAny Transformers `Trainer` you initialize from now on will upload models to your W&B project. The model checkpoints you log will be viewable through the Artifacts UI, and include the full model lineage (see an example model checkpoint in the UI here. \\n:::info\\nBy default, your model will be saved to W&B Artifacts as `model-{run_id}` when `WANDB_LOG_MODEL` is set to `end` or `checkpoint-{run_id}` when `WANDB_LOG_MODEL` is set to `checkpoint`.\\nHowever, If you pass a run_name in your `TrainingArguments`, the model will be saved as `model-{run_name}` or `checkpoint-{run_name}`.\\n:::\\nW&B Model Registry\\nOnce you have logged your checkpoints to Artifacts, you can then register your best model checkpoints and centralize them across your team using the Weights & Biases Model Registry. Here you can organize your best models by task, manage model lifecycle, facilitate easy tracking and auditing throughout the ML lifecyle, and automate downstream actions with webhooks or jobs. \\nSee the Model Registry documentation for how to link a model Artifact to the Model Registry.', 'source': 'guides/integrations/huggingface.md', 'score': 0.6389479, 'relevance': 2}, {'content': 'Loading a saved model\\nIf you saved your model to W&B Artifacts with `WANDB_LOG_MODEL`, you can download your model weights for additional training or to run inference. You just load them back into the same Hugging Face architecture that you used before.\\n# Create a new run\\nwith wandb.init(project=\"amazon_sentiment_analysis\") as run:\\n    # Pass the name and version of Artifact\\n    my_model_name = \"model-bert-base-high-lr:latest\"\\n    my_model_artifact = run.use_artifact(my_model_name)\\n    # Download model weights to a folder and return the path\\n    model_dir = my_model_artifact.download()\\n    # Load your Hugging Face model from that folder\\n    #  using the same model class\\n    model = AutoModelForSequenceClassification.from_pretrained(\\n        model_dir, num_labels=num_labels\\n    )\\n    # Do additional training, or run inference\\nResume training from a checkpoint\\nIf you had set `WANDB_LOG_MODEL=\\'checkpoint\\'` you can also resume training by you can using the `model_dir` as the `model_name_or_path` argument in your `TrainingArguments` and pass `resume_from_checkpoint=True` to `Trainer`.\\nlast_run_id = \"xxxxxxxx\"  # fetch the run_id from your wandb workspace\\n# resume the wandb run from the run_id\\nwith wandb.init(\\n    project=os.environ[\"WANDB_PROJECT\"],\\n    id=last_run_id,\\n    resume=\"must\",\\n) as run:\\n    # Connect an Artifact to the run\\n    my_checkpoint_name = f\"checkpoint-{last_run_id}:latest\"\\n    my_checkpoint_artifact = run.use_artifact(my_model_name)\\n    # Download checkpoint to a folder and return the path\\n    checkpoint_dir = my_checkpoint_artifact.download()\\n    # reinitialize your model and trainer\\n    model = AutoModelForSequenceClassification.from_pretrained(\\n        \"<model_name>\", num_labels=num_labels\\n    )\\n    # your awesome training arguments here.\\n    training_args = TrainingArguments()\\n    trainer = Trainer(model=model, args=training_args)\\n    # make sure use the checkpoint dir to resume training from the checkpoint\\n    trainer.train(resume_from_checkpoint=checkpoint_dir)', 'source': 'guides/integrations/huggingface.md', 'score': 0.684053, 'relevance': 1}, {'content': \"Hugging Face\\nTry in a Colab Notebook here →\\nVisualize your Hugging Face model's performance quickly with a seamless W&B integration.\\nCompare hyperparameters, output metrics, and system stats like GPU utilization across your models. \\n🤔 Why should I use W&B?\\nUnified dashboard: Central repository for all your model metrics and predictions\\nLightweight: No code changes required to integrate with Hugging Face\\nAccessible: Free for individuals and academic teams\\nSecure: All projects are private by default\\nTrusted: Used by machine learning teams at OpenAI, Toyota, Lyft and more\\nThink of W&B like GitHub for machine learning models— save machine learning experiments to your private, hosted dashboard. Experiment quickly with the confidence that all the versions of your models are saved for you, no matter where you're running your scripts.\\nW&B lightweight integrations works with any Python script, and all you need to do is sign up for a free W&B account to start tracking and visualizing your models.\\nIn the Hugging Face Transformers repo, we've instrumented the Trainer to automatically log training and evaluation metrics to W&B at each logging step.\\nHere's an in depth look at how the integration works: Hugging Face + W&B Report.\\n🚀 Install, Import, and Log in\\nInstall the Hugging Face and Weights & Biases libraries, and the GLUE dataset and training script for this tutorial.\\n- Hugging Face Transformers: Natural language models and datasets\\n- Weights & Biases: Experiment tracking and visualization\\n- GLUE dataset: A language understanding benchmark dataset\\n- GLUE script: Model training script for sequence classification\\n!pip install datasets wandb evaluate accelerate -qU\\n!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py\\n# the run_glue.py script requires transformers dev\\n!pip install -q git+https://github.com/huggingface/transformers\\n🖊️ Sign up for a free account →\\n🔑 Put in your API key\\nOnce you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook.\\nimport wandb\\nwandb.login()\\nOptionally, we can set environment variables to customize W&B logging. See documentation.\\n# Optional: log both gradients and parameters\\n%env WANDB_WATCH=all\", 'source': 'tutorials/huggingface.md', 'score': 0.6166473, 'relevance': 1}, {'content': 'Hugging Face Transformers\\nThe Hugging Face Transformers library makes state-of-the-art NLP models like BERT and training techniques like mixed precision and gradient checkpointing easy to use. The W&B integration adds rich, flexible experiment tracking and model versioning to interactive centralized dashboards without compromising that ease of use.\\n🤗 Next-level logging in few lines\\nos.environ[\"WANDB_PROJECT\"] = \"<my-amazing-project>\"  # name your W&B project\\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\\nfrom transformers import TrainingArguments, Trainer\\nargs = TrainingArguments(..., report_to=\"wandb\")  # turn on W&B logging\\ntrainer = Trainer(..., args=args)\\n:::info\\nIf you\\'d rather dive straight into working code, check out this Google Colab.\\n:::\\nGetting started: track experiments\\n1) Sign Up, install the wandb library and log in\\na) Sign up for a free account\\nb) Pip install the `wandb` library\\nc) To log in in your training script, you\\'ll need to be signed in to you account at www.wandb.ai, then you will find your API key on the Authorize page.\\nIf you are using Weights and Biases for the first time you might want to check out our quickstart\\npip install wandb\\nwandb login\\n!pip install wandb\\nimport wandb\\nwandb.login()\\n2) Name the project\\nA Project is where all of the charts, data, and models logged from related runs are stored. Naming your project helps you organize your work and keep all the information about a single project in one place.\\nTo add a run to a project simply set the `WANDB_PROJECT` environment variable to the name of your project. The `WandbCallback` will pick up this project name environment variable and use it when setting up your run.\\nWANDB_PROJECT=amazon_sentiment_analysis\\n%env WANDB_PROJECT=amazon_sentiment_analysis\\nimport os\\nos.environ[\"WANDB_PROJECT\"]=\"amazon_sentiment_analysis\"\\n:::info\\nMake sure you set the project name before you initialize the `Trainer`.\\n:::\\nIf a project name is not specified the project name defaults to \"huggingface\".', 'source': 'guides/integrations/huggingface.md', 'score': 0.2724115, 'relevance': 1}, {'content': \"Run The Library With wandb As Optional\\nIf you want to make `wandb` optional when your users use your library, you can either:\\nDefine a `wandb` flag such as:\\ntrainer = my_trainer(..., use_wandb=True)\\npython train.py ... --use-wandb\\nOr, set `wandb` to be disabled in `wandb.init`\\nwandb.init(mode=“disabled”)\\nexport WANDB_MODE=disabled\\nor\\nwandb disabled\\nOr, set `wandb` to be offline - note this will still run `wandb`, it just won't try and communicate back to W&B over the internet\\nexport WANDB_MODE=offline\\nor\\nos.environ['WANDB_MODE'] = 'offline'\\nwandb offline\\nDefining A wandb Run Config\\nWith a `wandb` run config you can provide metadata about your model, dataset, and so on when you create a W&B Run. You can use this information to compare different experiments and quickly understand what are the main differences.\\nTypical config parameters you can log include:\\nModel name, version, architecture parameters etc\\nDataset name, version, number of train/val examples etc\\nTraining parameters such as learning rate, batch size, optimizer etc\\nThe following code snippet shows how to log a config:\\nconfig = {“batch_size”:32, …}\\nwandb.init(…, config=config)\\nUpdating The wandb config\\nUse `wandb.config.update` to update the config. Updating your configuration dictionary is useful when parameters are obtained after the dictionary was defined, for example you might want to add a model’s parameters after the model is instantiated.\\nwandb.config.update({“model_parameters” = 3500})\\nFor more information on how to define a config file, see Configure Experiments with wandb.config\\nLogging To W&B\", 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.16344544, 'relevance': 0}, {'content': 'import os\\nimport wandb\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nconfig = {\"optimizer\": \"adam\", \"loss\": \"categorical_crossentropy\"}\\n# Initialize a W&B run\\nrun = wandb.init(entity=\"charlie\", project=\"mnist-experiments\", config=config)\\n# Hyperparameters\\nloss = run.config[\"loss\"]\\noptimizer = run.config[\"optimizer\"]\\nmetrics = [\"accuracy\"]\\nnum_classes = 10\\ninput_shape = (28, 28, 1)\\n# Training algorithm\\nmodel = keras.Sequential(\\n    [\\n        layers.Input(shape=input_shape),\\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Flatten(),\\n        layers.Dropout(0.5),\\n        layers.Dense(num_classes, activation=\"softmax\"),\\n    ]\\n)\\n# Configure the model for training\\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\\n# Save model\\nmodel_filename = \"model.h5\"\\nlocal_filepath = \"./\"\\nfull_path = os.path.join(local_filepath, model_filename)\\nmodel.save(filepath=full_path)\\n# Log the model to the W&B run\\nrun.log_model(path=full_path, name=\"MNIST\")\\nrun.finish()\\nWhen the user called `log_model`, a model artifact named `MNIST` was created and the file `model.h5` was added to the model artifact. Your terminal or notebook will print information of where to find information about the run the model was logged to.\\nView run different-surf-5 at: https://wandb.ai/charlie/mnist-experiments/runs/wlby6fuw\\nSynced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\\nFind logs at: ./wandb/run-20231206_103511-wlby6fuw/logs', 'source': 'guides/track/log/log-models.md', 'score': 0.07172112, 'relevance': 0}, {'content': '👟 Train the model\\nNext, call the downloaded training script run_glue.py and see training automatically get tracked to the Weights & Biases dashboard. This script fine-tunes BERT on the Microsoft Research Paraphrase Corpus— pairs of sentences with human annotations indicating whether they are semantically equivalent.\\n%env WANDB_PROJECT=huggingface-demo\\n%env TASK_NAME=MRPC\\n!python run_glue.py \\\\\\n  --model_name_or_path bert-base-uncased \\\\\\n  --task_name $TASK_NAME \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_seq_length 256 \\\\\\n  --per_device_train_batch_size 32 \\\\\\n  --learning_rate 2e-4 \\\\\\n  --num_train_epochs 3 \\\\\\n  --output_dir /tmp/$TASK_NAME/ \\\\\\n  --overwrite_output_dir \\\\\\n  --logging_steps 50\\n👀 Visualize results in dashboard\\nClick the link printed out above, or go to wandb.ai to see your results stream in live. The link to see your run in the browser will appear after all the dependencies are loaded — look for the following output: \"wandb: 🚀 View run at [URL to your unique run]\"\\nVisualize Model Performance\\nIt\\'s easy to look across dozens of experiments, zoom in on interesting findings, and visualize highly dimensional data.\\nCompare Architectures\\nHere\\'s an example comparing BERT vs DistilBERT — it\\'s easy to see how different architectures effect the evaluation accuracy throughout training with automatic line plot visualizations.\\n📈 Track key information effortlessly by default\\nWeights & Biases saves a new run for each experiment. Here\\'s the information that gets saved by default:\\n- Hyperparameters: Settings for your model are saved in Config\\n- Model Metrics: Time series data of metrics streaming in are saved in Log\\n- Terminal Logs: Command line outputs are saved and available in a tab\\n- System Metrics: GPU and CPU utilization, memory, temperature etc.\\n🤓 Learn more!\\nDocumentation: docs on the Weights & Biases and Hugging Face integration\\nVideos: tutorials, interviews with practitioners, and more on our YouTube channel\\nContact: Message us at contact@wandb.com with questions', 'source': 'tutorials/huggingface.md', 'score': 0.06359858, 'relevance': 0}, {'content': 'Keras Models\\nTry in a Colab Notebook here →\\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\\nThis colab notebook introduces the `WandbModelCheckpoint` callback. Use this callback to log your model checkpoints to Weight and Biases Artifacts.\\n🌴 Setup and Installation\\nFirst, let us install the latest version of Weights and Biases. We will then authenticate this colab instance to use W&B.\\n!pip install -qq -U wandb\\nimport os\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers\\nfrom tensorflow.keras import models\\nimport tensorflow_datasets as tfds\\n# Weights and Biases related imports\\nimport wandb\\nfrom wandb.integration.keras import WandbMetricsLogger\\nfrom wandb.integration.keras import WandbModelCheckpoint\\nIf this is your first time using W&B or you are not logged in, the link that appears after running `wandb.login()` will take you to sign-up/login page. Signing up for a free account is as easy as a few clicks.\\nwandb.login()\\n🌳 Hyperparameters\\nUse of proper config system is a recommended best practice for reproducible machine learning. We can track the hyperparameters for every experiment using W&B. In this colab we will be using simple Python `dict` as our config system.\\nconfigs = dict(\\n    num_classes = 10,\\n    shuffle_buffer = 1024,\\n    batch_size = 64,\\n    image_size = 28,\\n    image_channels = 1,\\n    earlystopping_patience = 3,\\n    learning_rate = 1e-3,\\n    epochs = 10\\n)', 'source': 'tutorials/keras_models.md', 'score': 0.044265877, 'relevance': 0}, {'content': \"MMF\\nThe `WandbLogger` class in Meta AI's MMF library will enable Weights & Biases to log the training/validation metrics, system (GPU and CPU) metrics, model checkpoints and configuration parameters.\\nCurrent features\\nThe following features are currently supported by the `WandbLogger` in MMF:\\nTraining & Validation metrics\\nLearning Rate over time\\nModel Checkpoint saving to W&B Artifacts\\nGPU and CPU system metrics\\nTraining configuration parameters\\nConfig parameters\\nThe following options are available in MMF config to enable and customize the wandb logging:\\ntraining:\\n    wandb:\\n        enabled: true\\n        # An entity is a username or team name where you're sending runs.\\n        # By default it will log the run to your user account.\\n        entity: null\\n        # Project name to be used while logging the experiment with wandb\\n        project: mmf\\n        # Experiment/ run name to be used while logging the experiment\\n        # under the project with wandb. The default experiment name\\n        # is: ${training.experiment_name}\\n        name: ${training.experiment_name}\\n        # Turn on model checkpointing, saving checkpoints to W&B Artifacts\\n        log_model_checkpoint: true\\n        # Additional argument values that you want to pass to wandb.init(). \\n        # Check out the documentation at https://docs.wandb.ai/ref/python/init\\n        # to see what arguments are available, such as:\\n        # job_type: 'train'\\n        # tags: ['tag1', 'tag2']\\nenv:\\n    # To change the path to the directory where wandb metadata would be \\n    # stored (Default: env.log_dir):\\n    wandb_logdir: ${env:MMF_WANDB_LOGDIR,}\", 'source': 'guides/integrations/other/mmf.md', 'score': 0.027117657000000003, 'relevance': 0}]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.rows[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import ALL_METRICS as RESPONSE_METRICS\n",
    "\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset.rows[:10],\n",
    "    scorers=RESPONSE_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "query_enhanced_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in answering questions about Weights & Biases (W&B). Your task is to provide accurate, concise, and helpful responses based on the retrieved documentation snippets. Follow these instructions carefully:\n",
      "\n",
      "1. You will receive retrieved documentation snippets related to W&B. These snippets contain relevant information for answering the user's query.\n",
      "2. You will also be given a user query.\n",
      "3. You should respond to the user in the following language:\n",
      "{language}\n",
      "4. We have identified the following intents based on the user's query:\n",
      "{intents}\n",
      "\n",
      "5. Analyze the retrieved snippets:\n",
      "   - Carefully read and understand the content of each snippet.\n",
      "   - Identify the most relevant information to answer the user's query.\n",
      "   - Pay special attention to code snippets, function names, class names, and method names.\n",
      "\n",
      "6. Formulate your response:\n",
      "   - Provide a concise answer that addresses the user's query.\n",
      "   - Use information from the retrieved snippets to support your response.\n",
      "   - Explain code snippets, functions, classes, and methods when they are relevant to the query.\n",
      "   - Present function, class, and method names exactly as they appear in the retrieved snippets.\n",
      "   - Include relevant citations from the snippets to support your answer.\n",
      "\n",
      "7. Format your response:\n",
      "   - Use markdown formatting for your entire response.\n",
      "   - Enclose your final answer within <answer> tags.\n",
      "   - Use appropriate markdown syntax for headings, lists, code blocks, and emphasis.\n",
      "   - For code snippets, use triple backticks (```) with the appropriate language specifier (e.g., ```python).\n",
      "   - For inline code or function/class/method names, use single backticks (`).\n",
      "   - Include citations using square brackets with numbers, e.g., [1], [2], etc.\n",
      "\n",
      "8. Examples of good responses:\n",
      "\n",
      "<answer>\n",
      "# How to Log Metrics in W&B\n",
      "\n",
      "To log metrics in Weights & Biases (W&B), you can use the `wandb.log()` function. This function lets you track various metrics during your model's training process.\n",
      "\n",
      "Here's a basic example of how to use `wandb.log()`:\n",
      "\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "# Initialize a W&B run\n",
      "wandb.init(project=\"my-project\")\n",
      "\n",
      "# Train your model and log metrics\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_epoch()\n",
      "    accuracy = evaluate_model()\n",
      "    \n",
      "    wandb.log({\n",
      "        \"epoch\": epoch,\n",
      "        \"loss\": loss,\n",
      "        \"accuracy\": accuracy\n",
      "    })\n",
      "```\n",
      "\n",
      "In this example, we're logging three metrics: the current epoch, the loss, and the accuracy [1]. You can log any number of metrics as key-value pairs in a dictionary.\n",
      "\n",
      "Remember to call `wandb.init()` at the beginning of your script to initialize a new run [2]. This sets up the connection to the W&B servers and creates a new experiment in your project.\n",
      "\n",
      "For more advanced logging, you can also log histograms, images, and other data types. The W&B documentation provides detailed information on these features [3].\n",
      "\n",
      "References:\n",
      "\n",
      "[1] https://docs.wandb.ai/guides/track/about\n",
      "[2] https://docs.wandb.ai/guides/track/visualize\n",
      "[3] https://docs.wandb.ai/guides/track/parameters-and-sweeps/about\n",
      "</answer>\n",
      "\n",
      "9. Handling edge cases:\n",
      "   - If the retrieved snippets do not contain enough information to fully answer the query, state this clearly in your response and provide the best possible answer with the available information.\n",
      "   - If the query is unrelated to W&B, politely inform the user that you can only answer questions about Weights & Biases.\n",
      "\n",
      "Remember, your goal is to provide helpful, correct, and concise responses that fully address the user's query while maintaining trustworthiness through proper citations and accurate representation of W&B documentation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can improve the prompt with a example of the response format\n",
    "\n",
    "IMPROVED_PROMPT_V2 = open(\"prompts/improved_prompt_v2.txt\").read()\n",
    "print(IMPROVED_PROMPT_V2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V2, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=response_generator,\n",
    ")\n",
    "query_enhanced_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant specializing in Weights & Biases (W&B). Your task is to provide accurate, detailed, and helpful responses using retrieved documentation snippets. Follow these instructions:\n",
      "\n",
      "1. You will receive documentation snippets and a user query.\n",
      "2. Respond in the specified language: {language}\n",
      "3. Identified intents: {intents}\n",
      "\n",
      "### Process:\n",
      "1. **Break Down the Query:** Divide the user's query into smaller steps and explain this breakdown.\n",
      "2. **Analyze Snippets:**\n",
      "   - Read each snippet.\n",
      "   - Identify relevant information and explain its importance.\n",
      "   - For code/functions/classes/methods:\n",
      "     - Explain their purpose and functionality.\n",
      "     - Describe their relevance to the query.\n",
      "     - Provide a step-by-step breakdown if applicable.\n",
      "3. **Formulate Response:**\n",
      "   - Address each query step with detailed explanations.\n",
      "   - Use snippets to support your response.\n",
      "   - Break down code explanations into logical steps.\n",
      "   - Use exact names from snippets for functions/classes/methods.\n",
      "   - Include citations [1], [2], etc.\n",
      "4. **Format Response:**\n",
      "   - Use markdown for headings, lists, code blocks, and emphasis.\n",
      "   - Enclose the final answer in <answer> tags.\n",
      "   - Use triple backticks for code (e.g., ```python).\n",
      "   - Use inline code formatting for function/class/method names.\n",
      "5. **Structure Response:**\n",
      "   - Overview of approach.\n",
      "   - For each step:\n",
      "     - State the step.\n",
      "     - Explain your thought process.\n",
      "     - Provide relevant information.\n",
      "     - Summarize the step's contribution to the overall answer.\n",
      "   - Conclude with a summary.\n",
      "\n",
      "### Example:\n",
      "\n",
      "<answer>\n",
      "# Logging Metrics in W&B\n",
      "\n",
      "### Approach:\n",
      "1. Define metrics.\n",
      "2. Explain basic logging method.\n",
      "3. Provide a code example.\n",
      "4. Discuss advanced features.\n",
      "\n",
      "### 1. Define Metrics\n",
      "Metrics in W&B are numerical values tracked during model training/evaluation, such as loss and accuracy [1].\n",
      "\n",
      "### 2. Basic Logging Method\n",
      "Use `wandb.log()` to log metrics. It takes a dictionary of key-value pairs (metrics) and sends data to W&B servers for visualization [2].\n",
      "\n",
      "### 3. Code Example\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "wandb.init(project=\"my-project\")\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_epoch()\n",
      "    accuracy = evaluate_model()\n",
      "    \n",
      "    wandb.log({\n",
      "        \"epoch\": epoch,\n",
      "        \"loss\": loss,\n",
      "        \"accuracy\": accuracy\n",
      "    })\n",
      "```\n",
      "- Initialize W&B with `wandb.init()`.\n",
      "- Log metrics with `wandb.log()` in each epoch [3].\n",
      "\n",
      "### 4. Advanced Features\n",
      "Log histograms, images, audio, and video for richer visualizations [4].\n",
      "```python\n",
      "wandb.log({\"histogram\": wandb.Histogram(numpy_array)})\n",
      "wandb.log({\"image\": wandb.Image(numpy_array)})\n",
      "```\n",
      "\n",
      "### Conclusion\n",
      "Consistently logging metrics with `wandb.log()` helps track model performance and make data-driven decisions.\n",
      "\n",
      "References:\n",
      "[1] https://docs.wandb.ai/guides/track/about\n",
      "[2] https://docs.wandb.ai/guides/track/visualize\n",
      "[3] https://docs.wandb.ai/guides/track/parameters-and-sweeps/about\n",
      "[4] https://docs.wandb.ai/guides/track/advanced-logging\n",
      "</answer>\n",
      "\n",
      "### Handling Edge Cases:\n",
      "- If snippets lack enough information:\n",
      "  - State this limitation.\n",
      "  - Provide the best partial answer.\n",
      "  - Suggest sources or methods to find missing info.\n",
      "- If the query is unrelated to W&B:\n",
      "  - Inform the user and explain why.\n",
      "  - Suggest how to rephrase the question to relate to W&B.\n",
      "\n",
      "Your goal is to provide helpful, correct, and detailed responses, maintaining trustworthiness through proper citations and accurate representation of W&B documentation. Always show your reasoning process.\n"
     ]
    }
   ],
   "source": [
    "# we can further improve the prompt to have chain-of-thought reasoning\n",
    "\n",
    "\n",
    "IMPROVED_PROMPT_V3 = open(\"prompts/improved_prompt_v3.txt\").read()\n",
    "\n",
    "print(IMPROVED_PROMPT_V3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=IMPROVED_PROMPT_V3, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=response_generator,\n",
    ")\n",
    "\n",
    "query_enhanced_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a better model to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r-plus\", prompt=IMPROVED_PROMPT_V3, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=hybrid_retriever,\n",
    "    response_generator=response_generator,\n",
    ")\n",
    "\n",
    "query_enhanced_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all the evals and see which one is the best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
