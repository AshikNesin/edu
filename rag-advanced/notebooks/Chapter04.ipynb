{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/e98981ef9d934b10a0c6b4855d8eb6bfc7f56f1a/rag-advanced/notebooks/Chapter04.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-advance-chapter-04} -->\n",
    "\n",
    "## Query Enhancement\n",
    "\n",
    "Improving the quality of data helps with improving the quality of generated response. Another way is to improve the quality of the query seen by the LLM.\n",
    "\n",
    "We cannot ask the user to provide the query in the best way possible. Many a times the user is not very sure of the query to be asked. Query enhancement as the name suggests, is an intermediate step that uses LLM to enhance the quality of the query. The enhancement can be - \n",
    "- making the query gramatically correct\n",
    "- breaking down a complex query into relevant sub-queries\n",
    "- extract the intent of the query (this can be passed for formatted answer in case of nefarious queries)\n",
    "- if you have a chat history, augment the query with past queries and generated answers/retrieved contexts.\n",
    "- extract keywords (can be about your product or anything related to your application) and pass it along with the query to your LLM\n",
    "\n",
    "One can imagine many different ways to enhance the quality of the query or extract meaningful stuff from a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, execute the following cell to clone the repository and install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/wandb/edu.git\n",
    "%cd edu\n",
    "!git checkout rag-irl\n",
    "\n",
    "%cd rag-advanced\n",
    "!pip install -qqq -U uv\n",
    "!uv pip install --system -r requirements.txt\n",
    "%cd notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the setup complete, we can now proceed with the chapter content.\n",
    "\n",
    "Initial steps:\n",
    "1. Log in to Weights & Biases (W&B)\n",
    "2. Configure environment variables for API access\n",
    "\n",
    "To obtain your Cohere API key, visit the [Cohere API dashboard](https://dashboard.cohere.com/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Please enter your COHERE_API_KEY\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio\n",
    "import cohere\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the chunked data from chapter 3. This chunking was done using semantic chunking strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 3\n",
    "chunked_data = weave.ref(\"chunked_data:v0\").get()\n",
    "\n",
    "chunked_data.rows[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our usecase we will use this query enhancement stage to -\n",
    "- identify the language of the query (our documentation in in English, Japanese and Korean and we want to answer in the language of the query)\n",
    "- indentify the intent of the query (a user might ask something that is not related to our documentation)\n",
    "- generate sub-queries (break down the main query into smaller queries) for retrieving more contexts for our LLM.\n",
    "\n",
    "These additional informations will be used to inform the response generator and improve the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.query_enhancer import QueryEnhancer\n",
    "from scripts.utils import display_source\n",
    "\n",
    "query_enhancer = QueryEnhancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "response = await query_enhancer.predict(\"How do I log images in lightning with wandb?\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the response below:\n",
    "\n",
    "- we identified the query to be in English.\n",
    "- We derived few sub-queries that make sense.\n",
    "- We classified the intent based on our intent classification \"prompt/guides\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "response"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our retriever will remain the same. Yes we have 5 sub-queries that we want to retrieve for but we can do so one by one. \n",
    "\n",
    "Let us use our BM25 based retriever from Chapter 2 and index our chunked data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retriever import BM25Retriever\n",
    "\n",
    "retriever = BM25Retriever()\n",
    "retriever.index_data(chunked_data.rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have more information extracted from our query - like the language and the intent of the query, we write `QueryEnhanedResponseGenerator` whihc uses a new system prompt augmented with language and intent information.\n",
    "\n",
    "Look at line 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_generator import QueryEnhanedResponseGenerator\n",
    "\n",
    "display_source(QueryEnhanedResponseGenerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `QueryEnhancedRAGPipeline` runs through different `search_queries` or sub-queries and retrieve the chunks. It also deduplicate the chunks so that we don't end up sending the same chunk twice.\n",
    "\n",
    "Note line 23-27. We check if the extracted intent is not in a list of intents to avoid. If that's the case, we do not do retrieval and can return a formatted answer like - \"This query is not related to Weights and Biases. Can you please ask again?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rag_pipeline import QueryEnhancedRAGPipeline\n",
    "\n",
    "display_source(QueryEnhancedRAGPipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Let us initialize the response generator and our RAG pipeline and run in on one query."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add the new prompt\n",
    "QUERY_ENHANCED_PROMPT = open(\"prompts/query_enhanced_system.txt\").read()\n",
    "\n",
    "response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r\", prompt=QUERY_ENHANCED_PROMPT, client=cohere.AsyncClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_enhanced_rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=retriever,\n",
    "    response_generator=response_generator,\n",
    "    top_k=5,\n",
    ")\n",
    "\n",
    "response = await query_enhanced_rag_pipeline.predict(\n",
    "    \"How do I log images in lightning with wandb?\"\n",
    ")\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:Qj4IFICc2EbdXu5A5UuhkPiWgxM1GvJMIvXEyv1DYnM\"\n",
    ").get()\n",
    "\n",
    "print(eval_dataset.rows[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let also initialize the baseline RAG pipeline from chapter 3\n",
    "\n",
    "from scripts.rag_pipeline import SimpleRAGPipeline\n",
    "from scripts.response_generator import SimpleResponseGenerator\n",
    "\n",
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\", \"r\").read()\n",
    "response_generator = SimpleResponseGenerator(model=\"command-r\", prompt=INITIAL_PROMPT)\n",
    "simple_rag_pipeline = SimpleRAGPipeline(\n",
    "    retriever=retriever, response_generator=response_generator, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are primarly interested in evaluating the response quality since we are using the same retriver in both pipelines\n",
    "# We will use LLM metrics to evaluate the response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import LLM_METRICS\n",
    "\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=LLM_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "\n",
    "baseline_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(simple_rag_pipeline)\n",
    ")\n",
    "\n",
    "query_enhanced_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(query_enhanced_rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare_retriever_responses](../images/04_compare_query_enhanced_responses.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
