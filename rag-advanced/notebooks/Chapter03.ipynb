{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 \n",
    "## Data Ingestion and Preprocessing\n",
    "\n",
    "Behind RAG's fanciness and jargon it is simply a way to connect your private data to a pretrained (instruct tuned) LLM. The best way to improve the quality of your RAG is to improve the quality of your data ingestion pipleine.\n",
    "\n",
    "Data ingestion on a whole constitue data sources and preprocessing. Like most ML systems LLMs also follow gargabe-in-garbage-out concept. The quality of your data ingestion pipeline directly correlate with your RAG's efficacy. \n",
    "\n",
    "An important aspect of efficient data ingestion is its ability to periodically update when the data sources update. We ideally want as little friction as possible.\n",
    "\n",
    "Tip: When building a POC, don't think much about the chunk size, parsing strategies, format (markdown or HTMl or plain text), etc. Just build something that works end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from scripts.utils import display_source\n",
    "import weave\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 3\",\n",
    ")\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will start our journey from the raw data. Below we are downloading the latest `raw_data` artifact. W&B Artifact is a great way to store, version control and integration your data sources with downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-use the raw dataset from the artifact in our previous step\n",
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In chapter 1, we naively counted each word (as they appear in English text) as one token (`raw_tokens`). Below we are updating to a correct token counting strategy (`tokens`).\n",
    "\n",
    "We will be using [Cohere's tokenizer](https://docs.cohere.com/docs/tokens-and-tokenizers) to calculate the number of tokens per document in our `raw_data`. The correct token count along with word count is stored as metadata of that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier we referred to words as tokens. We can be more correct in defining tokens by using a tokenizer.\n",
    "# We'll use the Cohere tokenizer for this example.\n",
    "\n",
    "from scripts.utils import (\n",
    "    length_function,\n",
    "    tokenize_text,\n",
    "    get_special_tokens_set,\n",
    "    TOKENIZERS,\n",
    ")\n",
    "\n",
    "display_source(tokenize_text)\n",
    "display_source(length_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in raw_data[:]:\n",
    "    doc[\"metadata\"][\"words\"] = doc[\"metadata\"].pop(\"raw_tokens\")\n",
    "    doc[\"metadata\"][\"tokens\"] = length_function(doc[\"content\"])\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the `words` as token (used in Chapter 1) is quite off from the actual `tokens` count. Knowing the correct token helps decide:\n",
    "- if we wanna actually build a RAG pipeline of ingest the whole document to an LLM (long context window is now supported by many top LLMs)\n",
    "- what chunk size makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "There is a lot of extra formatting information (markdown elements) that is not very useful to an LLM.\n",
    "\n",
    "We can remove this information by converting the contents to text. We can also remove any special characters and extra whitespace. \n",
    "\n",
    "Special characters here are ones that are defined in the tokenizer and will vary depending on the model used.\n",
    "\n",
    "Below we are using two functions:\n",
    "\n",
    "- `convert_contents_to_text`: This takes the raw markdown string and convert it to HTML. Using `BeautifulSoup` we remove the image links, images, and other formatting information.\n",
    "- `make_text_tokenization_safe`: This takes the text string and remove any special token present in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess import convert_contents_to_text, make_text_tokenization_safe\n",
    "\n",
    "display_source(convert_contents_to_text)\n",
    "display_source(make_text_tokenization_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are converting the raw markdown documents to text and making it tokenization safe. Check the first 5 special tokens.\n",
    "\n",
    "The `parsed_tokens` is smaller compared to `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_set = get_special_tokens_set(TOKENIZERS[\"command-r\"])\n",
    "print(list(special_tokens_set)[:5])\n",
    "\n",
    "parsed_data = []\n",
    "\n",
    "for doc in raw_data:\n",
    "    parsed_doc = doc.copy()\n",
    "    content = convert_contents_to_text(doc[\"content\"])\n",
    "    parsed_doc[\"parsed_content\"] = make_text_tokenization_safe(\n",
    "        content, special_tokens_set=special_tokens_set\n",
    "    )\n",
    "    parsed_doc[\"metadata\"][\"parsed_tokens\"] = length_function(\n",
    "        parsed_doc[\"parsed_content\"]\n",
    "    )\n",
    "    parsed_data.append(parsed_doc)\n",
    "parsed_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will log the preprocessed data as W&B Artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(map(lambda x: x[\"metadata\"][\"words\"], parsed_data))\n",
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"tokens\"], raw_data))\n",
    "total_parsed_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], parsed_data))\n",
    "\n",
    "preprocessed_artifact = wandb.Artifact(\n",
    "    name=\"preprocessed_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Preprocessed wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(parsed_data),\n",
    "        \"date_preprocessed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_words\": total_words,\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_parsed_tokens\": total_parsed_tokens,\n",
    "    },\n",
    ")\n",
    "with preprocessed_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in parsed_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(preprocessed_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Chunking\n",
    "\n",
    "We can split the processed data into smaller chunks. We do this to:\n",
    "- only send the data that is required for generation reducing the input token cost\n",
    "- the limited context allows the LLM to not miss on details we want the generation to have\n",
    "\n",
    "Obviously we can choose to send the entire document to the LLM but it is dependent on the total token count of your document and also the nature of your use case. Obviously this will be costlier but a good place to start.\n",
    "\n",
    "### Semantic Chunking\n",
    "\n",
    "One can do this chunking using different strategies - split after n words/tokens, split on headers, etc. Always try out these simple chunking strategies before moving to more sophisticated strategies.\n",
    "\n",
    "Below we are implementing semantic chunking (a sophisticated strategy) which we have seen work in practice. In this strategy, we group similiar sentences into chunks. \n",
    "\n",
    "1. First we split the text into sentences using [BlingFire](https://github.com/microsoft/BlingFire) library.\n",
    "2. Then we group and combine chunks using semantic similarity and create chunks.\n",
    "\n",
    "Read more here: https://research.trychroma.com/evaluating-chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/preprocessed_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = preprocessed_artifact.download()\n",
    "preprocessed_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "preprocessed_data = list(\n",
    "    map(json.loads, preprocessed_data_file.read_text().splitlines())\n",
    ")\n",
    "preprocessed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.chunking import chunk_documents\n",
    "\n",
    "display_source(chunk_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = chunk_documents(preprocessed_data)\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_chunk_size = np.mean([doc[\"metadata\"][\"parsed_tokens\"] for doc in chunked_data])\n",
    "std_chunk_size = np.std([doc[\"metadata\"][\"parsed_tokens\"] for doc in chunked_data])\n",
    "print(f\"Mean chunk size: {mean_chunk_size}, Std chunk size: {std_chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we'll store the cleaned data in an artifact for future use and reproducibility\n",
    "\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], chunked_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(chunked_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": {\"mean\": mean_chunk_size, \"std\": std_chunk_size},\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in chunked_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also try a different retriever and see how it performs in comparison to the Tf-Idf retriever we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rag_pipeline import SimpleRAGPipeline\n",
    "from scripts.response_generator import SimpleResponseGenerator\n",
    "from scripts.retriever import BM25Retriever, TFIDFRetriever\n",
    "\n",
    "display_source(BM25Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever()\n",
    "bm25_retriever.index_data(chunked_data)\n",
    "\n",
    "tfidf_retriever = TFIDFRetriever()\n",
    "tfidf_retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the rag pipeline remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\", \"r\").read()\n",
    "response_generator = SimpleResponseGenerator(model=\"command-r\", prompt=INITIAL_PROMPT)\n",
    "bm25_rag_pipeline = SimpleRAGPipeline(\n",
    "    retriever=bm25_retriever, response_generator=response_generator, top_k=5\n",
    ")\n",
    "tfidf_rag_pipeline = SimpleRAGPipeline(\n",
    "    retriever=tfidf_retriever, response_generator=response_generator, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and compare the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retrieval_metrics import ALL_METRICS as RETRIEVAL_METRICS\n",
    "from scripts.response_metrics import ALL_METRICS as RESPONSE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_samples[:10],\n",
    "    scorers=RETRIEVAL_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\": 5},\n",
    ")\n",
    "bm25_retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(bm25_retriever))\n",
    "tfidf_retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(tfidf_retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=RESPONSE_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "bm25_response_scores = asyncio.run(response_evaluations.evaluate(bm25_rag_pipeline))\n",
    "tfidf_response_scores = asyncio.run(response_evaluations.evaluate(tfidf_rag_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Add more data sources to the RAG system. - Add Jupyter Notbooks from the See wandb/examples repo.\n",
    "2. Use a different chunking method. - Try your own parsing and chunking method.\n",
    "3. Use a small-to-big retrieval method. Where we embed small documents but retrieve big documents -> You can add the parent document to the metadata and modify the `Retriever.search` method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
