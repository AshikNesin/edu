{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 \n",
    "## Data Ingestion and Preprocessing\n",
    "\n",
    "One way to improve our RAG system is to improve our data ingestion and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import wandb\n",
    "import cohere\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-use the raw dataset from the artifact in our previous step\n",
    "\n",
    "\n",
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier we referred to words as tokens. We can be more correct in defining tokens by using a tokenizer.\n",
    "# We'll use the Cohere tokenizer for this example.\n",
    "\n",
    "co = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    return co.tokenize(text=text, model=\"command-r\", offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    \"command-r\": \"https://storage.googleapis.com/cohere-public/tokenizers/command-r.json\",\n",
    "    \"command-r-plus\": \"https://storage.googleapis.com/cohere-public/tokenizers/command-r-plus.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in raw_data[:]:\n",
    "    doc['metadata']['words'] = doc['metadata'].pop('raw_tokens')\n",
    "    doc['metadata']['tokens'] = len(tokenize_text(doc['content']).tokens)\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "There is a lot of extra formatting information (markdown elements) that is not very useful to an LLM.\n",
    "\n",
    "We can remove this information by converting the contents to text. We can also remove any special characters and extra whitespace. \n",
    "\n",
    "Special characters here are ones that are defined in the tokenizer and will vary depending on the model used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/preprocess\n",
    "import frontmatter\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def convert_contents_to_text(contents: str) -> str:\n",
    "    _, content = frontmatter.parse(contents)\n",
    "    # use some extensions to convert the markdown to html\n",
    "    markdown_document = markdown.markdown(\n",
    "        content,\n",
    "        extensions=[\n",
    "            \"toc\",\n",
    "            \"pymdownx.extra\",\n",
    "            \"pymdownx.blocks.admonition\",\n",
    "            \"pymdownx.magiclink\",\n",
    "            \"pymdownx.blocks.tab\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.saneheaders\",\n",
    "            \"pymdownx.striphtml\",\n",
    "            \"pymdownx.highlight\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.escapeall\"\n",
    "        ],\n",
    "    )\n",
    "    soup = BeautifulSoup(markdown_document, \"html.parser\")\n",
    "    def remove_urls_a_tags_hrefs(soup):\n",
    "        # For hyperlinks, keep the text but remove the link\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.replace_with(a_tag.text)\n",
    "        \n",
    "        # Remove all images\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.decompose()\n",
    "        \n",
    "        return soup\n",
    "\n",
    "    # Use the function as before\n",
    "    soup = remove_urls_a_tags_hrefs(soup)\n",
    "\n",
    "    def remove_javascript_import_statements(soup):\n",
    "        for p in soup.find_all('p'):\n",
    "            if p.text.strip().startswith('import') and ';' in p.text:\n",
    "                p.decompose()\n",
    "        return soup\n",
    "    soup = remove_javascript_import_statements(soup)\n",
    "\n",
    "    return soup.get_text()\n",
    "\n",
    "def get_special_tokens_set(tokenizer_url):\n",
    "    # https://docs.cohere.com/docs/tokens-and-tokenizers\n",
    "    response = requests.get(tokenizer_url)\n",
    "    return set([tok[\"content\"] for tok in response.json()[\"added_tokens\"]])\n",
    "\n",
    "def make_text_tokenization_safe(content: str, special_tokens_set: set) -> str:\n",
    "    def remove_special_tokens(text: str) -> str:\n",
    "        \"\"\"Removes special tokens from the given text.\n",
    "\n",
    "        Args:\n",
    "            text: A string representing the text.\n",
    "\n",
    "        Returns:\n",
    "            The text with special tokens removed.\n",
    "        \"\"\"\n",
    "        for token in special_tokens_set:\n",
    "            text = text.replace(token, \"\")\n",
    "        return text\n",
    "\n",
    "    cleaned_content = remove_special_tokens(content)\n",
    "    return cleaned_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_set = get_special_tokens_set(tokenizers[\"command-r\"])\n",
    "parsed_data = []\n",
    "\n",
    "for doc in raw_data:\n",
    "    parsed_doc = doc.copy()\n",
    "    content = convert_contents_to_text(doc[\"content\"])\n",
    "    parsed_doc[\"parsed_content\"] = make_text_tokenization_safe(content, special_tokens_set=special_tokens_set)\n",
    "    parsed_doc[\"metadata\"][\"parsed_tokens\"] = len(tokenize_text(parsed_doc[\"parsed_content\"]).tokens)\n",
    "    parsed_data.append(parsed_doc)\n",
    "parsed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(map(lambda x: x[\"metadata\"][\"words\"], parsed_data))\n",
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"tokens\"], raw_data))\n",
    "total_parsed_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], parsed_data))\n",
    "\n",
    "preprocessed_artifact = wandb.Artifact(name=\"preprocessed_data\", type=\"dataset\",\n",
    "description=\"Preprocessed wandb documentation\", metadata={\n",
    "    \"total_files\": len(parsed_data),\n",
    "    \"date_preprocessed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"total_words\": total_words,\n",
    "    \"total_raw_tokens\": total_raw_tokens,\n",
    "    \"total_parsed_tokens\": total_parsed_tokens,\n",
    "    }\n",
    ")\n",
    "with preprocessed_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in parsed_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(preprocessed_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Chunking\n",
    "\n",
    "1. First we split the text into sentences using [BlingFire](https://github.com/microsoft/BlingFire) library.\n",
    "2. Then we split the sentences into chunks of a maximum number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_artifact = run.use_artifact(f'{WANDB_ENTITY}/{WANDB_PROJECT}/preprocessed_data:latest', type='dataset')\n",
    "artifact_dir = preprocessed_artifact.download()\n",
    "preprocessed_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "preprocessed_data = list(map(json.loads, preprocessed_data_file.read_text().splitlines()))\n",
    "preprocessed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/chunking\n",
    "from blingfire import text_to_sentences\n",
    "from typing import List, Callable\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "\n",
    "\n",
    "def split_into_chunks(\n",
    "    text: str, tokenize_text: Callable[[str], List[str]], max_tokens: int = CHUNK_SIZE\n",
    ") -> List[str]:\n",
    "    # Split the text into sentences\n",
    "    sentences = text_to_sentences(text).split(\"\\n\")\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenize_text(\"\\n\" + sentence).tokens) for sentence in sentences]\n",
    "\n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\"\\n\".join(chunk))\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    # Add any remaining chunk\n",
    "    if chunk:\n",
    "        chunks.append(\"\\n\".join(chunk))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "for doc in preprocessed_data:\n",
    "    chunks = split_into_chunks(doc[\"parsed_content\"], tokenize_text=tokenize_text)\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            {\n",
    "                \"cleaned_content\" : chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"parsed_tokens\": len(tokenize_text(chunk).tokens)\n",
    "            }})\n",
    "        \n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we'll store the cleaned data in an artifact for future use and reproducibility\n",
    "\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], chunked_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(chunked_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in chunked_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/retriever\n",
    "import weave\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class Retriever(weave.Model):\n",
    "    vectorizer: TfidfVectorizer = TfidfVectorizer()\n",
    "    index: list = None\n",
    "    data: list = None\n",
    "\n",
    "    @weave.op()\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    @weave.op()\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, k: int):\n",
    "        return self.search(query, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.vectorizer = TfidfVectorizer()\n",
    "retriever.index_data(chunked_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/response_generator\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import cohere\n",
    "import weave\n",
    "\n",
    "class ResponseGenerator(weave.Model):\n",
    "    model: str\n",
    "    prompt: str\n",
    "    client: cohere.Client = None\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_context(self, context: List[Dict[str, any]]) -> str:\n",
    "        return [{\"source\": item['source'], \"text\": item['text']} for item in context]\n",
    "    \n",
    "    @weave.op()\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        contexts = self.generate_context(context)\n",
    "        response = self.client.chat(\n",
    "            preamble=self.prompt,\n",
    "            message=query,\n",
    "            model=self.model,\n",
    "            documents=contexts,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, context: List[Dict[str, any]]):\n",
    "        return self.generate_response(query, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prompts/initial_system\n",
    "INITIAL_PROMPT = \"\"\"\n",
    "Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=INITIAL_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/rag_pipeline\n",
    "import weave\n",
    "from typing import Optional, Union\n",
    "from scripts.retriever import Retriever\n",
    "from scripts.response_generator import ResponseGenerator\n",
    "\n",
    "\n",
    "class RAGPipeline(weave.Model):\n",
    "    retriever: Union[weave.Model, Retriever] = None\n",
    "    response_generator: Union[weave.Model, ResponseGenerator] = None\n",
    "    top_k: int = 5\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str):\n",
    "        context = self.retriever.predict(query, self.top_k)\n",
    "        return self.response_generator.predict(query, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RAGPipeline(retriever=retriever, response_generator=response_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/final_eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/retrieval_metrics.py\n",
    "import weave\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_hit_rate(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate (precision) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The hit rate (precision).\n",
    "\n",
    "    The hit rate (precision) measures the proportion of retrieved documents that are relevant.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{Hit Rate (Precision)} = \\frac{\\text{Number of Relevant Documents Retrieved}}{\\text{Total Number of Documents Retrieved}} \\]\n",
    "\n",
    "    This metric is useful for assessing the accuracy of the retrieval system by determining the relevance of the retrieved documents.\n",
    "    ```\n",
    "    \"\"\"\n",
    "    search_results = [doc[\"source\"] for doc in model_output]\n",
    "    relevant_sources = [\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    ]\n",
    "\n",
    "    # Calculate the number of relevant documents retrieved\n",
    "    relevant_retrieved = sum(\n",
    "        1 for source in search_results if source in relevant_sources\n",
    "    )\n",
    "\n",
    "    # Calculate the hit rate (precision)\n",
    "    hit_rate = relevant_retrieved / len(search_results) if search_results else 0.0\n",
    "\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def compute_mrr(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:<- this is not working\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score for the given query.\n",
    "\n",
    "    MRR measures the rank of the first relevant document in the result list.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{MRR} = \\frac{1}{\\text{rank of first relevant document}} \\]\n",
    "\n",
    "    If no relevant document is found, MRR is 0.\n",
    "\n",
    "    This metric is useful for evaluating systems where there is typically one relevant document\n",
    "    and the user is interested in finding that document quickly.\n",
    "    \"\"\"\n",
    "    relevant_sources = [\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    ]\n",
    "\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(model_output, 1):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score\n",
    "\n",
    "\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "@weave.op\n",
    "def compute_ndcg(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Discounted Cumulative Gain (NDCG) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The cosine similarity score of the document to the query.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The NDCG score for the given query.\n",
    "\n",
    "    NDCG measures the ranking quality of the search results, taking into account the position of relevant documents.\n",
    "\n",
    "    NDCG Formula:\n",
    "    1. Calculate the Discounted Cumulative Gain (DCG):\n",
    "       \\[ \\text{DCG}_p = \\sum_{i=1}^p \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} \\]\n",
    "       where \\( rel_i \\) is the relevance score of the document at position \\( i \\).\n",
    "\n",
    "    2. Calculate the Ideal Discounted Cumulative Gain (IDCG), which is the DCG of the ideal ranking:\n",
    "       \\[ \\text{IDCG}_p = \\sum_{i=1}^p \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} \\]\n",
    "       where documents are sorted by their relevance scores in descending order.\n",
    "\n",
    "    3. Normalize the DCG by dividing it by the IDCG to get NDCG:\n",
    "       \\[ \\text{NDCG}_p = \\frac{\\text{DCG}_p}{\\text{IDCG}_p} \\]\n",
    "\n",
    "    This implementation uses continuous relevance scores.\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "\n",
    "    # Calculate DCG\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            dcg += (2 ** result[\"score\"] - 1) / np.log2(\n",
    "                i + 2\n",
    "            )  # i+2 because log2 starts at 1 for i=0\n",
    "\n",
    "    # Sort the results by score to calculate IDCG\n",
    "    sorted_model_output = sorted(model_output, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # Calculate IDCG\n",
    "    for i, result in enumerate(sorted_model_output):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            idcg += (2 ** result[\"score\"] - 1) / np.log2(i + 2)\n",
    "\n",
    "    # To avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate nDCG\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "# MAP (Mean Average Precision)\n",
    "@weave.op()\n",
    "def compute_map(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision (MAP) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The MAP score for the given query.\n",
    "\n",
    "    MAP provides a single-figure measure of quality across recall levels.\n",
    "    For a single query, it's equivalent to the Average Precision (AP).\n",
    "    It's calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{MAP} = \\frac{\\sum_{k=1}^n P(k) \\times \\text{rel}(k)}{\\text{number of relevant documents}} \\]\n",
    "\n",
    "    Where:\n",
    "    - n is the number of retrieved documents\n",
    "    - P(k) is the precision at cut-off k in the list\n",
    "    - rel(k) is an indicator function: 1 if the item at rank k is relevant, 0 otherwise\n",
    "    MAP considers both precision and recall, as well as the ranking of relevant documents.\n",
    "\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "\n",
    "    num_relevant = 0\n",
    "    sum_precision = 0.0\n",
    "\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            num_relevant += 1\n",
    "            sum_precision += num_relevant / (i + 1)\n",
    "\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_precision = sum_precision / len(relevant_sources)\n",
    "    return average_precision\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_precision(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Precision for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The Precision score for the given query.\n",
    "\n",
    "    Precision measures the proportion of retrieved documents that are relevant.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{Precision} = \\frac{\\text{Number of Relevant Documents Retrieved}}{\\text{Total Number of Documents Retrieved}} \\]\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "    retrieved_sources = {result[\"source\"] for result in model_output}\n",
    "\n",
    "    relevant_retrieved = relevant_sources & retrieved_sources\n",
    "\n",
    "    precision = (\n",
    "        len(relevant_retrieved) / len(retrieved_sources) if retrieved_sources else 0.0\n",
    "    )\n",
    "    return precision\n",
    "\n",
    "\n",
    "# Recall\n",
    "@weave.op()\n",
    "def compute_recall(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Recall for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The Recall score for the given query.\n",
    "\n",
    "    Recall measures the proportion of relevant documents that are retrieved.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{Recall} = \\frac{\\text{Number of Relevant Documents Retrieved}}{\\text{Total Number of Relevant Documents}} \\]\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "    retrieved_sources = {result[\"source\"] for result in model_output}\n",
    "\n",
    "    relevant_retrieved = relevant_sources & retrieved_sources\n",
    "\n",
    "    recall = (\n",
    "        len(relevant_retrieved) / len(relevant_sources) if relevant_sources else 0.0\n",
    "    )\n",
    "    return recall\n",
    "\n",
    "\n",
    "# F1 Score\n",
    "@weave.op()\n",
    "def compute_f1_score(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the F1-Score for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1-Score for the given query.\n",
    "\n",
    "    F1-Score is the harmonic mean of Precision and Recall.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "    \"\"\"\n",
    "    precision = compute_precision(model_output, contexts)\n",
    "    recall = compute_recall(model_output, contexts)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "retrieval_scorers = [\n",
    "    compute_mrr, compute_ndcg, compute_map,\n",
    "    compute_hit_rate, compute_precision,\n",
    "    compute_recall, compute_f1_score\n",
    "]\n",
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset.to_dict(orient=\"records\"),\n",
    "    scorers=retrieval_scorers,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 2 with the following meanings:\n",
    "    0 = represents that the excerpt is irrelevant to the query,\n",
    "    1 = represents that the excerpt is somewhat relevant to the query,\n",
    "    2 = represents that the excerpt is is highly relevant to the query.\n",
    "    \n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the query but not completely, category 2 if the excerpt only and entirely refers to the query. If neither of these criteria satisfies the query, give it category 0.\n",
    "\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\n",
    "## Examples\n",
    "\n",
    "Example 1: \n",
    "<Query>\n",
    "How do I programmatically access the human-readable run name?\n",
    "</Query>\n",
    "<Document>\n",
    "If you do not explicitly name your run, a random run name will be assigned to the run to help identify the run in the UI. For instance, random run names will look like \"pleasant-flower-4\" or \"misunderstood-glade-2\".\n",
    "\n",
    "If you'd like to overwrite the run name (like snowy-owl-10) with the run ID (like qvlp96vk) you can use this snippet:\n",
    "\n",
    "import wandbRetrieval_Evaluation\n",
    "\n",
    "wandb.init()\n",
    "wandb.run.name = wandb.run.id\n",
    "wandb.run.save()\n",
    "\n",
    "</Document>\n",
    "{{\"final_score\": 0}}\n",
    "\n",
    "Example 2:\n",
    "<Query>\n",
    "What are Runs?\n",
    "</Query>\n",
    "<Document>\n",
    "A single unit of computation logged by W&B is called a run. You can think of a W&B run as an atomic element of your whole project. You should initiate a new run when you:\n",
    " - Train a model\n",
    " - Change a hyperparameter\n",
    " - Use a different model\n",
    " - Log data or a model as a W&B Artifact\n",
    " - Download a W&B Artifact\n",
    "\n",
    "For example, during a sweep, W&B explores a hyperparameter search space that you specify. Each new hyperparameter combination created by the sweep is implemented and recorded as a unique run. \n",
    "</Document>\n",
    "{{\"final_score\": 2}}\n",
    "\n",
    "Example 3:\n",
    "<Query>\n",
    "How do I use W&B with Keras ?\n",
    "</Query>\n",
    "<Document>\n",
    "We have added three new callbacks for Keras and TensorFlow users, available from wandb v0.13.4. For the legacy WandbCallback scroll down.\n",
    "These new callbacks,\n",
    " - Adhere to Keras design philosophy\n",
    " - Reduce the cognitive load of using a single callback (WandbCallback) for everything\n",
    " - Make it easy for Keras users to modify the callback by subclassing it to support their niche use case\n",
    "</Document>\n",
    "{{\"final_score\": 1}}\n",
    "\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "\n",
    "<Document>\n",
    "{document}\n",
    "</Document>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> str:\n",
    "    response = await client.chat(\n",
    "        message=RETRIEVAL_EVAL_PROMPT.format(query=query, document=passage),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "@weave.op()\n",
    "\n",
    "async def run_retriever_evaluation_using_llm(eval_samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 2 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 2:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/response_eval\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate import meteor\n",
    "from nltk.corpus import wordnet as wn\n",
    "import weave\n",
    "import re\n",
    "import string\n",
    "\n",
    "wn.ensure_loaded()\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize the input text by lowercasing, removing punctuation, and extra whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on punctuation before removing it, ensuring numbers are not split\n",
    "    text = re.sub(r\"[^\\w\\s\\d]\", \" \", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_diff(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the similarity ratio between the normalized model output and the expected answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The output generated by the model.\n",
    "        answer (str): The expected answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity ratio between the normalized model output and the expected answer.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    return difflib.SequenceMatcher(None, norm_output, norm_answer).ratio()\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_levenshtein(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein ratio between the normalized model output and the answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The output generated by the model.\n",
    "        answer (str): The expected answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The Levenshtein ratio between the normalized model output and the answer.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    return Levenshtein.ratio(norm_output, norm_answer)\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_rouge(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the ROUGE-L F1 score between the normalized model output and the reference answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model's generated output.\n",
    "        answer (str): The reference answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The ROUGE-L F1 score.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(norm_output, norm_answer)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_bleu(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the BLEU score between the normalized model output and the reference answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The generated output from the model.\n",
    "        answer (str): The reference answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The BLEU score between the normalized model output and the reference answer.\n",
    "    \"\"\"\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    reference = word_tokenize(norm_answer)\n",
    "    candidate = word_tokenize(norm_output)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def compute_meteor(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the METEOR score between the normalized model output and the reference answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model's generated output.\n",
    "        answer (str): The reference answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The METEOR score rounded to 4 decimal places.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    reference = word_tokenize(norm_answer)\n",
    "    candidate = word_tokenize(norm_output)\n",
    "    meteor_score = round(meteor([candidate], reference), 4)\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [\n",
    "    compute_diff,\n",
    "    compute_levenshtein,\n",
    "    compute_rouge,\n",
    "    compute_bleu,\n",
    "    ]\n",
    "\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(response_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 2 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 3:\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it significantly misses the user's intent,\n",
    "    \"final_score\": 1,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 4:\n",
    "{{\n",
    "    \"reason\": \"The generated is not factually correct and includes assumptions about code methods completely different from the reference answer\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_correctness_using_llm_judge(question: str, answer: str, model_output: str) -> Dict[str, Any]:\n",
    "    response = await client.chat(\n",
    "        message=CORRECTNESS_EVAL_PROMPT.format(query=question, reference_answer=answer, generated_answer=model_output),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [evaluate_correctness_using_llm_judge]\n",
    "correctness_evaluations = weave.Evaluation(\n",
    "    name=\"Correctness_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(correctness_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
