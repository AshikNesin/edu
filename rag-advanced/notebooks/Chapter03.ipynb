{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 \n",
    "## Data Ingestion and Preprocessing\n",
    "\n",
    "One way to improve our RAG system is to improve our data ingestion and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from scripts.utils import display_source\n",
    "import weave\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 3\",\n",
    ")\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-use the raw dataset from the artifact in our previous step\n",
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier we referred to words as tokens. We can be more correct in defining tokens by using a tokenizer.\n",
    "# We'll use the Cohere tokenizer for this example.\n",
    "\n",
    "from scripts.utils import (\n",
    "    length_function,\n",
    "    tokenize_text,\n",
    "    get_special_tokens_set,\n",
    "    TOKENIZERS,\n",
    ")\n",
    "\n",
    "display_source(tokenize_text)\n",
    "display_source(length_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in raw_data[:]:\n",
    "    doc[\"metadata\"][\"words\"] = doc[\"metadata\"].pop(\"raw_tokens\")\n",
    "    doc[\"metadata\"][\"tokens\"] = length_function(doc[\"content\"])\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "There is a lot of extra formatting information (markdown elements) that is not very useful to an LLM.\n",
    "\n",
    "We can remove this information by converting the contents to text. We can also remove any special characters and extra whitespace. \n",
    "\n",
    "Special characters here are ones that are defined in the tokenizer and will vary depending on the model used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocess import convert_contents_to_text, make_text_tokenization_safe\n",
    "\n",
    "display_source(convert_contents_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_set = get_special_tokens_set(TOKENIZERS[\"command-r\"])\n",
    "parsed_data = []\n",
    "\n",
    "for doc in raw_data:\n",
    "    parsed_doc = doc.copy()\n",
    "    content = convert_contents_to_text(doc[\"content\"])\n",
    "    parsed_doc[\"parsed_content\"] = make_text_tokenization_safe(\n",
    "        content, special_tokens_set=special_tokens_set\n",
    "    )\n",
    "    parsed_doc[\"metadata\"][\"parsed_tokens\"] = length_function(\n",
    "        parsed_doc[\"parsed_content\"]\n",
    "    )\n",
    "    parsed_data.append(parsed_doc)\n",
    "parsed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(map(lambda x: x[\"metadata\"][\"words\"], parsed_data))\n",
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"tokens\"], raw_data))\n",
    "total_parsed_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], parsed_data))\n",
    "\n",
    "preprocessed_artifact = wandb.Artifact(\n",
    "    name=\"preprocessed_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Preprocessed wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(parsed_data),\n",
    "        \"date_preprocessed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_words\": total_words,\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_parsed_tokens\": total_parsed_tokens,\n",
    "    },\n",
    ")\n",
    "with preprocessed_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in parsed_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(preprocessed_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Chunking\n",
    "\n",
    "### Semantic Chunking\n",
    "\n",
    "We can improve our data ingestion by grouping similar sentences together into chunks. This is called Semantic Chunking.\n",
    "This will help the model to understand the context of the text better.\n",
    "\n",
    "Ref: https://research.trychroma.com/evaluating-chunking\n",
    "\n",
    "1. First we split the text into sentences using [BlingFire](https://github.com/microsoft/BlingFire) library.\n",
    "2. Then we group and combine chunks using semantic similarity and create chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/preprocessed_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = preprocessed_artifact.download()\n",
    "preprocessed_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "preprocessed_data = list(\n",
    "    map(json.loads, preprocessed_data_file.read_text().splitlines())\n",
    ")\n",
    "preprocessed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.chunking import chunk_documents\n",
    "\n",
    "display_source(chunk_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = chunk_documents(preprocessed_data)\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_chunk_size = np.mean([doc[\"metadata\"][\"parsed_tokens\"] for doc in chunked_data])\n",
    "std_chunk_size = np.std([doc[\"metadata\"][\"parsed_tokens\"] for doc in chunked_data])\n",
    "print(f\"Mean chunk size: {mean_chunk_size}, Std chunk size: {std_chunk_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we'll store the cleaned data in an artifact for future use and reproducibility\n",
    "\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], chunked_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(chunked_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": {\"mean\": mean_chunk_size, \"std\": std_chunk_size},\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in chunked_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also try a different retriever and see how it performs in comparison to the Tf-Idf retriever we had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rag_pipeline import SimpleRAGPipeline\n",
    "from scripts.response_generator import SimpleResponseGenerator\n",
    "from scripts.retriever import BM25Retriever, TFIDFRetriever\n",
    "\n",
    "display_source(BM25Retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever()\n",
    "bm25_retriever.index_data(chunked_data)\n",
    "\n",
    "tfidf_retriever = TFIDFRetriever()\n",
    "tfidf_retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of the rag pipeline remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\", \"r\").read()\n",
    "response_generator = SimpleResponseGenerator(model=\"command-r\", prompt=INITIAL_PROMPT)\n",
    "bm25_rag_pipeline = SimpleRAGPipeline(\n",
    "    retriever=bm25_retriever, response_generator=response_generator, top_k=5\n",
    ")\n",
    "tfidf_rag_pipeline = SimpleRAGPipeline(\n",
    "    retriever=tfidf_retriever, response_generator=response_generator, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and comapre the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retrieval_metrics import ALL_METRICS as RETRIEVAL_METRICS\n",
    "from scripts.response_metrics import ALL_METRICS as RESPONSE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_samples[:10],\n",
    "    scorers=RETRIEVAL_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\": 5},\n",
    ")\n",
    "bm25_retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(bm25_retriever))\n",
    "tfidf_retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(tfidf_retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=RESPONSE_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "bm25_response_scores = asyncio.run(response_evaluations.evaluate(bm25_rag_pipeline))\n",
    "tfidf_response_scores = asyncio.run(response_evaluations.evaluate(tfidf_rag_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "1. Add more data sources to the RAG system. - Add Jupyter Notbooks from the See wandb/examples repo.\n",
    "2. Use a different chunking method. - Try your own parsing and chunking method.\n",
    "3. Use a small-to-big retrieval method. Where we embed small documents but retrieve big documents -> You can add the parent document to the metadata and modify the `Retriever.search` method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
