{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 \n",
    "## Data Ingestion and Preprocessing\n",
    "\n",
    "One way to improve our RAG system is to improve our data ingestion and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import wandb\n",
    "import cohere\n",
    "import requests\n",
    "import markdown\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-use the raw dataset from the artifact in our previous step\n",
    "\n",
    "\n",
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier we referred to words as tokens. We can be more correct in defining tokens by using a tokenizer.\n",
    "# We'll use the Cohere tokenizer for this example.\n",
    "\n",
    "co = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    return co.tokenize(text=text, model=\"command-r\", offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {\n",
    "    \"command-r\": \"https://storage.googleapis.com/cohere-public/tokenizers/command-r.json\",\n",
    "    \"command-r-plus\": \"https://storage.googleapis.com/cohere-public/tokenizers/command-r-plus.json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in raw_data[:]:\n",
    "    doc['metadata']['words'] = doc['metadata'].pop('raw_tokens')\n",
    "    doc['metadata']['tokens'] = len(tokenize_text(doc['content']).tokens)\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "There is a lot of extra formatting information (markdown elements) that is not very useful to an LLM.\n",
    "\n",
    "We can remove this information by converting the contents to text. We can also remove any special characters and extra whitespace. \n",
    "\n",
    "Special characters here are ones that are defined in the tokenizer and will vary depending on the model used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import frontmatter\n",
    "\n",
    "def convert_contents_to_text(contents: str) -> str:\n",
    "    _, content = frontmatter.parse(contents)\n",
    "    # use some extensions to convert the markdown to html\n",
    "    markdown_document = markdown.markdown(\n",
    "        content,\n",
    "        extensions=[\n",
    "            \"toc\",\n",
    "            \"pymdownx.extra\",\n",
    "            \"pymdownx.blocks.admonition\",\n",
    "            \"pymdownx.magiclink\",\n",
    "            \"pymdownx.blocks.tab\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.saneheaders\",\n",
    "            \"pymdownx.striphtml\",\n",
    "            \"pymdownx.highlight\",\n",
    "            \"pymdownx.pathconverter\",\n",
    "            \"pymdownx.escapeall\"\n",
    "        ],\n",
    "    )\n",
    "    soup = BeautifulSoup(markdown_document, \"html.parser\")\n",
    "    def remove_urls_a_tags_hrefs(soup):\n",
    "        # For hyperlinks, keep the text but remove the link\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            a_tag.replace_with(a_tag.text)\n",
    "        \n",
    "        # Remove all images\n",
    "        for img_tag in soup.find_all('img'):\n",
    "            img_tag.decompose()\n",
    "        \n",
    "        # Remove all href attributes (this is now redundant for <a> tags, but keeps other elements clean)\n",
    "        # for tag in soup.find_all(href=True):\n",
    "        #     del tag['href']\n",
    "        \n",
    "        return soup\n",
    "\n",
    "    # Use the function as before\n",
    "    soup = remove_urls_a_tags_hrefs(soup)\n",
    "\n",
    "    def remove_javascript_import_statements(soup):\n",
    "        for p in soup.find_all('p'):\n",
    "            if p.text.strip().startswith('import') and ';' in p.text:\n",
    "                p.decompose()\n",
    "        return soup\n",
    "    soup = remove_javascript_import_statements(soup)\n",
    "\n",
    "    return soup.get_text()\n",
    "\n",
    "def get_special_tokens_set(tokenizer_url):\n",
    "    # https://docs.cohere.com/docs/tokens-and-tokenizers\n",
    "    response = requests.get(tokenizer_url)\n",
    "    return set([tok[\"content\"] for tok in response.json()[\"added_tokens\"]])\n",
    "\n",
    "special_tokens_set = get_special_tokens_set(tokenizers[\"command-r\"])\n",
    "def make_text_tokenization_safe(content: str, special_tokens_set: set=special_tokens_set) -> str:\n",
    "    \n",
    "    # Normalize newlines and replace multiple new lines with a single new line\n",
    "    # content = re.sub(r'\\n+', '\\n', content, flags=re.UNICODE)\n",
    "    \n",
    "\n",
    "    def remove_special_tokens(text: str) -> str:\n",
    "        \"\"\"Removes special tokens from the given text.\n",
    "\n",
    "        Args:\n",
    "            text: A string representing the text.\n",
    "\n",
    "        Returns:\n",
    "            The text with special tokens removed.\n",
    "        \"\"\"\n",
    "        for token in special_tokens_set:\n",
    "            text = text.replace(token, \"\")\n",
    "        return text\n",
    "\n",
    "    cleaned_content = remove_special_tokens(content)\n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = []\n",
    "\n",
    "for doc in raw_data:\n",
    "    parsed_doc = doc.copy()\n",
    "    content = convert_contents_to_text(doc[\"content\"])\n",
    "    parsed_doc[\"parsed_content\"] = make_text_tokenization_safe(content)\n",
    "    parsed_doc[\"metadata\"][\"parsed_tokens\"] = len(tokenize_text(parsed_doc[\"parsed_content\"]).tokens)\n",
    "    parsed_data.append(parsed_doc)\n",
    "parsed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum(map(lambda x: x[\"metadata\"][\"words\"], parsed_data))\n",
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"tokens\"], raw_data))\n",
    "total_parsed_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], parsed_data))\n",
    "\n",
    "preprocessed_artifact = wandb.Artifact(name=\"preprocessed_data\", type=\"dataset\",\n",
    "description=\"Preprocessed wandb documentation\", metadata={\n",
    "    \"total_files\": len(parsed_data),\n",
    "    \"date_preprocessed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"total_words\": total_words,\n",
    "    \"total_raw_tokens\": total_raw_tokens,\n",
    "    \"total_parsed_tokens\": total_parsed_tokens,\n",
    "    }\n",
    ")\n",
    "with preprocessed_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in parsed_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(preprocessed_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Chunking\n",
    "\n",
    "1. First we split the text into sentences using [BlingFire](https://github.com/microsoft/BlingFire) library.\n",
    "2. Then we split the sentences into chunks of a maximum number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blingfire import text_to_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_artifact = run.use_artifact(f'{WANDB_ENTITY}/{WANDB_PROJECT}/preprocessed_data:latest', type='dataset')\n",
    "artifact_dir = preprocessed_artifact.download()\n",
    "preprocessed_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "preprocessed_data = list(map(json.loads, preprocessed_data_file.read_text().splitlines()))\n",
    "preprocessed_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://platform.openai.com/docs/tutorials/web-qa-embeddings\n",
    "\n",
    "CHUNK_SIZE=500\n",
    "\n",
    "# Function to split the text into chunks of a maximum number of tokens\n",
    "\n",
    "\n",
    "def split_into_chunks(text, max_tokens = CHUNK_SIZE):\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text_to_sentences(text).split(\"\\n\")\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenize_text(\"\\n\" + sentence).tokens) for sentence in sentences]\n",
    "\n",
    "    chunks = []\n",
    "    tokens_so_far = 0\n",
    "    chunk = []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater\n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\"\\n\".join(chunk))\n",
    "            chunk = []\n",
    "            tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of\n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "for doc in preprocessed_data:\n",
    "    chunks = split_into_chunks(doc[\"parsed_content\"])\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            {\n",
    "                \"parsed_content\" : chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"parsed_tokens\": len(tokenize_text(chunk).tokens)\n",
    "            }})\n",
    "        \n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we'll store the cleaned data in an artifact for future use and reproducibility\n",
    "\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"parsed_tokens\"], chunked_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(chunked_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in chunked_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.index = None\n",
    "        self.data = None\n",
    "\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"parsed_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"parsed_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with a simple query\n",
    "\n",
    "\n",
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)\n",
    "\n",
    "query = \"How do I use W&B to log metrics in my training script?\"\n",
    "search_results = retriever.search(query)\n",
    "for result in search_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to generate a response grounded on the documentation.\n",
    "\n",
    "\n",
    "class ResponseGenerator:\n",
    "    def __init__(self, model: str, prompt: str):\n",
    "        self.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "        self.model = model\n",
    "        self.prompt = prompt\n",
    "\n",
    "    # @weave.op()\n",
    "\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        \n",
    "        documents = [{\"source\": item['source'], \"text\": item['text']} for item in context]\n",
    "        response = self.client.chat(\n",
    "            preamble=self.prompt,\n",
    "            message=query,\n",
    "            model=self.model,\n",
    "            documents=documents,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=PROMPT)\n",
    "answer = response_generator.generate_response(query, search_results)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(self, retriever: Retriever, response_generator: ResponseGenerator, top_k: int = 5):\n",
    "        self.retriever = retriever\n",
    "        self.response_generator = response_generator\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def __call__(self, query: str):\n",
    "        context = self.retriever.search(query, self.top_k)\n",
    "        return self.response_generator.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RAGPipeline(retriever, response_generator, top_k=10)\n",
    "response = rag_pipeline(query=\"Where do I find my API Key?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run, evaluate\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "RETRIEVAL_METRICS = [\"ndcg@10\", \"map@10\", \"mrr\", \"hit_rate\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "\n",
    "def evaluate_retriever(retrieved_docs: List[Dict[str, Any]], actual_doc: str) -> Dict[str, Any]:\n",
    "    qrels = Qrels({\"query\": {actual_doc: 1}})\n",
    "    run = Run({\"query\": {doc[\"source\"]: doc[\"score\"] for doc in retrieved_docs}})\n",
    "    return evaluate(qrels, run, metrics=RETRIEVAL_METRICS)\n",
    "\n",
    "\n",
    "retrieval_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = retriever.search(query, k=10)\n",
    "    eval_scores = evaluate_retriever(search_results, expected_source)\n",
    "    retrieval_scores.append({\"query\": query, **eval_scores})\n",
    "\n",
    "retrieval_scores_df = pd.DataFrame(retrieval_scores)\n",
    "display(retrieval_scores_df)\n",
    "\n",
    "print(\"\\nMean Overall Retrieval Scores:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Retrieval Score Statistics:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
    "    0 = represent that the excerpt has nothing to do with the query,\n",
    "    1 = represents that the excerpt seems related to the query but does not help answer it,\n",
    "    2 = represents that the excerpt has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information and\n",
    "    3 = represents that the excerpt is dedicated to the query and contains the exact answer.\n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the topic but not completely, category 2 if excerpt presents something very important related to the entire topic but also has some extra information and category 3 if the excerpt only and entirely refers to the topic. If none of the above satisfies give it category 0.\n",
    "\n",
    "Query: {query}\n",
    "Document: {document}\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "\n",
    "\n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> int:\n",
    "    response = await client.chat(\n",
    "        message=RETRIEVAL_EVAL_PROMPT.format(query=query, document=passage),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "sample = eval_samples[0]\n",
    "query = sample[\"question\"]\n",
    "search_results = retriever.search(query, k=5)\n",
    "tasks = []\n",
    "for result in search_results:\n",
    "    tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "sample_scores = asyncio.run(asyncio.gather(*tasks))\n",
    "sample_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_retriever_evaluation_using_llm(eval_samples):\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 3 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 3:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import Levenshtein\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate import meteor\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "def calculate_diff_score(candidate, reference):\n",
    "    return difflib.SequenceMatcher(None, candidate, reference).ratio()\n",
    "\n",
    "\n",
    "def calculate_levenshtein_score(candidate, reference):\n",
    "    return Levenshtein.ratio(candidate, reference)\n",
    "\n",
    "\n",
    "\n",
    "# semantic answer similarity. (SAS) - https://arxiv.org/abs/2108.06130\n",
    "# Originally, one should use a transformer based cross-encoder to measure and classify this. \n",
    "# For example, use something from https://sbert.net/docs/cross_encoder/usage/usage.html\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "def calculate_similarity(candidate, reference):\n",
    "    vectors = retriever.vectorizer.transform([candidate, reference])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    meteor_score = meteor([candidate], reference)\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RAGPipeline(retriever, response_generator)\n",
    "\n",
    "response_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample['question']\n",
    "    actual_answer = rag_pipeline(query)\n",
    "    expected_answer = sample['answer']\n",
    "    diff_score = calculate_diff_score(actual_answer, expected_answer)\n",
    "    levenshtein_score = calculate_levenshtein_score(actual_answer, expected_answer)\n",
    "    rouge_score = calculate_rouge(actual_answer, expected_answer)\n",
    "    bleu_score = calculate_bleu(actual_answer, expected_answer)\n",
    "    meteor_score = calculate_meteor(actual_answer, expected_answer)\n",
    "    similarity_score = calculate_similarity(actual_answer, expected_answer)\n",
    "\n",
    "    response_scores.append({\n",
    "        \"query\": query,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"diff_score\": diff_score,\n",
    "        \"levenshtein_score\": levenshtein_score,\n",
    "        \"rouge_score\": rouge_score,\n",
    "        \"bleu_score\": bleu_score,\n",
    "        \"meteor_score\": meteor_score,\n",
    "        \"similarity_score\": similarity_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_scores_df = pd.DataFrame(response_scores)\n",
    "display(response_scores_df)\n",
    "\n",
    "GENERATION_METRICS = [col for col in response_scores_df.columns if \"score\" in col]\n",
    "\n",
    "\n",
    "print(\"\\nMean Overall Generation Scores:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Generation Score Statistics:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 3 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct but includes some extra information, is incomplete or misses some evaluation criteria.\n",
    "- 3 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 3,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 3:\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it includes assumptions about functions that are not requested in the user's query\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 4:\n",
    "{{\n",
    "    \"reason\": \"The generated answer is incorrect, irrelevant, and not factually correct and completely misses the user's intent.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "async def evaluate_correctness_using_llm_judge(query: str, reference_answer: str, generated_answer: str) -> int:\n",
    "    response = await client.chat(\n",
    "        message=CORRECTNESS_EVAL_PROMPT.format(query=query, reference_answer=reference_answer, generated_answer=generated_answer),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_correctness_evaluation_using_llm(response_scores):\n",
    "    tasks = []\n",
    "    for row in response_scores:\n",
    "        query = row[\"query\"]\n",
    "        expected_answer = row[\"expected_answer\"]\n",
    "        generated_answer = row[\"actual_answer\"]\n",
    "        tasks.append(evaluate_correctness_using_llm_judge(query, expected_answer, generated_answer))\n",
    "    scores = await asyncio.gather(*tasks)\n",
    "    scores = list(map(json.loads, scores))\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_correctness_results = asyncio.run(run_correctness_evaluation_using_llm(response_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_df = pd.DataFrame(llm_judge_correctness_results)\n",
    "response_evals_df = pd.concat([response_scores_df, correctness_eval_df], axis=1)\n",
    "response_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_response_accuracy = (response_evals_df[\"decision\"] == \"correct\").sum()/len(response_evals_df)\n",
    "print(f\"LLM Judge Response Accuracy: {llm_judge_response_accuracy:.4f}\")\n",
    "response_evals_df['final_score'].value_counts().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
