{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP Initial simple RAG example with eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import dotenv\n",
    "import Levenshtein\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from IPython.display import Markdown\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from openai import AsyncOpenAI\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"parambharat\"\n",
    "WANDB_PROJECT = \"advanced_rag\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    job_type=\"data_ingestion\",\n",
    "    group=\"initial_example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/documentation_data:latest\", type=\"dataset\"\n",
    ")\n",
    "data_dir = \"../data/wandb_docs\"\n",
    "\n",
    "docs_dir = documents_artifact.download(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = pathlib.Path(docs_dir)\n",
    "docs_files = sorted(docs_dir.rglob(\"*.md\"))\n",
    "\n",
    "print(f\"Number of files: {len(docs_files)}\\n\")\n",
    "print(\"First 5 files:\\n{files}\".format(files=\"\\n\".join(map(str, docs_files[:5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at an example file\n",
    "print(docs_files[0].read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll store the files as dictionaries with some content and metadata\n",
    "data = []\n",
    "for file in docs_files:\n",
    "    content = file.read_text()\n",
    "    data.append(\n",
    "        {\n",
    "            \"content\": content,\n",
    "            \"metadata\": {\n",
    "                \"source\": str(file.relative_to(docs_dir)),\n",
    "                \"raw_tokens\": len(content.split()),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], data))\n",
    "print(f\"Total Tokens in dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store the raw data in an artifact for future use and reproducibility\n",
    "\n",
    "raw_artifact = wandb.Artifact(\n",
    "    name=\"raw_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Raw wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_tokens,\n",
    "    },\n",
    ")\n",
    "with raw_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(raw_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are hyperparameters of our ingestion pipeline\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 0\n",
    "\n",
    "\n",
    "def split_into_chunks(\n",
    "    text: str, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> List[str]:\n",
    "    \"\"\"Function to split the text into chunks of a maximum number of tokens\n",
    "    ensure that the chunks are of size CHUNK_SIZE and overlap by chunk_overlap tokens\n",
    "    use the `tokenizer.encode` method to tokenize the text\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start = end - chunk_overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-use the raw dataset from the artifact in our previous step\n",
    "\n",
    "\n",
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "for doc in raw_data:\n",
    "    chunks = split_into_chunks(doc[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            {\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"raw_tokens\": len(chunk.split()),\n",
    "                },\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of our examples have special tokens that we need to remove otherwise it will break the chat.completions api.\n",
    "\n",
    "\n",
    "def make_text_tokenization_safe(content: str) -> str:\n",
    "    special_tokens_set = {\n",
    "        \"<|endofprompt|>\",\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|fim_middle|>\",\n",
    "        \"<|fim_prefix|>\",\n",
    "        \"<|fim_suffix|>\",\n",
    "    }\n",
    "\n",
    "    def remove_special_tokens(text: str) -> str:\n",
    "        \"\"\"Removes special tokens from the given text.\n",
    "\n",
    "        Args:\n",
    "            text: A string representing the text.\n",
    "\n",
    "        Returns:\n",
    "            The text with special tokens removed.\n",
    "        \"\"\"\n",
    "        for token in special_tokens_set:\n",
    "            text = text.replace(token, \"\")\n",
    "        return text\n",
    "\n",
    "    cleaned_content = remove_special_tokens(content)\n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "for doc in chunked_data:\n",
    "    cleaned_doc = doc.copy()\n",
    "    cleaned_doc[\"cleaned_content\"] = make_text_tokenization_safe(doc[\"content\"])\n",
    "    cleaned_doc[\"metadata\"][\"cleaned_tokens\"] = len(\n",
    "        cleaned_doc[\"cleaned_content\"].split()\n",
    "    )\n",
    "    cleaned_data.append(cleaned_doc)\n",
    "cleaned_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we'll store the cleaned data in an artifact for future use and reproducibility\n",
    "\n",
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], cleaned_data))\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"cleaned_tokens\"], cleaned_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(cleaned_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in cleaned_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can re-use the chunked data from the artifact in our previous step\n",
    "\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a simple retriever class to get the most relevant chunks of data for a given query.\n",
    "# We'll use TF-IDF to vectorize the documents and cosine distance to measure the similarity between the query and the documents.\n",
    "# Two methods: index_data and search\n",
    "# index_data will take the data and vectorize it and store the index\n",
    "# search will take a query and return the most relevant chunks from the index\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.index = None\n",
    "        self.data = None\n",
    "\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    # @weave.op()\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test with a simple query\n",
    "\n",
    "\n",
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)\n",
    "retriever.search(\"How do I get get started with wandb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to generate a response grounded on the documentation.\n",
    "\n",
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "\n",
    "# @weave.op()\n",
    "async def generate_response(query):\n",
    "    context = retriever.search(query)\n",
    "    context_text = \"\\n\".join(\n",
    "        [f\"Source: {item['source']}\\nText: {item['text']}\\n\\n\" for item in context]\n",
    "    )\n",
    "\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful customer support assistant that can answer questions about W&B\\n\\n\"\n",
    "        \"Your answers must be based only on the provided context.\\n\\n\"\n",
    "        f\"<context>\\n{context_text}\\n</context>\",\n",
    "    }\n",
    "\n",
    "    user_message = {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nAnswer:\"}\n",
    "\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=[system_message, user_message]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = asyncio.run(generate_response(\"How do I get get started with wandb?\"))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RAG system\n",
    "\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank) is a metric that measures the quality of the retrieval system by evaluating the proportion of queries for which the most relevant document is retrieved.\n",
    "# Let's calculate the MRR score for our retrieval system\n",
    "\n",
    "\n",
    "def calculate_mrr(\n",
    "    retriever: Retriever, eval_samples: List[Dict[str, Any]], k: int = 5\n",
    ") -> pd.DataFrame:\n",
    "    results = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        expected_source = sample[\"source\"]\n",
    "\n",
    "        search_results = retriever.search(query, k=k)\n",
    "\n",
    "        # the rank of the expected source\n",
    "        for rank, result in enumerate(search_results, 1):\n",
    "            if result[\"source\"] == expected_source:\n",
    "                mrr_score = 1 / rank\n",
    "                break\n",
    "        else:\n",
    "            # expected source not found in top k results\n",
    "            mrr_score = 0\n",
    "\n",
    "        results.append(\n",
    "            {\"Query\": query, \"Expected Source\": expected_source, \"MRR Score\": mrr_score}\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df[\"MRR Score\"] = df[\"MRR Score\"].astype(float)  # Ensure MRR Score is float\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results_df = calculate_mrr(retriever, eval_samples)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mrr = results_df[\"MRR Score\"].mean()\n",
    "print(f\"Mean MRR Score: {overall_mrr:.4f}\")\n",
    "\n",
    "# Calculate and print overall MRR score\n",
    "display(pd.DataFrame(results_df[\"MRR Score\"].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other metrics include:\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "# MAP (Mean Average Precision)\n",
    "# Hit Rate\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 Score\n",
    "\n",
    "RETRIEVAL_METRICS = [\"ndcg@5\", \"map@5\", \"mrr\", \"hit_rate\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "\n",
    "def evaluate_retriever(retriever, eval_samples, k=5, metrics=RETRIEVAL_METRICS):\n",
    "    # Prepare qrels_dict\n",
    "    qrels_dict = {}\n",
    "    for i, sample in enumerate(eval_samples):\n",
    "        qrels_dict[f\"q_{i}\"] = {\n",
    "            sample[\"source\"]: 1\n",
    "        }  # Assuming relevance of 1 for the correct source\n",
    "\n",
    "    # Prepare run_dict\n",
    "    run_dict = {}\n",
    "    for i, sample in enumerate(eval_samples):\n",
    "        query = sample[\"question\"]\n",
    "        results = retriever.search(query, k=k)\n",
    "        run_dict[f\"q_{i}\"] = {result[\"source\"]: result[\"score\"] for result in results}\n",
    "\n",
    "    # Create Qrels and Run objects\n",
    "    qrels = Qrels(qrels_dict)\n",
    "    run = Run(run_dict)\n",
    "\n",
    "    # Compute metrics\n",
    "    score_dict = evaluate(qrels, run, metrics, return_mean=False)\n",
    "\n",
    "    # Combine eval_samples and scores into a DataFrame\n",
    "    results_df = pd.concat(\n",
    "        [pd.DataFrame(eval_samples), pd.DataFrame(score_dict)], axis=1\n",
    "    )\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = evaluate_retriever(retriever, eval_samples)\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nMean Overall Retrieval Scores:\")\n",
    "display(pd.DataFrame(results_df[RETRIEVAL_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Retrieval Score Statistics:\")\n",
    "display(pd.DataFrame(results_df[RETRIEVAL_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "\n",
    "def calculate_diff_score(candidate, reference):\n",
    "    return difflib.SequenceMatcher(None, candidate, reference).ratio()\n",
    "\n",
    "\n",
    "def calculate_levenshtein_score(candidate, reference):\n",
    "    return Levenshtein.ratio(candidate, reference)\n",
    "\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    meteor_score = meteor([candidate], reference)\n",
    "    return meteor_score\n",
    "\n",
    "\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "\n",
    "\n",
    "def calculate_similarity(candidate, reference):\n",
    "    vectors = retriever.vectorizer.transform([candidate, reference])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "async def evaluate_response(eval_samples):\n",
    "    tasks = []\n",
    "    for eval_sample in eval_samples:\n",
    "        tasks.append(generate_response(eval_sample[\"question\"]))\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "    results = []\n",
    "    for response, eval_sample in zip(responses, eval_samples):\n",
    "        results.append(\n",
    "            {\n",
    "                \"Query\": eval_sample[\"question\"],\n",
    "                \"Source\": eval_sample[\"source\"],\n",
    "                \"Expected_Answer\": eval_sample[\"answer\"],\n",
    "                \"Actual_Answer\": response,\n",
    "                \"Diff_Score\": calculate_diff_score(response, eval_sample[\"answer\"]),\n",
    "                \"Levenshtein_Score\": calculate_levenshtein_score(\n",
    "                    response, eval_sample[\"answer\"]\n",
    "                ),\n",
    "                \"Rouge(l)_Score\": calculate_rouge(response, eval_sample[\"answer\"]),\n",
    "                \"BLEU_Score\": calculate_bleu(response, eval_sample[\"answer\"]),\n",
    "                \"Meteor_Score\": calculate_meteor(response, eval_sample[\"answer\"]),\n",
    "                \"Similarity_Score\": calculate_similarity(\n",
    "                    response, eval_sample[\"answer\"]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_eval = asyncio.run(evaluate_response(eval_samples))\n",
    "generation_eval_df = pd.DataFrame(generation_eval).round(4)\n",
    "display(generation_eval_df)\n",
    "\n",
    "\n",
    "GENERATION_METRICS = [col for col in generation_eval_df.columns if \"Score\" in col]\n",
    "\n",
    "\n",
    "print(\"\\nMean Overall Generation Scores:\")\n",
    "display(pd.DataFrame(generation_eval_df[GENERATION_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Generation Score Statistics:\")\n",
    "display(pd.DataFrame(generation_eval_df[GENERATION_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
