{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP # Chapter 2:\n",
    "\n",
    "**Comprehensive Evaluation Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and improving an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this once we more to the final project\n",
    "# eval_artifact = wandb.Artifact(\n",
    "#     name=\"eval_dataset\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"Evaluation dataset for RAG\",\n",
    "#     metadata={\n",
    "#         \"total_samples\": 20,\n",
    "#         \"date_collected\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#         \"chapter\": \"Chapter 1\",\n",
    "#     },\n",
    "# )\n",
    "# eval_artifact.add_file(\"../data/eval/eval_dataset.jsonl\")\n",
    "# run.log_artifact(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparambharat\u001b[0m (\u001b[33mrag-course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/edu/rag-advanced/notebooks/wandb/run-20240702_163437-okxu0mvh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rag-course/dev/runs/okxu0mvh' target=\"_blank\">daily-durian-4</a></strong> to <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">https://wandb.ai/rag-course/dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rag-course/dev/runs/okxu0mvh' target=\"_blank\">https://wandb.ai/rag-course/dev/runs/okxu0mvh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/02 16:34:40 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/0z2t11h3/artifact/936067298/wandb_manifest.json?Expires=1719921880&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=QSPBA2p3h5o6EKKERbHYIZZm8Y1w3D71wbhqX3FDorBwnIdrbGWczBnqsepoPKrUMb5N6sqg%2BMfPUzK%2BpDl%2FIFFHly4%2Fz7IwIjmiBYPihprH0eELpcyRC%2FuDePSFflKy8toziKsH%2F6wcAq9wMGftxd1yH%2B%2BYrzQwRavwASKuBKgeBkcLHqlxfYoOUCkQVHymVfBDCIpFEIiMGKHa91yVexwHO6zIsB7PMbN1T8z32RqLCPNqb5iSQiTqYrklRtV7jxJ2lxp%2FoU52hNq0L2heo5jVgcfBzh3hfrj742C7mBWKIXQoSbkyBReU3QS1rqJcFOon9y%2BiCkCYxoG5rph6mQ%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>guides/technical-faq/admin.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>guides/technical-faq/admin.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>guides/technical-faq/setup.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>guides/technical-faq/troubleshooting.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>guides/technical-faq/troubleshooting.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>quickstart.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>guides/track/launch.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>track/log/log-tables.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>track/log/logging-faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>guides/sweep/faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>guides/sweep/faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>guides/track/tracking-faq.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>guides/track/tracking-faq.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `\\/` as a separator for organizing lo...</td>\n",
       "      <td>guides/track/log/logging-faqs.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `\\/` as a separator for organizing lo...   \n",
       "\n",
       "                                             source  \n",
       "0                   guides/technical-faq/general.md  \n",
       "1                   guides/technical-faq/general.md  \n",
       "2                   guides/technical-faq/general.md  \n",
       "3                     guides/technical-faq/admin.md  \n",
       "4                     guides/technical-faq/admin.md  \n",
       "5   guides/technical-faq/metrics-and-performance.md  \n",
       "6   guides/technical-faq/metrics-and-performance.md  \n",
       "7      guides/technical-faq/metrics-and-performance  \n",
       "8                     guides/technical-faq/setup.md  \n",
       "9           guides/technical-faq/troubleshooting.md  \n",
       "10          guides/technical-faq/troubleshooting.md  \n",
       "11                                    quickstart.md  \n",
       "12                           guides/track/launch.md  \n",
       "13                          track/log/log-tables.md  \n",
       "14                        track/log/logging-faqs.md  \n",
       "15                             guides/sweep/faqs.md  \n",
       "16                             guides/sweep/faqs.md  \n",
       "17                     guides/track/tracking-faq.md  \n",
       "18                     guides/track/tracking-faq.md  \n",
       "19                 guides/track/log/logging-faqs.md  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics\n",
    "\n",
    "**TODO** Add weave model and evals in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/02 16:34:46 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/0z2t11h3/artifact/936065852/wandb_manifest.json?Expires=1719921885&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=UocaW4lsv%2FPIMt0iLvLunZwbiJDSL6o9h8BvfW41Hh%2B%2BMyzeV42j6t1keNmJDcCT5iM4MHenBiMeVRCc8VV65bjROvKSGmQ8t8JXwGCfR7PTZop52AJHOAeCch8uDiSN5pauqVJTbN2aU%2BWFehQ6UKn%2BX08X%2ByavCSWu7OFYYmDOlN16WNZs3zykPSg222UGdSsQd9G%2FzAJYFLE154FtpPu%2BNuZLkqU4CAS8cdrQXpxA%2Fw49ZPpKyTKjqSjAHk%2FLuxF9WoXlQBvlyCKEDC0TXuU0CemsJuPtBGFAPgO2albWw97KhMOoWQXJk4sDgC7HB0L95Oc129%2BrRsZhU5sSDA%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 470,\n",
       "   'cleaned_tokens': 470},\n",
       "  'cleaned_content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```'},\n",
       " {'content': '--- slug: /guides/app/features/custom-charts displayed_sidebar: default --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; # Custom Charts Use **Custom Charts** to create charts that aren\\'t possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of [Vega](https://vega.github.io/vega/). * **What\\'s possible**: Read the[ launch announcement →](https://wandb.ai/wandb/posts/reports/Announcing-the-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg) * **Code**: Try a live example in a[ hosted notebook →](https://tiny.cc/custom-charts) * **Video**: Watch a quick [walkthrough video →](https://www.youtube.com/watch?v=3-N9OV6bkSM) * **Example**: Quick Keras and Sklearn [demo notebook →](https://colab.research.google.com/drive/1g-gNGokPWM2Qbc8p1Gofud0\\\\_5AoZdoSD?usp=sharing) ![Supported charts from vega.github.io/vega](/images/app_ui/supported_charts.png) ### How it works 1. **Log data**: From your script, log [config](../../../../guides/track/config.md) and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table` 2. **Customize the chart**: Pull in any of this logged data with a [GraphQL](https://graphql.org) query. Visualize the results of your query with [Vega](https://vega.github.io/vega/), a powerful visualization grammar. 3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`. ![](/images/app_ui/pr_roc.png) ## Log charts from a script ### Builtin presets These presets have builtin `wandb.plot` methods that make it fast to log charts directly from your script and see the exact visualizations you\\'re looking for in the UI. <Tabs defaultValue=\"line-plot\" values={[ {label: \\'Line plot\\', value: \\'line-plot\\'}, {label: \\'Scatter plot\\', value: \\'scatter-plot\\'}, {label: \\'Bar chart\\', value: \\'bar-chart\\'}, {label: \\'Histogram\\', value: \\'histogram\\'}, {label: \\'PR curve\\', value: \\'pr-curve\\'}, {label: \\'ROC curve\\', value: \\'roc-curve\\'}, ]}> <TabItem value=\"line-plot\"> `wandb.plot.line()` Log a custom line plot—a list of connected and ordered points (x,y) on arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(x_values, y_values)] table = wandb.Table(data=data, columns=[\"x\", \"y\"]) wandb.log( { \"my_custom_plot_id\": wandb.plot.line( table, \"x\", \"y\", title=\"Custom Y vs X Line Plot\" ) } ) ``` You can use this to log curves on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/line_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Line-Plots--VmlldzoyNjk5NTA) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"scatter-plot\"> `wandb.plot.scatter()` Log a custom scatter plot—a list of points (x, y) on a pair of arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(class_x_prediction_scores, class_y_prediction_scores)] table = wandb.Table(data=data, columns=[\"class_x\", \"class_y\"]) wandb.log({\"my_custom_id\": wandb.plot.scatter(table, \"class_x\", \"class_y\")}) ``` You can use this to log scatter points on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/demo_scatter_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Scatter-Plots--VmlldzoyNjk5NDQ) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"bar-chart\"> `wandb.plot.bar()` Log a custom bar chart—a list of labeled values as bars—natively in a few lines: ```python data = [[label, val] for (label, val) in zip(labels, values)] table = wandb.Table(data=data, columns=[\"label\", \"value\"]) wandb.log( { \"my_bar_chart_id\": wandb.plot.bar( table, \"label\", \"value\", title=\"Custom Bar Chart\" ) } ) ``` You',\n",
       "  'metadata': {'source': 'guides/app/features/custom-charts/intro.md',\n",
       "   'raw_tokens': 500,\n",
       "   'cleaned_tokens': 500},\n",
       "  'cleaned_content': '--- slug: /guides/app/features/custom-charts displayed_sidebar: default --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; # Custom Charts Use **Custom Charts** to create charts that aren\\'t possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of [Vega](https://vega.github.io/vega/). * **What\\'s possible**: Read the[ launch announcement →](https://wandb.ai/wandb/posts/reports/Announcing-the-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg) * **Code**: Try a live example in a[ hosted notebook →](https://tiny.cc/custom-charts) * **Video**: Watch a quick [walkthrough video →](https://www.youtube.com/watch?v=3-N9OV6bkSM) * **Example**: Quick Keras and Sklearn [demo notebook →](https://colab.research.google.com/drive/1g-gNGokPWM2Qbc8p1Gofud0\\\\_5AoZdoSD?usp=sharing) ![Supported charts from vega.github.io/vega](/images/app_ui/supported_charts.png) ### How it works 1. **Log data**: From your script, log [config](../../../../guides/track/config.md) and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table` 2. **Customize the chart**: Pull in any of this logged data with a [GraphQL](https://graphql.org) query. Visualize the results of your query with [Vega](https://vega.github.io/vega/), a powerful visualization grammar. 3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`. ![](/images/app_ui/pr_roc.png) ## Log charts from a script ### Builtin presets These presets have builtin `wandb.plot` methods that make it fast to log charts directly from your script and see the exact visualizations you\\'re looking for in the UI. <Tabs defaultValue=\"line-plot\" values={[ {label: \\'Line plot\\', value: \\'line-plot\\'}, {label: \\'Scatter plot\\', value: \\'scatter-plot\\'}, {label: \\'Bar chart\\', value: \\'bar-chart\\'}, {label: \\'Histogram\\', value: \\'histogram\\'}, {label: \\'PR curve\\', value: \\'pr-curve\\'}, {label: \\'ROC curve\\', value: \\'roc-curve\\'}, ]}> <TabItem value=\"line-plot\"> `wandb.plot.line()` Log a custom line plot—a list of connected and ordered points (x,y) on arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(x_values, y_values)] table = wandb.Table(data=data, columns=[\"x\", \"y\"]) wandb.log( { \"my_custom_plot_id\": wandb.plot.line( table, \"x\", \"y\", title=\"Custom Y vs X Line Plot\" ) } ) ``` You can use this to log curves on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/line_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Line-Plots--VmlldzoyNjk5NTA) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"scatter-plot\"> `wandb.plot.scatter()` Log a custom scatter plot—a list of points (x, y) on a pair of arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(class_x_prediction_scores, class_y_prediction_scores)] table = wandb.Table(data=data, columns=[\"class_x\", \"class_y\"]) wandb.log({\"my_custom_id\": wandb.plot.scatter(table, \"class_x\", \"class_y\")}) ``` You can use this to log scatter points on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/demo_scatter_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Scatter-Plots--VmlldzoyNjk5NDQ) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"bar-chart\"> `wandb.plot.bar()` Log a custom bar chart—a list of labeled values as bars—natively in a few lines: ```python data = [[label, val] for (label, val) in zip(labels, values)] table = wandb.Table(data=data, columns=[\"label\", \"value\"]) wandb.log( { \"my_bar_chart_id\": wandb.plot.bar( table, \"label\", \"value\", title=\"Custom Bar Chart\" ) } ) ``` You'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the Retriever class from Chapter 1\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.index = None\n",
    "        self.data = None\n",
    "\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(retrieved_docs: List[str], actual_doc: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate for a single query.\n",
    "\n",
    "    :param retrieved_docs: List of retrieved documents\n",
    "    :param actual_doc: The single actual relevant document\n",
    "    :return: Hit rate (1 if the relevant document is retrieved, 0 otherwise)\n",
    "    \"\"\"\n",
    "    return 1 if actual_doc in retrieved_docs else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  hit_rate\n",
       "0   What is the difference between `.log()` and `....         1\n",
       "1   How do I switch between accounts on the same m...         1\n",
       "2              How is W&B different from TensorBoard?         1\n",
       "3   What is the difference between team and organi...         1\n",
       "4   What is the difference between team and entity...         1\n",
       "5   Can I just log metrics, no code or dataset exa...         1\n",
       "6   How can I log a metric that doesn't change ove...         1\n",
       "7                How many runs to create per project?         0\n",
       "8                            Can I run wandb offline?         0\n",
       "9                  How do I deal with network issues?         1\n",
       "10  What happens if internet connection is lost wh...         1\n",
       "11                        Where do I find my API key?         1\n",
       "12                  How do I create a W&B Experiment?         1\n",
       "13                               Log a table to a run         0\n",
       "14                     How do I log a list of values?         0\n",
       "15  Is there a way to add extra values to a sweep,...         0\n",
       "16  Can we flag boolean variables as hyperparameters?         0\n",
       "17  How do I programmatically access the human-rea...         1\n",
       "18  How can I save the git commit associated with ...         1\n",
       "19  How can I organize my logged charts and media ...         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hit_rates = []\n",
    "for sample in eval_samples:\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "    hit_rate = calculate_hit_rate(search_results, expected_source)\n",
    "    hit_rates.append({\"query\": query, \"hit_rate\": hit_rate})\n",
    "\n",
    "hit_rate_df = pd.DataFrame(hit_rates)\n",
    "display(hit_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Hit Rate: 0.7000\n",
      "Std-dev Hit Rate: 0.4702\n"
     ]
    }
   ],
   "source": [
    "# we need a single number to rate the retrieval system\n",
    "# the mean hit rate is a good metric to evaluate the retrieval system as a whole\n",
    "\n",
    "print(f\"Mean Hit Rate: {hit_rate_df['hit_rate'].mean():.4f}\")\n",
    "print(f\"Std-dev Hit Rate: {hit_rate_df['hit_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank) is a metric that measures the quality of the retrieval system by evaluating the proportion of queries for which the most relevant document is retrieved.\n",
    "# Let's calculate the MRR score for our retrieval system\n",
    "\n",
    "def calculate_mrr(retrieved_docs: List[str], actual_doc: str) -> float:\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(retrieved_docs, 1):\n",
    "        if result == actual_doc:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>mrr_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  mrr_score\n",
       "0   What is the difference between `.log()` and `....   0.500000\n",
       "1   How do I switch between accounts on the same m...   1.000000\n",
       "2              How is W&B different from TensorBoard?   0.333333\n",
       "3   What is the difference between team and organi...   1.000000\n",
       "4   What is the difference between team and entity...   1.000000\n",
       "5   Can I just log metrics, no code or dataset exa...   1.000000\n",
       "6   How can I log a metric that doesn't change ove...   0.500000\n",
       "7                How many runs to create per project?   0.000000\n",
       "8                            Can I run wandb offline?   0.000000\n",
       "9                  How do I deal with network issues?   1.000000\n",
       "10  What happens if internet connection is lost wh...   1.000000\n",
       "11                        Where do I find my API key?   0.500000\n",
       "12                  How do I create a W&B Experiment?   1.000000\n",
       "13                               Log a table to a run   0.000000\n",
       "14                     How do I log a list of values?   0.000000\n",
       "15  Is there a way to add extra values to a sweep,...   0.000000\n",
       "16  Can we flag boolean variables as hyperparameters?   0.000000\n",
       "17  How do I programmatically access the human-rea...   0.500000\n",
       "18  How can I save the git commit associated with ...   0.500000\n",
       "19  How can I organize my logged charts and media ...   0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mrr_scores = []\n",
    "for sample in eval_samples:\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "    mrr_score = calculate_mrr(search_results, expected_source)\n",
    "    mrr_scores.append({\"query\": query, \"mrr_score\": mrr_score})\n",
    "\n",
    "mrr_scores_df = pd.DataFrame(mrr_scores)\n",
    "display(mrr_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MRR Score: 0.5083\n",
      "MRR Score Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mrr_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.417017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count      mean       std  min  25%  50%  75%  max\n",
       "mrr_score   20.0  0.508333  0.417017  0.0  0.0  0.5  1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need a single number to rate the retrieval system\n",
    "# the mean mrr score is a good metric to evaluate the retrieval system\n",
    "print(f\"Mean MRR Score: {mrr_scores_df['mrr_score'].mean():.4f}\")\n",
    "\n",
    "# Looking at the mean might not give us the complete picture. We can also look at the distribution of the MRR scores\n",
    "print(\"MRR Score Statistics:\")\n",
    "display(pd.DataFrame(mrr_scores_df[\"mrr_score\"].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can also evaluate the retrieval system on other metrics\n",
    "# Writing these might be tedious, but we can use the `ranx` library to evaluate the retrieval system\n",
    "# Metrics Include\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "# MAP (Mean Average Precision)\n",
    "# Hit Rate\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 Score\n",
    "\n",
    "\n",
    "RETRIEVAL_METRICS = [\"ndcg@5\", \"map@5\", \"mrr\", \"hit_rate\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "\n",
    "def evaluate_retriever(retrieved_docs: List[Dict[str, Any]], actual_doc: str) -> Dict[str, Any]:\n",
    "    qrels = Qrels({\"query\": {actual_doc: 1}})\n",
    "    run = Run({\"query\": {doc[\"source\"]: doc[\"score\"] for doc in retrieved_docs}})\n",
    "    return evaluate(qrels, run, metrics=RETRIEVAL_METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 26.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>map@5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query   ndcg@5     map@5  \\\n",
       "0   What is the difference between `.log()` and `....  0.63093  0.500000   \n",
       "1   How do I switch between accounts on the same m...  1.00000  1.000000   \n",
       "2              How is W&B different from TensorBoard?  0.50000  0.333333   \n",
       "3   What is the difference between team and organi...  1.00000  1.000000   \n",
       "4   What is the difference between team and entity...  1.00000  1.000000   \n",
       "5   Can I just log metrics, no code or dataset exa...  1.00000  1.000000   \n",
       "6   How can I log a metric that doesn't change ove...  0.63093  0.500000   \n",
       "7                How many runs to create per project?  0.00000  0.000000   \n",
       "8                            Can I run wandb offline?  0.00000  0.000000   \n",
       "9                  How do I deal with network issues?  1.00000  1.000000   \n",
       "10  What happens if internet connection is lost wh...  1.00000  1.000000   \n",
       "11                        Where do I find my API key?  0.63093  0.500000   \n",
       "12                  How do I create a W&B Experiment?  1.00000  1.000000   \n",
       "13                               Log a table to a run  0.00000  0.000000   \n",
       "14                     How do I log a list of values?  0.00000  0.000000   \n",
       "15  Is there a way to add extra values to a sweep,...  0.00000  0.000000   \n",
       "16  Can we flag boolean variables as hyperparameters?  0.00000  0.000000   \n",
       "17  How do I programmatically access the human-rea...  0.63093  0.500000   \n",
       "18  How can I save the git commit associated with ...  0.63093  0.500000   \n",
       "19  How can I organize my logged charts and media ...  0.50000  0.333333   \n",
       "\n",
       "         mrr  hit_rate  precision  recall        f1  \n",
       "0   0.500000       1.0   0.250000     1.0  0.400000  \n",
       "1   1.000000       1.0   0.200000     1.0  0.333333  \n",
       "2   0.333333       1.0   0.333333     1.0  0.500000  \n",
       "3   1.000000       1.0   0.250000     1.0  0.400000  \n",
       "4   1.000000       1.0   0.200000     1.0  0.333333  \n",
       "5   1.000000       1.0   0.250000     1.0  0.400000  \n",
       "6   0.500000       1.0   0.200000     1.0  0.333333  \n",
       "7   0.000000       0.0   0.000000     0.0  0.000000  \n",
       "8   0.000000       0.0   0.000000     0.0  0.000000  \n",
       "9   1.000000       1.0   0.200000     1.0  0.333333  \n",
       "10  1.000000       1.0   0.250000     1.0  0.400000  \n",
       "11  0.500000       1.0   0.200000     1.0  0.333333  \n",
       "12  1.000000       1.0   0.200000     1.0  0.333333  \n",
       "13  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "14  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "15  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "16  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "17  0.500000       1.0   0.250000     1.0  0.400000  \n",
       "18  0.500000       1.0   0.250000     1.0  0.400000  \n",
       "19  0.333333       1.0   0.200000     1.0  0.333333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieval_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = retriever.search(query, k=5)\n",
    "    eval_scores = evaluate_retriever(search_results, expected_source)\n",
    "    retrieval_scores.append({\"query\": query, **eval_scores})\n",
    "\n",
    "retrieval_scores_df = pd.DataFrame(retrieval_scores)\n",
    "display(retrieval_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Overall Retrieval Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>map@5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557732</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.161667</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.261667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ndcg@5     map@5       mrr  hit_rate  precision  recall        f1\n",
       "0  0.557732  0.508333  0.508333       0.7   0.161667     0.7  0.261667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Retrieval Score Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ndcg@5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.557732</td>\n",
       "      <td>0.414796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map@5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.417017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrr</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.417017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_rate</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.470162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.161667</td>\n",
       "      <td>0.113181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.470162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.180407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count      mean       std  min  25%       50%   75%       max\n",
       "ndcg@5      20.0  0.557732  0.414796  0.0  0.0  0.630930  1.00  1.000000\n",
       "map@5       20.0  0.508333  0.417017  0.0  0.0  0.500000  1.00  1.000000\n",
       "mrr         20.0  0.508333  0.417017  0.0  0.0  0.500000  1.00  1.000000\n",
       "hit_rate    20.0  0.700000  0.470162  0.0  0.0  1.000000  1.00  1.000000\n",
       "precision   20.0  0.161667  0.113181  0.0  0.0  0.200000  0.25  0.333333\n",
       "recall      20.0  0.700000  0.470162  0.0  0.0  1.000000  1.00  1.000000\n",
       "f1          20.0  0.261667  0.180407  0.0  0.0  0.333333  0.40  0.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nMean Overall Retrieval Scores:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Retrieval Score Statistics:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a passage, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
    "    0 = represent that the passage has nothing to do with the query,\n",
    "    1 = represents that the passage seems related to the query but does not answer it,\n",
    "    2 = represents that the passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information and\n",
    "    3 = represents that the passage is dedicated to the query and contains the exact answer.\n",
    "\n",
    "Important Instruction: Assign category 1 if the passage is somewhat related to the topic but not completely, category 2 if passage presents something very important related to the entire topic but also has some extra information and category 3 if the passage only and entirely refers to the topic. If none of the above satisfies give it category 0.\n",
    "\n",
    "Query: {query}\n",
    "Passage: {passage}\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the search. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the passage is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> int:\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": RETRIEVAL_EVAL_PROMPT.format(query=query, passage=passage)}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_retriever_evaluation_using_llm(eval_samples):\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 0, 1, 0] 0.5\n",
      "[3, 0, 0, 0, 0] 1.0\n",
      "[2, 1, 3, 3, 2] 0.3333333333333333\n",
      "[3, 1, 0, 1, 0] 1.0\n",
      "[3, 1, 2, 0, 1] 1.0\n",
      "[3, 1, 1, 1, 2] 1.0\n",
      "[1, 3, 2, 2, 1] 0.5\n",
      "[3, 0, 1, 0, 1] 1.0\n",
      "[3, 3, 2, 2, 2] 1.0\n",
      "[3, 0, 1, 0, 0] 1.0\n",
      "[3, 0, 0, 2, 0] 1.0\n",
      "[0, 3, 3, 0, 0] 0.5\n",
      "[3, 3, 1, 3, 0] 1.0\n",
      "[3, 3, 3, 1, 3] 1.0\n",
      "[0, 3, 1, 2, 1] 0.5\n",
      "[3, 0, 2, 0, 0] 1.0\n",
      "[0, 0, 3, 2, 0] 0.3333333333333333\n",
      "[3, 3, 1, 1, 1] 1.0\n",
      "[0, 3, 2, 1, 0] 0.5\n",
      "[1, 1, 3, 1, 0] 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for item in llm_judge_retrieval_results:\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(item['scores'], 1):\n",
    "        if result == 3:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    print(item['scores'], mrr_score)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets reload the Response Generator and the RAGPipeline from the previous chapter\n",
    "class ResponseGenerator:\n",
    "    def __init__(self, model: str, prompt: str):\n",
    "        self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        self.model = model\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def generate_context(self, context: List[Dict[str, any]]) -> str:\n",
    "        return \"\\n\".join(\n",
    "            [f\"Source: {item['source']}\\nText: {item['text']}\\n\\n\" for item in context]\n",
    "        )\n",
    "\n",
    "    # @weave.op()\n",
    "\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        context_text = self.generate_context(context)\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": self.prompt.format(context=context_text),\n",
    "        }\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nAnswer:\"}\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=[system_message, user_message]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "PROMPT = (\n",
    "    \"You are a helpful customer support assistant that can answer questions about W&B\\n\\n\"\n",
    "    \"Your answers must be based only on the provided context.\\n\\n\"\n",
    "    \"<context>\\n{context}\\n</context>\"\n",
    ")\n",
    "response_generator = ResponseGenerator(model=\"gpt-3.5-turbo\", prompt=PROMPT)\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever: Retriever, response_generator: ResponseGenerator, top_k: int = 5):\n",
    "        self.retriever = retriever\n",
    "        self.response_generator = response_generator\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def __call__(self, query: str) -> str:\n",
    "        context = self.retriever.search(query, self.top_k)\n",
    "        return self.response_generator.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "def calculate_diff_score(candidate, reference):\n",
    "    return difflib.SequenceMatcher(None, candidate, reference).ratio()\n",
    "\n",
    "\n",
    "def calculate_levenshtein_score(candidate, reference):\n",
    "    return Levenshtein.ratio(candidate, reference)\n",
    "\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "\n",
    "\n",
    "def calculate_similarity(candidate, reference):\n",
    "    vectors = retriever.vectorizer.transform([candidate, reference])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    meteor_score = meteor([candidate], reference)\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:41<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RAGPipeline(retriever, response_generator)\n",
    "\n",
    "response_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample['question']\n",
    "    actual_answer = rag_pipeline(query)\n",
    "    expected_answer = sample['answer']\n",
    "    diff_score = calculate_diff_score(actual_answer, expected_answer)\n",
    "    levenshtein_score = calculate_levenshtein_score(actual_answer, expected_answer)\n",
    "    rouge_score = calculate_rouge(actual_answer, expected_answer)\n",
    "    bleu_score = calculate_bleu(actual_answer, expected_answer)\n",
    "    meteor_score = calculate_meteor(actual_answer, expected_answer)\n",
    "    similarity_score = calculate_similarity(actual_answer, expected_answer)\n",
    "\n",
    "    response_scores.append({\n",
    "        \"query\": query,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"diff_score\": diff_score,\n",
    "        \"levenshtein_score\": levenshtein_score,\n",
    "        \"rouge_score\": rouge_score,\n",
    "        \"bleu_score\": bleu_score,\n",
    "        \"meteor_score\": meteor_score,\n",
    "        \"similarity_score\": similarity_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>diff_score</th>\n",
       "      <th>levenshtein_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>The summary attribute in W&amp;B is the value that...</td>\n",
       "      <td>0.245971</td>\n",
       "      <td>0.736217</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.336888</td>\n",
       "      <td>0.646982</td>\n",
       "      <td>0.821012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>To switch between accounts on the same machine...</td>\n",
       "      <td>0.365775</td>\n",
       "      <td>0.579679</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.437705</td>\n",
       "      <td>0.684834</td>\n",
       "      <td>0.718002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>Weights &amp; Biases (W&amp;B) offers several enhancem...</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.511105</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>0.084789</td>\n",
       "      <td>0.510090</td>\n",
       "      <td>0.448065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.780324</td>\n",
       "      <td>0.858583</td>\n",
       "      <td>0.956628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>By default, you can log metrics without any co...</td>\n",
       "      <td>0.072323</td>\n",
       "      <td>0.336579</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.013382</td>\n",
       "      <td>0.297883</td>\n",
       "      <td>0.186024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>You can log a metric that doesn't change over ...</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.734615</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.499397</td>\n",
       "      <td>0.648995</td>\n",
       "      <td>0.870270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>We recommend creating roughly a maximum of 10,...</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.341966</td>\n",
       "      <td>0.649664</td>\n",
       "      <td>0.665426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>Yes, you can run W&amp;B offline by using the foll...</td>\n",
       "      <td>0.048656</td>\n",
       "      <td>0.332907</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.022130</td>\n",
       "      <td>0.296134</td>\n",
       "      <td>0.323211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>If you encounter SSL or network errors like `w...</td>\n",
       "      <td>0.595469</td>\n",
       "      <td>0.744337</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.389193</td>\n",
       "      <td>0.688742</td>\n",
       "      <td>0.882851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>If your internet connection is lost while you'...</td>\n",
       "      <td>0.351145</td>\n",
       "      <td>0.806979</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.567873</td>\n",
       "      <td>0.723410</td>\n",
       "      <td>0.807970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, you can...</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.725227</td>\n",
       "      <td>0.844435</td>\n",
       "      <td>0.943022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>To create a W&amp;B Experiment, you can follow the...</td>\n",
       "      <td>0.051787</td>\n",
       "      <td>0.216135</td>\n",
       "      <td>0.269341</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.335047</td>\n",
       "      <td>0.486992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>To log a table to a run using W&amp;B, you can fol...</td>\n",
       "      <td>0.172926</td>\n",
       "      <td>0.455895</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.171196</td>\n",
       "      <td>0.459150</td>\n",
       "      <td>0.584383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>To log a list of values, you can use a diction...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.412906</td>\n",
       "      <td>0.547554</td>\n",
       "      <td>0.664085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.802390</td>\n",
       "      <td>0.954196</td>\n",
       "      <td>0.964731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>Yes, you can flag boolean variables as hyperpa...</td>\n",
       "      <td>0.558528</td>\n",
       "      <td>0.638796</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.427738</td>\n",
       "      <td>0.694252</td>\n",
       "      <td>0.650006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>You can programmatically access the human-read...</td>\n",
       "      <td>0.682081</td>\n",
       "      <td>0.687861</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.525282</td>\n",
       "      <td>0.590962</td>\n",
       "      <td>0.667153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>When you call `wandb.init` in your script, the...</td>\n",
       "      <td>0.281054</td>\n",
       "      <td>0.501882</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.132382</td>\n",
       "      <td>0.450760</td>\n",
       "      <td>0.561342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `\\/` as a separator for organizing lo...</td>\n",
       "      <td>You can use `/` as a separator to organize you...</td>\n",
       "      <td>0.388290</td>\n",
       "      <td>0.573190</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.732269</td>\n",
       "      <td>0.637288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                      expected_answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `\\/` as a separator for organizing lo...   \n",
       "\n",
       "                                        actual_answer  diff_score  \\\n",
       "0   The summary attribute in W&B is the value that...    0.245971   \n",
       "1   To switch between accounts on the same machine...    0.365775   \n",
       "2   Weights & Biases (W&B) offers several enhancem...    0.191886   \n",
       "3   A team is a collaborative workspace for a grou...    1.000000   \n",
       "4   A team is a collaborative workspace for a grou...    0.703704   \n",
       "5   By default, you can log metrics without any co...    0.072323   \n",
       "6   You can log a metric that doesn't change over ...    0.723077   \n",
       "7   We recommend creating roughly a maximum of 10,...    0.788235   \n",
       "8   Yes, you can run W&B offline by using the foll...    0.048656   \n",
       "9   If you encounter SSL or network errors like `w...    0.595469   \n",
       "10  If your internet connection is lost while you'...    0.351145   \n",
       "11  Once you've signed in to www.wandb.ai, you can...    0.877193   \n",
       "12  To create a W&B Experiment, you can follow the...    0.051787   \n",
       "13  To log a table to a run using W&B, you can fol...    0.172926   \n",
       "14  To log a list of values, you can use a diction...    0.250000   \n",
       "15  You cannot change the Sweep configuration once...    0.879167   \n",
       "16  Yes, you can flag boolean variables as hyperpa...    0.558528   \n",
       "17  You can programmatically access the human-read...    0.682081   \n",
       "18  When you call `wandb.init` in your script, the...    0.281054   \n",
       "19  You can use `/` as a separator to organize you...    0.388290   \n",
       "\n",
       "    levenshtein_score  rouge_score  bleu_score  meteor_score  similarity_score  \n",
       "0            0.736217     0.634921    0.336888      0.646982          0.821012  \n",
       "1            0.579679     0.595041    0.437705      0.684834          0.718002  \n",
       "2            0.511105     0.312883    0.084789      0.510090          0.448065  \n",
       "3            1.000000     1.000000    1.000000      0.999992          1.000000  \n",
       "4            0.922559     0.911392    0.780324      0.858583          0.956628  \n",
       "5            0.336579     0.247423    0.013382      0.297883          0.186024  \n",
       "6            0.734615     0.562500    0.499397      0.648995          0.870270  \n",
       "7            0.788235     0.666667    0.341966      0.649664          0.665426  \n",
       "8            0.332907     0.207547    0.022130      0.296134          0.323211  \n",
       "9            0.744337     0.579545    0.389193      0.688742          0.882851  \n",
       "10           0.806979     0.713043    0.567873      0.723410          0.807970  \n",
       "11           0.877193     0.833333    0.725227      0.844435          0.943022  \n",
       "12           0.216135     0.269341    0.001167      0.335047          0.486992  \n",
       "13           0.455895     0.285714    0.171196      0.459150          0.584383  \n",
       "14           0.581579     0.481928    0.412906      0.547554          0.664085  \n",
       "15           0.900000     0.895522    0.802390      0.954196          0.964731  \n",
       "16           0.638796     0.558824    0.427738      0.694252          0.650006  \n",
       "17           0.687861     0.666667    0.525282      0.590962          0.667153  \n",
       "18           0.501882     0.428571    0.132382      0.450760          0.561342  \n",
       "19           0.573190     0.568182    0.224300      0.732269          0.637288  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Overall Generation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_score</th>\n",
       "      <th>levenshtein_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.461363</td>\n",
       "      <td>0.646287</td>\n",
       "      <td>0.570952</td>\n",
       "      <td>0.394812</td>\n",
       "      <td>0.630697</td>\n",
       "      <td>0.691923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diff_score  levenshtein_score  rouge_score  bleu_score  meteor_score  \\\n",
       "0    0.461363           0.646287     0.570952    0.394812      0.630697   \n",
       "\n",
       "   similarity_score  \n",
       "0          0.691923  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Generation Score Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.461363</td>\n",
       "      <td>0.302904</td>\n",
       "      <td>0.048656</td>\n",
       "      <td>0.232450</td>\n",
       "      <td>0.377033</td>\n",
       "      <td>0.708547</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>levenshtein_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.646287</td>\n",
       "      <td>0.212861</td>\n",
       "      <td>0.216135</td>\n",
       "      <td>0.508799</td>\n",
       "      <td>0.663329</td>\n",
       "      <td>0.792921</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.570952</td>\n",
       "      <td>0.231871</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.399649</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.394812</td>\n",
       "      <td>0.284781</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.161492</td>\n",
       "      <td>0.401050</td>\n",
       "      <td>0.535930</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteor_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.630697</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.296134</td>\n",
       "      <td>0.497355</td>\n",
       "      <td>0.649330</td>\n",
       "      <td>0.725625</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similarity_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.691923</td>\n",
       "      <td>0.221279</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>0.578623</td>\n",
       "      <td>0.666289</td>\n",
       "      <td>0.873415</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count      mean       std       min       25%       50%  \\\n",
       "diff_score          20.0  0.461363  0.302904  0.048656  0.232450  0.377033   \n",
       "levenshtein_score   20.0  0.646287  0.212861  0.216135  0.508799  0.663329   \n",
       "rouge_score         20.0  0.570952  0.231871  0.207547  0.399649  0.573864   \n",
       "bleu_score          20.0  0.394812  0.284781  0.001167  0.161492  0.401050   \n",
       "meteor_score        20.0  0.630697  0.200500  0.296134  0.497355  0.649330   \n",
       "similarity_score    20.0  0.691923  0.221279  0.186024  0.578623  0.666289   \n",
       "\n",
       "                        75%       max  \n",
       "diff_score         0.708547  1.000000  \n",
       "levenshtein_score  0.792921  1.000000  \n",
       "rouge_score        0.678261  1.000000  \n",
       "bleu_score         0.535930  1.000000  \n",
       "meteor_score       0.725625  0.999992  \n",
       "similarity_score   0.873415  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "response_scores_df = pd.DataFrame(response_scores)\n",
    "display(response_scores_df)\n",
    "\n",
    "GENERATION_METRICS = [col for col in response_scores_df.columns if \"score\" in col]\n",
    "\n",
    "\n",
    "print(\"\\nMean Overall Generation Scores:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Generation Score Statistics:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "Given a query, an expected answer and a generated answer, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
    "    0 = represent that the generated answer is incorrect,\n",
    "    1 = represents that the generated answer is partially correct, and contains mistakes\n",
    "    2 = represents that the generated answer is correct, but the answer may be a bit unclear or incomplete when compared to the expected answer\n",
    "    3 = represents that the generated answer is correct and completely answers the query and does not contain any mistakes and is factually consistent with the expected answer.\n",
    "\n",
    "Important Instruction: Assign category 1 if the generated answer is only partially correct and contains mistakes, category 2 if the generated answer is correct but contains some extra information or is incomplete when compared to the expected answer, category 3 if the generated answer is correct and completely answers the query and does not contain any mistakes and is factually consistent with the expected answer. If none of the above satisfies give it category 0.\n",
    "\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "<Expected Answer>\n",
    "{expected_answer}\n",
    "</Expected Answer>\n",
    "<Generated Answer>\n",
    "{generated_answer}\n",
    "</Generated Answer>\n",
    "\n",
    "\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the generated answer matches the expected answer(M).\n",
    "Measure how trustworthy the generated answer is (T) when compared to the expected answer.\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "async def evaluate_correctness_using_llm_judge(query: str, expected_answer: str, generated_answer: str) -> int:\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": CORRECTNESS_EVAL_PROMPT.format(query=query, expected_answer=expected_answer, generated_answer=generated_answer)}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_correctness_evaluation_using_llm(response_scores):\n",
    "    tasks = []\n",
    "    for row in response_scores:\n",
    "        query = row[\"query\"]\n",
    "        expected_answer = row[\"expected_answer\"]\n",
    "        generated_answer = row[\"actual_answer\"]\n",
    "        tasks.append(evaluate_correctness_using_llm_judge(query, expected_answer, generated_answer))\n",
    "    scores = await asyncio.gather(*tasks)\n",
    "    scores = list(map(json.loads, scores))\n",
    "    scores = list(map(lambda x: x[\"final_score\"], scores))\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_correctness_results = asyncio.run(run_correctness_evaluation_using_llm(response_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>diff_score</th>\n",
       "      <th>levenshtein_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>correctness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>The summary attribute in W&amp;B is the value that...</td>\n",
       "      <td>0.245971</td>\n",
       "      <td>0.736217</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>0.336888</td>\n",
       "      <td>0.646982</td>\n",
       "      <td>0.821012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>To switch between accounts on the same machine...</td>\n",
       "      <td>0.365775</td>\n",
       "      <td>0.579679</td>\n",
       "      <td>0.595041</td>\n",
       "      <td>0.437705</td>\n",
       "      <td>0.684834</td>\n",
       "      <td>0.718002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>Weights &amp; Biases (W&amp;B) offers several enhancem...</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.511105</td>\n",
       "      <td>0.312883</td>\n",
       "      <td>0.084789</td>\n",
       "      <td>0.510090</td>\n",
       "      <td>0.448065</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.780324</td>\n",
       "      <td>0.858583</td>\n",
       "      <td>0.956628</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>By default, you can log metrics without any co...</td>\n",
       "      <td>0.072323</td>\n",
       "      <td>0.336579</td>\n",
       "      <td>0.247423</td>\n",
       "      <td>0.013382</td>\n",
       "      <td>0.297883</td>\n",
       "      <td>0.186024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>You can log a metric that doesn't change over ...</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.734615</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.499397</td>\n",
       "      <td>0.648995</td>\n",
       "      <td>0.870270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>We recommend creating roughly a maximum of 10,...</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.341966</td>\n",
       "      <td>0.649664</td>\n",
       "      <td>0.665426</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>Yes, you can run W&amp;B offline by using the foll...</td>\n",
       "      <td>0.048656</td>\n",
       "      <td>0.332907</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.022130</td>\n",
       "      <td>0.296134</td>\n",
       "      <td>0.323211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>If you encounter SSL or network errors like `w...</td>\n",
       "      <td>0.595469</td>\n",
       "      <td>0.744337</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.389193</td>\n",
       "      <td>0.688742</td>\n",
       "      <td>0.882851</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>If your internet connection is lost while you'...</td>\n",
       "      <td>0.351145</td>\n",
       "      <td>0.806979</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.567873</td>\n",
       "      <td>0.723410</td>\n",
       "      <td>0.807970</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, you can...</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.725227</td>\n",
       "      <td>0.844435</td>\n",
       "      <td>0.943022</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>To create a W&amp;B Experiment, you can follow the...</td>\n",
       "      <td>0.051787</td>\n",
       "      <td>0.216135</td>\n",
       "      <td>0.269341</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.335047</td>\n",
       "      <td>0.486992</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>To log a table to a run using W&amp;B, you can fol...</td>\n",
       "      <td>0.172926</td>\n",
       "      <td>0.455895</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.171196</td>\n",
       "      <td>0.459150</td>\n",
       "      <td>0.584383</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>To log a list of values, you can use a diction...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.412906</td>\n",
       "      <td>0.547554</td>\n",
       "      <td>0.664085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>0.879167</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.802390</td>\n",
       "      <td>0.954196</td>\n",
       "      <td>0.964731</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>Yes, you can flag boolean variables as hyperpa...</td>\n",
       "      <td>0.558528</td>\n",
       "      <td>0.638796</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.427738</td>\n",
       "      <td>0.694252</td>\n",
       "      <td>0.650006</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>You can programmatically access the human-read...</td>\n",
       "      <td>0.682081</td>\n",
       "      <td>0.687861</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.525282</td>\n",
       "      <td>0.590962</td>\n",
       "      <td>0.667153</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>When you call `wandb.init` in your script, the...</td>\n",
       "      <td>0.281054</td>\n",
       "      <td>0.501882</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.132382</td>\n",
       "      <td>0.450760</td>\n",
       "      <td>0.561342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `\\/` as a separator for organizing lo...</td>\n",
       "      <td>You can use `/` as a separator to organize you...</td>\n",
       "      <td>0.388290</td>\n",
       "      <td>0.573190</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.732269</td>\n",
       "      <td>0.637288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                      expected_answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `\\/` as a separator for organizing lo...   \n",
       "\n",
       "                                        actual_answer  diff_score  \\\n",
       "0   The summary attribute in W&B is the value that...    0.245971   \n",
       "1   To switch between accounts on the same machine...    0.365775   \n",
       "2   Weights & Biases (W&B) offers several enhancem...    0.191886   \n",
       "3   A team is a collaborative workspace for a grou...    1.000000   \n",
       "4   A team is a collaborative workspace for a grou...    0.703704   \n",
       "5   By default, you can log metrics without any co...    0.072323   \n",
       "6   You can log a metric that doesn't change over ...    0.723077   \n",
       "7   We recommend creating roughly a maximum of 10,...    0.788235   \n",
       "8   Yes, you can run W&B offline by using the foll...    0.048656   \n",
       "9   If you encounter SSL or network errors like `w...    0.595469   \n",
       "10  If your internet connection is lost while you'...    0.351145   \n",
       "11  Once you've signed in to www.wandb.ai, you can...    0.877193   \n",
       "12  To create a W&B Experiment, you can follow the...    0.051787   \n",
       "13  To log a table to a run using W&B, you can fol...    0.172926   \n",
       "14  To log a list of values, you can use a diction...    0.250000   \n",
       "15  You cannot change the Sweep configuration once...    0.879167   \n",
       "16  Yes, you can flag boolean variables as hyperpa...    0.558528   \n",
       "17  You can programmatically access the human-read...    0.682081   \n",
       "18  When you call `wandb.init` in your script, the...    0.281054   \n",
       "19  You can use `/` as a separator to organize you...    0.388290   \n",
       "\n",
       "    levenshtein_score  rouge_score  bleu_score  meteor_score  \\\n",
       "0            0.736217     0.634921    0.336888      0.646982   \n",
       "1            0.579679     0.595041    0.437705      0.684834   \n",
       "2            0.511105     0.312883    0.084789      0.510090   \n",
       "3            1.000000     1.000000    1.000000      0.999992   \n",
       "4            0.922559     0.911392    0.780324      0.858583   \n",
       "5            0.336579     0.247423    0.013382      0.297883   \n",
       "6            0.734615     0.562500    0.499397      0.648995   \n",
       "7            0.788235     0.666667    0.341966      0.649664   \n",
       "8            0.332907     0.207547    0.022130      0.296134   \n",
       "9            0.744337     0.579545    0.389193      0.688742   \n",
       "10           0.806979     0.713043    0.567873      0.723410   \n",
       "11           0.877193     0.833333    0.725227      0.844435   \n",
       "12           0.216135     0.269341    0.001167      0.335047   \n",
       "13           0.455895     0.285714    0.171196      0.459150   \n",
       "14           0.581579     0.481928    0.412906      0.547554   \n",
       "15           0.900000     0.895522    0.802390      0.954196   \n",
       "16           0.638796     0.558824    0.427738      0.694252   \n",
       "17           0.687861     0.666667    0.525282      0.590962   \n",
       "18           0.501882     0.428571    0.132382      0.450760   \n",
       "19           0.573190     0.568182    0.224300      0.732269   \n",
       "\n",
       "    similarity_score  correctness_score  \n",
       "0           0.821012                  3  \n",
       "1           0.718002                  2  \n",
       "2           0.448065                  3  \n",
       "3           1.000000                  3  \n",
       "4           0.956628                  3  \n",
       "5           0.186024                  0  \n",
       "6           0.870270                  1  \n",
       "7           0.665426                  3  \n",
       "8           0.323211                  1  \n",
       "9           0.882851                  3  \n",
       "10          0.807970                  3  \n",
       "11          0.943022                  3  \n",
       "12          0.486992                  2  \n",
       "13          0.584383                  2  \n",
       "14          0.664085                  1  \n",
       "15          0.964731                  3  \n",
       "16          0.650006                  3  \n",
       "17          0.667153                  3  \n",
       "18          0.561342                  2  \n",
       "19          0.637288                  1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_scores_df[\"correctness_score\"] = llm_judge_correctness_results\n",
    "display(response_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can I just log metrics, no code or dataset examples?',\n",
       " 'expected_answer': \"**Dataset Examples**\\n\\nBy default, we don't log any of your dataset examples. You can explicitly turn this feature on to see example predictions in our web interface.\\n\\n**Code Logging**\\n\\nThere are two ways to turn off code logging:\\n\\n1. Set `WANDB_DISABLE_CODE` to `true` to turn off all code tracking. We won't pick up the git SHA or the diff patch.\\n2. Set `WANDB_IGNORE_GLOBS` to `*.patch` to turn off syncing the diff patch to our servers. You'll still have it locally and be able to apply it with the `wandb restore` command.\\n\",\n",
       " 'actual_answer': 'By default, you can log metrics without any code or dataset examples. Simply use `wandb.log()` to log your metrics, such as accuracy or loss values, without including code or dataset examples.',\n",
       " 'diff_score': 0.07232267037552156,\n",
       " 'levenshtein_score': 0.3365785813630042,\n",
       " 'rouge_score': 0.24742267658624723,\n",
       " 'bleu_score': 0.013382192107865966,\n",
       " 'meteor_score': 0.29788312840664216,\n",
       " 'similarity_score': 0.18602423185305558,\n",
       " 'correctness_score': 0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_scores_df.iloc[5].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
