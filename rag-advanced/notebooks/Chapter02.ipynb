{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP # Chapter 2:\n",
    "\n",
    "**Comprehensive Evaluation Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and improving an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # TODO: Remove this once we more to the final project\n",
    "# eval_artifact = wandb.Artifact(\n",
    "#     name=\"eval_dataset\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"Evaluation dataset for RAG\",\n",
    "#     metadata={\n",
    "#         \"total_samples\": 20,\n",
    "#         \"date_collected\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#         \"chapter\": \"Chapter 1\",\n",
    "#     },\n",
    "# )\n",
    "# eval_artifact.add_file(\"../data/eval/eval_dataset.jsonl\")\n",
    "# run.log_artifact(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/final_eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics\n",
    "\n",
    "**TODO** Add weave model and evals in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/retriever\n",
    "import weave\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class Retriever(weave.Model):\n",
    "    vectorizer: TfidfVectorizer = TfidfVectorizer()\n",
    "    index: list = None\n",
    "    data: list = None\n",
    "\n",
    "    @weave.op()\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    @weave.op()\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, k: int):\n",
    "        return self.search(query, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)\n",
    "retriever.predict(\"what is wandb\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/retrieval_metrics.py\n",
    "import weave\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_hit_rate(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate (precision) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The hit rate (precision).\n",
    "\n",
    "    The hit rate (precision) measures the proportion of retrieved documents that are relevant.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{Hit Rate (Precision)} = \\frac{\\text{Number of Relevant Documents Retrieved}}{\\text{Total Number of Documents Retrieved}} \\]\n",
    "\n",
    "    This metric is useful for assessing the accuracy of the retrieval system by determining the relevance of the retrieved documents.\n",
    "    ```\n",
    "    \"\"\"\n",
    "    search_results = [doc[\"source\"] for doc in model_output]\n",
    "    relevant_sources = [\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    ]\n",
    "\n",
    "    # Calculate the number of relevant documents retrieved\n",
    "    relevant_retrieved = sum(\n",
    "        1 for source in search_results if source in relevant_sources\n",
    "    )\n",
    "\n",
    "    # Calculate the hit rate (precision)\n",
    "    hit_rate = relevant_retrieved / len(search_results) if search_results else 0.0\n",
    "\n",
    "    return hit_rate\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def compute_mrr(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The MRR score for the given query.\n",
    "\n",
    "    MRR measures the rank of the first relevant document in the result list.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{MRR} = \\frac{1}{\\text{rank of first relevant document}} \\]\n",
    "\n",
    "    If no relevant document is found, MRR is 0.\n",
    "\n",
    "    This metric is useful for evaluating systems where there is typically one relevant document\n",
    "    and the user is interested in finding that document quickly.\n",
    "    \"\"\"\n",
    "    relevant_sources = [\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    ]\n",
    "\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(model_output, 1):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score\n",
    "\n",
    "\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "@weave.op\n",
    "def compute_ndcg(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Discounted Cumulative Gain (NDCG) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The cosine similarity score of the document to the query.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The NDCG score for the given query.\n",
    "\n",
    "    NDCG measures the ranking quality of the search results, taking into account the position of relevant documents.\n",
    "\n",
    "    NDCG Formula:\n",
    "    1. Calculate the Discounted Cumulative Gain (DCG):\n",
    "       \\[ \\text{DCG}_p = \\sum_{i=1}^p \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} \\]\n",
    "       where \\( rel_i \\) is the relevance score of the document at position \\( i \\).\n",
    "\n",
    "    2. Calculate the Ideal Discounted Cumulative Gain (IDCG), which is the DCG of the ideal ranking:\n",
    "       \\[ \\text{IDCG}_p = \\sum_{i=1}^p \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} \\]\n",
    "       where documents are sorted by their relevance scores in descending order.\n",
    "\n",
    "    3. Normalize the DCG by dividing it by the IDCG to get NDCG:\n",
    "       \\[ \\text{NDCG}_p = \\frac{\\text{DCG}_p}{\\text{IDCG}_p} \\]\n",
    "\n",
    "    This implementation uses continuous relevance scores.\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "\n",
    "    # Calculate DCG\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            dcg += (2 ** result[\"score\"] - 1) / np.log2(\n",
    "                i + 2\n",
    "            )  # i+2 because log2 starts at 1 for i=0\n",
    "\n",
    "    # Sort the results by score to calculate IDCG\n",
    "    sorted_model_output = sorted(model_output, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    # Calculate IDCG\n",
    "    for i, result in enumerate(sorted_model_output):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            idcg += (2 ** result[\"score\"] - 1) / np.log2(i + 2)\n",
    "\n",
    "    # To avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate nDCG\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "# MAP (Mean Average Precision)\n",
    "@weave.op()\n",
    "def compute_map(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision (MAP) for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The MAP score for the given query.\n",
    "\n",
    "    MAP provides a single-figure measure of quality across recall levels.\n",
    "    For a single query, it's equivalent to the Average Precision (AP).\n",
    "    It's calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{MAP} = \\frac{\\sum_{k=1}^n P(k) \\times \\text{rel}(k)}{\\text{number of relevant documents}} \\]\n",
    "\n",
    "    Where:\n",
    "    - n is the number of retrieved documents\n",
    "    - P(k) is the precision at cut-off k in the list\n",
    "    - rel(k) is an indicator function: 1 if the item at rank k is relevant, 0 otherwise\n",
    "    MAP considers both precision and recall, as well as the ranking of relevant documents.\n",
    "\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "\n",
    "    num_relevant = 0\n",
    "    sum_precision = 0.0\n",
    "\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result[\"source\"] in relevant_sources:\n",
    "            num_relevant += 1\n",
    "            sum_precision += num_relevant / (i + 1)\n",
    "\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_precision = sum_precision / len(relevant_sources)\n",
    "    return average_precision\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_precision(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Precision for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The Precision score for the given query.\n",
    "\n",
    "    Precision measures the proportion of retrieved documents that are relevant.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{Precision} = \\frac{\\text{Number of Relevant Documents Retrieved}}{\\text{Total Number of Documents Retrieved}} \\]\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "    retrieved_sources = {result[\"source\"] for result in model_output}\n",
    "\n",
    "    relevant_retrieved = relevant_sources & retrieved_sources\n",
    "\n",
    "    precision = (\n",
    "        len(relevant_retrieved) / len(retrieved_sources) if retrieved_sources else 0.0\n",
    "    )\n",
    "    return precision\n",
    "\n",
    "\n",
    "# Recall\n",
    "@weave.op()\n",
    "def compute_recall(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Recall for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The Recall score for the given query.\n",
    "\n",
    "    Recall measures the proportion of relevant documents that are retrieved.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{Recall} = \\frac{\\text{Number of Relevant Documents Retrieved}}{\\text{Total Number of Relevant Documents}} \\]\n",
    "    \"\"\"\n",
    "    relevant_sources = {\n",
    "        context[\"source\"] for context in contexts if context[\"relevance\"] != 0\n",
    "    }\n",
    "    retrieved_sources = {result[\"source\"] for result in model_output}\n",
    "\n",
    "    relevant_retrieved = relevant_sources & retrieved_sources\n",
    "\n",
    "    recall = (\n",
    "        len(relevant_retrieved) / len(relevant_sources) if relevant_sources else 0.0\n",
    "    )\n",
    "    return recall\n",
    "\n",
    "\n",
    "# F1 Score\n",
    "@weave.op()\n",
    "def compute_f1_score(\n",
    "    model_output: List[Dict[str, Any]], contexts: List[Dict[str, Any]]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the F1-Score for a single query.\n",
    "\n",
    "    Args:\n",
    "        model_output (List[Dict[str, Any]]): The list of retrieved documents from the model.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the document.\n",
    "                - 'score': The relevance score of the document.\n",
    "        contexts (List[Dict[str, Any]]): A list of dictionaries representing the relevant contexts.\n",
    "            Each dictionary contains:\n",
    "                - 'source': A unique identifier for the relevant document.\n",
    "\n",
    "    Returns:\n",
    "        float: The F1-Score for the given query.\n",
    "\n",
    "    F1-Score is the harmonic mean of Precision and Recall.\n",
    "    It is calculated using the following formula:\n",
    "\n",
    "    \\[ \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "    \"\"\"\n",
    "    precision = compute_precision(model_output, contexts)\n",
    "    recall = compute_recall(model_output, contexts)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's how we can evaluate the retrieval system in python\n",
    "\n",
    "# hit_rates = []\n",
    "# for sample in eval_samples:\n",
    "#     query = sample[\"question\"]\n",
    "#     expected_source = sample[\"source\"]\n",
    "#     search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "#     hit_rate = compute_hit_rate(search_results, expected_source)\n",
    "#     hit_rates.append({\"query\": query, \"hit_rate\": hit_rate})\n",
    "\n",
    "# hit_rate_df = pd.DataFrame(hit_rates)\n",
    "# display(hit_rate_df)\n",
    "\n",
    "\n",
    "# # we need a single number to rate the retrieval system\n",
    "# # the mean hit rate is a good metric to evaluate the retrieval system as a whole\n",
    "\n",
    "# print(f\"Mean Hit Rate: {hit_rate_df['hit_rate'].mean():.4f}\")\n",
    "# print(f\"Std-dev Hit Rate: {hit_rate_df['hit_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_scorers = [compute_mrr, compute_ndcg, compute_map, compute_hit_rate, compute_precision, compute_recall, compute_f1_score]\n",
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=retrieval_scorers,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? \n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prompts/retrieval_eval.py\n",
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 2 with the following meanings:\n",
    "    0 = represents that the excerpt is irrelevant to the query,\n",
    "    1 = represents that the excerpt is somewhat relevant to the query,\n",
    "    2 = represents that the excerpt is is highly relevant to the query.\n",
    "    \n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the query but not completely, category 2 if the excerpt only and entirely refers to the query. If neither of these criteria satisfies the query, give it category 0.\n",
    "\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\n",
    "## Examples\n",
    "\n",
    "Example 1: \n",
    "<Query>\n",
    "How do I programmatically access the human-readable run name?\n",
    "</Query>\n",
    "<Document>\n",
    "If you do not explicitly name your run, a random run name will be assigned to the run to help identify the run in the UI. For instance, random run names will look like \"pleasant-flower-4\" or \"misunderstood-glade-2\".\n",
    "\n",
    "If you'd like to overwrite the run name (like snowy-owl-10) with the run ID (like qvlp96vk) you can use this snippet:\n",
    "\n",
    "import wandbRetrieval_Evaluation\n",
    "\n",
    "wandb.init()\n",
    "wandb.run.name = wandb.run.id\n",
    "wandb.run.save()\n",
    "\n",
    "</Document>\n",
    "{{\"final_score\": 0}}\n",
    "\n",
    "Example 2:\n",
    "<Query>\n",
    "What are Runs?\n",
    "</Query>\n",
    "<Document>\n",
    "A single unit of computation logged by W&B is called a run. You can think of a W&B run as an atomic element of your whole project. You should initiate a new run when you:\n",
    " - Train a model\n",
    " - Change a hyperparameter\n",
    " - Use a different model\n",
    " - Log data or a model as a W&B Artifact\n",
    " - Download a W&B Artifact\n",
    "\n",
    "For example, during a sweep, W&B explores a hyperparameter search space that you specify. Each new hyperparameter combination created by the sweep is implemented and recorded as a unique run. \n",
    "</Document>\n",
    "{{\"final_score\": 2}}\n",
    "\n",
    "Example 3:\n",
    "<Query>\n",
    "How do I use W&B with Keras ?\n",
    "</Query>\n",
    "<Document>\n",
    "We have added three new callbacks for Keras and TensorFlow users, available from wandb v0.13.4. For the legacy WandbCallback scroll down.\n",
    "These new callbacks,\n",
    " - Adhere to Keras design philosophy\n",
    " - Reduce the cognitive load of using a single callback (WandbCallback) for everything\n",
    " - Make it easy for Keras users to modify the callback by subclassing it to support their niche use case\n",
    "</Document>\n",
    "{{\"final_score\": 1}}\n",
    "\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "\n",
    "<Document>\n",
    "{document}\n",
    "</Document>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/retrieval_judge.py\n",
    "\n",
    "import cohere\n",
    "import os\n",
    "import weave\n",
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str, eval_prompt: str) -> str:\n",
    "    response = await client.chat(\n",
    "        message=eval_prompt.format(query=query, document=passage),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "async def run_retriever_evaluation_using_llm(\n",
    "    eval_samples: List[Dict[str, Any]],\n",
    "    retriever: Any,\n",
    "    eval_prompt: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"], eval_prompt))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        parsed_scores = []\n",
    "        for score in sample_scores:\n",
    "            try:\n",
    "                score = json.loads(score)\n",
    "                parsed_scores.append(score[\"final_score\"])\n",
    "            except json.JSONDecodeError:\n",
    "                parsed_scores.append(None)\n",
    "        scores.append({\"query\": query, \"scores\": parsed_scores})\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples, retriever=retriever, eval_prompt=RETRIEVAL_EVAL_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 2 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 2:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/response_generator\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import cohere\n",
    "import weave\n",
    "\n",
    "class ResponseGenerator(weave.Model):\n",
    "    model: str\n",
    "    prompt: str\n",
    "    client: cohere.Client = None\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_context(self, context: List[Dict[str, any]]) -> str:\n",
    "        return [{\"source\": item['source'], \"text\": item['text']} for item in context]\n",
    "    \n",
    "    @weave.op()\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        contexts = self.generate_context(context)\n",
    "        response = self.client.chat(\n",
    "            preamble=self.prompt,\n",
    "            message=query,\n",
    "            model=self.model,\n",
    "            documents=contexts,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, context: List[Dict[str, any]]):\n",
    "        return self.generate_response(query, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prompts/initial_system\n",
    "INITIAL_PROMPT = \"\"\"\n",
    "Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=INITIAL_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/rag_pipeline\n",
    "import weave\n",
    "from typing import Optional, Union\n",
    "from scripts.retriever import Retriever\n",
    "from scripts.response_generator import ResponseGenerator\n",
    "\n",
    "\n",
    "class RAGPipeline(weave.Model):\n",
    "    retriever: Union[weave.Model, Retriever] = None\n",
    "    response_generator: Union[weave.Model,ResponseGenerator] = None\n",
    "    top_k: int = 5\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str):\n",
    "        context = self.retriever.predict(query, self.top_k)\n",
    "        return self.response_generator.predict(query, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RAGPipeline(retriever=retriever, response_generator=response_generator, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load scripts/response_eval\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate import meteor\n",
    "from nltk.corpus import wordnet as wn\n",
    "import weave\n",
    "import re\n",
    "import string\n",
    "\n",
    "wn.ensure_loaded()\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize the input text by lowercasing, removing punctuation, and extra whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized text.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on punctuation before removing it, ensuring numbers are not split\n",
    "    text = re.sub(r\"[^\\w\\s\\d]\", \" \", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_diff(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the similarity ratio between the normalized model output and the expected answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The output generated by the model.\n",
    "        answer (str): The expected answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity ratio between the normalized model output and the expected answer.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    return difflib.SequenceMatcher(None, norm_output, norm_answer).ratio()\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_levenshtein(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein ratio between the normalized model output and the answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The output generated by the model.\n",
    "        answer (str): The expected answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The Levenshtein ratio between the normalized model output and the answer.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    return Levenshtein.ratio(norm_output, norm_answer)\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_rouge(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the ROUGE-L F1 score between the normalized model output and the reference answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model's generated output.\n",
    "        answer (str): The reference answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The ROUGE-L F1 score.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(norm_output, norm_answer)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_bleu(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the BLEU score between the normalized model output and the reference answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The generated output from the model.\n",
    "        answer (str): The reference answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The BLEU score between the normalized model output and the reference answer.\n",
    "    \"\"\"\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    reference = word_tokenize(norm_answer)\n",
    "    candidate = word_tokenize(norm_output)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "@weave.op\n",
    "def compute_meteor(model_output: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the METEOR score between the normalized model output and the reference answer.\n",
    "\n",
    "    Args:\n",
    "        model_output (str): The model's generated output.\n",
    "        answer (str): The reference answer.\n",
    "\n",
    "    Returns:\n",
    "        float: The METEOR score rounded to 4 decimal places.\n",
    "    \"\"\"\n",
    "    norm_output = normalize_text(model_output)\n",
    "    norm_answer = normalize_text(answer)\n",
    "    reference = word_tokenize(norm_answer)\n",
    "    candidate = word_tokenize(norm_output)\n",
    "    meteor_score = round(meteor([candidate], reference), 4)\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [compute_diff, compute_levenshtein, compute_rouge, compute_bleu]\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(response_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load prompts/correctness_eval\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 2 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 3:\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it significantly misses the user's intent,\n",
    "    \"final_score\": 1,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 4:\n",
    "{{\n",
    "    \"reason\": \"The generated is not factually correct and includes assumptions about code methods completely different from the reference answer\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_correctness_using_llm_judge(question: str, answer: str, model_output: str) -> Dict[str, Any]:\n",
    "    response = await client.chat(\n",
    "        message=CORRECTNESS_EVAL_PROMPT.format(query=question, reference_answer=answer, generated_answer=model_output),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [evaluate_correctness_using_llm_judge]\n",
    "correctness_evaluations = weave.Evaluation(\n",
    "    name=\"Correctness_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(correctness_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` evaluators and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retreival Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evalations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evalations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
