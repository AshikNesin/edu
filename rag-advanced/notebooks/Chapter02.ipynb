{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP # Chapter 2:\n",
    "\n",
    "**Comprehensive Evaluation Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and improving an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # TODO: Remove this once we more to the final project\n",
    "# eval_artifact = wandb.Artifact(\n",
    "#     name=\"eval_dataset\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"Evaluation dataset for RAG\",\n",
    "#     metadata={\n",
    "#         \"total_samples\": 20,\n",
    "#         \"date_collected\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#         \"chapter\": \"Chapter 1\",\n",
    "#     },\n",
    "# )\n",
    "# eval_artifact.add_file(\"../data/eval/eval_dataset.jsonl\")\n",
    "# run.log_artifact(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics\n",
    "\n",
    "**TODO** Add weave model and evals in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "\n",
    "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the Retriever class from Chapter 1\n",
    "retriever = weave.ref(\"weave:///rag-course/dev/object/Retriever:OvLVBKNX0eRiaaGOBCavdnlxzNQiC6SCMtOGRRhi0uM\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorizer = TfidfVectorizer()\n",
    "retriever.index_data(chunked_data)\n",
    "retriever.predict(\"what is wandb\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def compute_hit_rate(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate for a single query.\n",
    "\n",
    "    :param retrieved_docs: List of retrieved documents\n",
    "    :param actual_doc: The single actual relevant document\n",
    "    :return: Hit rate (1 if the relevant document is retrieved, 0 otherwise)\n",
    "    \"\"\"\n",
    "    search_results = [doc['source'] for doc in model_output]\n",
    "    return 1 if source in search_results else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's how we can evaluate the retrieval system normally in python\n",
    "\n",
    "# hit_rates = []\n",
    "# for sample in eval_samples:\n",
    "#     query = sample[\"question\"]\n",
    "#     expected_source = sample[\"source\"]\n",
    "#     search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "#     hit_rate = compute_hit_rate(search_results, expected_source)\n",
    "#     hit_rates.append({\"query\": query, \"hit_rate\": hit_rate})\n",
    "\n",
    "# hit_rate_df = pd.DataFrame(hit_rates)\n",
    "# display(hit_rate_df)\n",
    "\n",
    "\n",
    "# # we need a single number to rate the retrieval system\n",
    "# # the mean hit rate is a good metric to evaluate the retrieval system as a whole\n",
    "\n",
    "# print(f\"Mean Hit Rate: {hit_rate_df['hit_rate'].mean():.4f}\")\n",
    "# print(f\"Std-dev Hit Rate: {hit_rate_df['hit_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same evaluatuion can be done in weave\n",
    "\n",
    "hit_rate_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Hit_Score\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=[compute_hit_rate],\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "hit_rate = asyncio.run(hit_rate_evaluation.evaluate(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank)\n",
    "@weave.op()\n",
    "def compute_mrr(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(model_output, 1):\n",
    "        if result['source'] == source:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score\n",
    "\n",
    "\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "@weave.op()\n",
    "def compute_ndcg(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    dcg = 0.0\n",
    "    idcg = 0.0\n",
    "\n",
    "    # Sort the results by score to calculate IDCG\n",
    "    sorted_model_output = sorted(model_output, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result['source'] == source:\n",
    "            # Calculate DCG\n",
    "            dcg += (2 ** result['score'] - 1) / np.log2(i + 2)  # i+2 because log2 starts at 1 for i=0\n",
    "\n",
    "    for i, result in enumerate(sorted_model_output):\n",
    "        if result['source'] == source:\n",
    "            # Calculate IDCG\n",
    "            idcg += (2 ** result['score'] - 1) / np.log2(i + 2)\n",
    "\n",
    "    # To avoid division by zero\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate nDCG\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "\n",
    "# MAP (Mean Average Precision)\n",
    "@weave.op()\n",
    "def compute_map(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    num_relevant = 0\n",
    "    sum_precision = 0.0\n",
    "\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result['source'] == source:\n",
    "            num_relevant += 1\n",
    "            sum_precision += num_relevant / (i + 1)\n",
    "\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    average_precision = sum_precision / num_relevant\n",
    "    return average_precision\n",
    "\n",
    "\n",
    "# Precision <- this is more discounted precision because we only have 1 reference \n",
    "@weave.op()\n",
    "def compute_rank_precision(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    total_score = 0.0\n",
    "    relevant_count = 0\n",
    "    for i, result in enumerate(model_output):\n",
    "        if result['source'] == source:\n",
    "            total_score += max(1 - 0.2 * i, 0)\n",
    "            relevant_count += 1\n",
    "    return total_score / relevant_count if relevant_count > 0 else 0.0\n",
    "\n",
    "# Recall\n",
    "@weave.op()\n",
    "def compute_recall(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    total_relevant_items = sum(1 for result in model_output if result['source'] == source)\n",
    "    retrieved_relevant_items = total_relevant_items\n",
    "    return retrieved_relevant_items / total_relevant_items if total_relevant_items > 0 else 0.0\n",
    "\n",
    "# F1 Score\n",
    "@weave.op()\n",
    "def compute_f1(model_output: List[Dict[str, Any]], source: str) -> float:\n",
    "    precision = compute_rank_precision(model_output, source)\n",
    "    recall = compute_recall(model_output, source)\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_scorers = [compute_mrr, compute_ndcg, compute_map, compute_hit_rate, compute_rank_precision, compute_recall, compute_f1]\n",
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_samples,\n",
    "    scorers=retrieval_scorers,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? \n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 2 with the following meanings:\n",
    "    0 = represents that the excerpt is irrelevant to the query,\n",
    "    1 = represents that the excerpt is somewhat relevant to the query,\n",
    "    2 = represents that the excerpt is is highly relevant to the query.\n",
    "    \n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the query but not completely, category 2 if the excerpt only and entirely refers to the query. If neither of these criteria satisfies the query, give it category 0.\n",
    "\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\n",
    "## Examples\n",
    "\n",
    "Example 1: \n",
    "<Query>\n",
    "How do I programmatically access the human-readable run name?\n",
    "</Query>\n",
    "<Document>\n",
    "If you do not explicitly name your run, a random run name will be assigned to the run to help identify the run in the UI. For instance, random run names will look like \"pleasant-flower-4\" or \"misunderstood-glade-2\".\n",
    "\n",
    "If you'd like to overwrite the run name (like snowy-owl-10) with the run ID (like qvlp96vk) you can use this snippet:\n",
    "\n",
    "import wandbRetrieval_Evaluation\n",
    "\n",
    "wandb.init()\n",
    "wandb.run.name = wandb.run.id\n",
    "wandb.run.save()\n",
    "\n",
    "</Document>\n",
    "{{\"final_score\": 0}}\n",
    "\n",
    "Example 2:\n",
    "<Query>\n",
    "What are Runs?\n",
    "</Query>\n",
    "<Document>\n",
    "A single unit of computation logged by W&B is called a run. You can think of a W&B run as an atomic element of your whole project. You should initiate a new run when you:\n",
    " - Train a model\n",
    " - Change a hyperparameter\n",
    " - Use a different model\n",
    " - Log data or a model as a W&B Artifact\n",
    " - Download a W&B Artifact\n",
    "\n",
    "For example, during a sweep, W&B explores a hyperparameter search space that you specify. Each new hyperparameter combination created by the sweep is implemented and recorded as a unique run. \n",
    "</Document>\n",
    "{{\"final_score\": 2}}\n",
    "\n",
    "Example 3:\n",
    "<Query>\n",
    "How do I use W&B with Keras ?\n",
    "</Query>\n",
    "<Document>\n",
    "We have added three new callbacks for Keras and TensorFlow users, available from wandb v0.13.4. For the legacy WandbCallback scroll down.\n",
    "These new callbacks,\n",
    " - Adhere to Keras design philosophy\n",
    " - Reduce the cognitive load of using a single callback (WandbCallback) for everything\n",
    " - Make it easy for Keras users to modify the callback by subclassing it to support their niche use case\n",
    "</Document>\n",
    "{{\"final_score\": 1}}\n",
    "\n",
    "<Query>\n",
    "{query}\n",
    "</Query>\n",
    "\n",
    "<Document>\n",
    "{document}\n",
    "</Document>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> str:\n",
    "    response = await client.chat(\n",
    "        message=RETRIEVAL_EVAL_PROMPT.format(query=query, document=passage),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=20,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def run_retriever_evaluation_using_llm(eval_samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 2 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 2:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_generator = weave.ref(\"weave:///rag-course/dev/object/ResponseGenerator:YQiRpgHDhvJcrZEVpXJbvy2PuJyh5W3gXuhX06zZiYQ\").get()\n",
    "query = \"What is w&b?\"\n",
    "context = retriever.search(query, 5)\n",
    "\n",
    "response_generator.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "response_generator.predict(query, context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = weave.ref(\"weave:///rag-course/dev/object/RAGPipeline:njpUKmvYezO3X9cJJ5BXAujh0XSTHcpRyDsYswHbPRs\").get()\n",
    "rag_pipeline.retriever = retriever\n",
    "rag_pipeline.response_generator = response_generator\n",
    "rag_pipeline.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.ensure_loaded()\n",
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "@weave.op()\n",
    "def compute_diff(model_output: str, answer: str) -> float:\n",
    "    return difflib.SequenceMatcher(None, model_output, answer).ratio()\n",
    "\n",
    "@weave.op()\n",
    "def compute_levenshtein(model_output: str, answer: str) -> float:\n",
    "    return Levenshtein.ratio(model_output, answer)\n",
    "\n",
    "\n",
    "\n",
    "# semantic answer similarity. (SAS) - https://arxiv.org/abs/2108.06130\n",
    "# Originally, one should use a transformer based cross-encoder to measure and classify this. \n",
    "# For example, use something from https://sbert.net/docs/cross_encoder/usage/usage.html\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "@weave.op()\n",
    "def compute_similarity(model_output: str, answer: str) -> float:\n",
    "    vectors = retriever.vectorizer.transform([model_output, answer])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "@weave.op()\n",
    "def compute_rouge(model_output: str, answer: str) -> float:\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(model_output, answer)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def compute_bleu(model_output: str, answer: str) -> float:\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(answer)\n",
    "    candidate = word_tokenize(model_output)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [compute_diff, compute_levenshtein, compute_similarity, compute_rouge, compute_bleu]\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(response_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 2 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 3:\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it significantly misses the user's intent,\n",
    "    \"final_score\": 1,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 4:\n",
    "{{\n",
    "    \"reason\": \"The generated is not factually correct and includes assumptions about code methods completely different from the reference answer\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "@weave.op()\n",
    "async def evaluate_correctness_using_llm_judge(question: str, answer: str, model_output: str) -> Dict[str, Any]:\n",
    "    response = await client.chat(\n",
    "        message=CORRECTNESS_EVAL_PROMPT.format(query=question, reference_answer=answer, generated_answer=model_output),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scorers = [evaluate_correctness_using_llm_judge]\n",
    "correctness_evaluations = weave.Evaluation(\n",
    "    name=\"Correctness_Evaluation\",\n",
    "    dataset=eval_samples, \n",
    "    scorers=response_scorers, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(correctness_evaluations.evaluate(rag_pipeline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` evaluators and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retreival Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evalations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evalations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
