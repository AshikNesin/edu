{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2:\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/rag-irl/rag-advanced/notebooks/Chapter02.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "**Comprehensive Evaluation Strategies**\n",
    "\n",
    "In this chapter, we will evaluate the two main components of a RAG pipeline - retriever and response generator.\n",
    "\n",
    "Evaluating the retriever can be considered component evaluation. Depending on your RAG pipeline, there can be a few components and for ensuring robustness of your system, it is recommended to come up with evaluation for each components. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "import wandb\n",
    "\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from scripts.utils import display_source\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will also use W&B Weave for our evaluation purposes. The `weave.Evaluation` class is a light weight class that can be used to evaluate the performance of a `weave.Model` on a `weave.Dataset`. We will go into more details.\n",
    "\n",
    "We will initialize a weave client which will track traces and evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data for evaluation\n",
    "\n",
    "We used our [FAQs](https://docs.wandb.ai/guides/technical-faq) section from the docs website to build our evaluation set. \n",
    "\n",
    "The evaluation samples are logged as [`weave.Dataset`](https://wandb.github.io/weave/guides/core-types/datasets/). `weave.Dataset` enable you to collect examples for evaluation and automatically track versions for accurate comparisons. \n",
    "\n",
    "Below we will download the latest version locally with a simple API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy eval dataset with 20 samples.\n",
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:Qj4IFICc2EbdXu5A5UuhkPiWgxM1GvJMIvXEyv1DYnM\"\n",
    ").get()\n",
    "\n",
    "print(\"Number of evaluation samples: \", len(eval_dataset.rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each sample is easy.\n",
    "\n",
    "We have the question, ground truth answer and ground truth contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(eval_dataset.rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Retriever\n",
    "\n",
    "The fundamental idea of evaluating a retriever is to check how well the retrieved content matches the expected contents. For evaluating a RAG pipeline end to end, we need query and ground truth answer pairs. The ground truth answer must be grounded on some \"ground\" truth chunks. This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "You might already have access to such evaluation dataset depending on the nature of your application or you can synthetically build one. To build one you can retrieve random documents/chunks and ask an LLM to generate query-answer pairs - the underlying documents/chunks will act as your ground truth chunk.\n",
    "\n",
    "In the sections below, we will look at different metrics that can be used to evaluate the retriever we built in the last chapter.\n",
    "\n",
    "First let us download the chunked data (from chapter 1) and index it using a simple TFIDF based retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_data = weave.ref('chunked_data:v0').get()\n",
    "chunked_data.rows[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the `TFIDFRetriever` which we created in the last chapter and index the chunked data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retriever import TFIDFRetriever\n",
    "\n",
    "display_source(TFIDFRetriever)\n",
    "\n",
    "retriever = TFIDFRetriever()\n",
    "retriever.index_data(chunked_data.rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to evaluate retriever\n",
    "\n",
    "We can evaluate a retriever using traditional ML metrics. We can also evaluate by using a powerful LLM (next section).\n",
    "\n",
    "Below we are importing both traditional metrics and LLM as a judge metric from the `scripts/retrieval_metrics.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retrieval_metrics import IR_METRICS, LLM_METRICS as RETRIEVAL_LLM_METRICS\n",
    "from scripts.utils import display_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first understand the basic traditional metrics we will be using. Each metric expects a `model_output` which is a list of retrieved chunks from the retriever and `contexts` which is a list of ground truth contexts.\n",
    "\n",
    "- `compute_hit_rate`: The Hit Rate metric measures the proportion of retrieved documents that are relevant. We compute the proportion of relevant documents retrieved out of the total documents retrieved.\n",
    "- `compute_mrr`: MRR or mean reciprocal ranking measure how quickly a retrieval system can return the first relevant document in response to a query. Reciprocal ranking is computed by taking `1/rank` where rank is the position of the retrieved chunk in the ground truth chunks. MRR is the mean of these reciprocal ranks.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scorer in IR_METRICS:\n",
    "    display_source(scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset.rows,\n",
    "    scorers=IR_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? \n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in RETRIEVAL_LLM_METRICS:\n",
    "    display_source(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"LLM_Judge_Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=RETRIEVAL_LLM_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"k\":5}\n",
    ")\n",
    "retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.rag_pipeline import SimpleRAGPipeline\n",
    "from scripts.response_generator import SimpleResponseGenerator\n",
    "\n",
    "INITIAL_PROMPT = open(\"prompts/initial_system.txt\", \"r\").read()\n",
    "response_generator = SimpleResponseGenerator(model=\"command-r\", prompt=INITIAL_PROMPT)\n",
    "rag_pipeline = SimpleRAGPipeline(retriever=retriever, response_generator=response_generator, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import NLP_METRICS, LLM_METRICS as RESPONSE_LLM_METRICS\n",
    "for scorer in NLP_METRICS:\n",
    "    display_source(scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=NLP_METRICS[:-1],\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(response_evaluations.evaluate(rag_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in RESPONSE_LLM_METRICS:\n",
    "    display_source(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_evaluations = weave.Evaluation(\n",
    "    name=\"Correctness_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=RESPONSE_LLM_METRICS, \n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]})\n",
    "response_scores = asyncio.run(correctness_evaluations.evaluate(rag_pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` evaluators and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retreival Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evalations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evalations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
