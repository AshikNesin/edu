{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP # Chapter 2:\n",
    "\n",
    "**Comprehensive Evaluation Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and improving an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # TODO: Remove this once we more to the final project\n",
    "# eval_artifact = wandb.Artifact(\n",
    "#     name=\"eval_dataset\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"Evaluation dataset for RAG\",\n",
    "#     metadata={\n",
    "#         \"total_samples\": 20,\n",
    "#         \"date_collected\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#         \"chapter\": \"Chapter 1\",\n",
    "#     },\n",
    "# )\n",
    "# eval_artifact.add_file(\"../data/eval/eval_dataset.jsonl\")\n",
    "# run.log_artifact(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparambharat\u001b[0m (\u001b[33mrag-course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/edu/rag-advanced/notebooks/wandb/run-20240703_111530-7u9l72rm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rag-course/dev/runs/7u9l72rm' target=\"_blank\">peachy-sunset-5</a></strong> to <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">https://wandb.ai/rag-course/dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rag-course/dev/runs/7u9l72rm' target=\"_blank\">https://wandb.ai/rag-course/dev/runs/7u9l72rm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/03 11:15:34 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/okxu0mvh/artifact/938190750/wandb_manifest.json?Expires=1719989134&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=lhO4D3O9Lvd%2BytjLLF8RwTgRaf5%2BlhFgxiNilPlSNGxaSN9Cpqa0p8kcmRLcqG9tZJh3mYqty%2BGo7IRG8qJzxNdCyjLUBf9OFbEOh9DrXK2i7fZYTJ5THqBbHyDyrIFb9dyeY5r3qVWJF6JQJXzucW%2BhnIKu7OcrLu36fJSoArD%2FpwHZEdpJI4rwz5eX4Z0NP6n0PNyEHXR6lZ3b3eh8cQMKubQaZgp%2BuaGXXMfTXXzL6EAuhtquapwBjeGB6xryHh6vPnNCawW0htoF7svt2lXcIEr32mVrbh6l4IKFosFeRZeirNyuw6Hl8HGBxdVvvSnfFcDaDaYGuN6iZCEdwQ%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>guides/technical-faq/general.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>guides/technical-faq/admin.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>guides/technical-faq/admin.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>guides/technical-faq/metrics-and-performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>guides/technical-faq/setup.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>guides/technical-faq/troubleshooting.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>guides/technical-faq/troubleshooting.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>quickstart.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>guides/track/launch.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>track/log/log-tables.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>track/log/logging-faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>guides/sweep/faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>guides/sweep/faqs.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>guides/track/tracking-faq.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>guides/track/tracking-faq.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `/` as a separator for organizing log...</td>\n",
       "      <td>guides/track/log/logging-faqs.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `/` as a separator for organizing log...   \n",
       "\n",
       "                                             source  \n",
       "0                   guides/technical-faq/general.md  \n",
       "1                   guides/technical-faq/general.md  \n",
       "2                   guides/technical-faq/general.md  \n",
       "3                     guides/technical-faq/admin.md  \n",
       "4                     guides/technical-faq/admin.md  \n",
       "5   guides/technical-faq/metrics-and-performance.md  \n",
       "6   guides/technical-faq/metrics-and-performance.md  \n",
       "7      guides/technical-faq/metrics-and-performance  \n",
       "8                     guides/technical-faq/setup.md  \n",
       "9           guides/technical-faq/troubleshooting.md  \n",
       "10          guides/technical-faq/troubleshooting.md  \n",
       "11                                    quickstart.md  \n",
       "12                           guides/track/launch.md  \n",
       "13                          track/log/log-tables.md  \n",
       "14                        track/log/logging-faqs.md  \n",
       "15                             guides/sweep/faqs.md  \n",
       "16                             guides/sweep/faqs.md  \n",
       "17                     guides/track/tracking-faq.md  \n",
       "18                     guides/track/tracking-faq.md  \n",
       "19                 guides/track/log/logging-faqs.md  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics\n",
    "\n",
    "**TODO** Add weave model and evals in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/03 11:15:40 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/0z2t11h3/artifact/936065852/wandb_manifest.json?Expires=1719989140&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=JTlCAuLgo9flPZRV6xn0UMyUODR%2FFQ0HCVccTX%2F57%2Flp%2FgJd837C30XMJw45HXMHr2dUi5Q2kZg%2F6blnoZ2Rd3RoLADc8G%2BH8oA8bilHkIIPh1m0EanO6%2B%2FdB4Ywe4K0t7wg06c6FYa8VjkFOXtzlNu%2BWWBPndsuQ8XPClXITzjbKUJpWPzTxP6ItWvkVVCmbVB2ZZdki2bNAkiExE1%2BT%2FGIc9nmviBMB%2B1eBeCFnQ4diLTK42BmTbnsu7dcUOar7b%2B5sNNeUA3TTWVhStCdAisntdspZDan7pNIXi8p6esmvV4wL%2FibhmgSikgKpGJp61lPFEGLNUQNtr0ZEjnHtA%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 470,\n",
       "   'cleaned_tokens': 470},\n",
       "  'cleaned_content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```'},\n",
       " {'content': '--- slug: /guides/app/features/custom-charts displayed_sidebar: default --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; # Custom Charts Use **Custom Charts** to create charts that aren\\'t possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of [Vega](https://vega.github.io/vega/). * **What\\'s possible**: Read the[ launch announcement →](https://wandb.ai/wandb/posts/reports/Announcing-the-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg) * **Code**: Try a live example in a[ hosted notebook →](https://tiny.cc/custom-charts) * **Video**: Watch a quick [walkthrough video →](https://www.youtube.com/watch?v=3-N9OV6bkSM) * **Example**: Quick Keras and Sklearn [demo notebook →](https://colab.research.google.com/drive/1g-gNGokPWM2Qbc8p1Gofud0\\\\_5AoZdoSD?usp=sharing) ![Supported charts from vega.github.io/vega](/images/app_ui/supported_charts.png) ### How it works 1. **Log data**: From your script, log [config](../../../../guides/track/config.md) and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table` 2. **Customize the chart**: Pull in any of this logged data with a [GraphQL](https://graphql.org) query. Visualize the results of your query with [Vega](https://vega.github.io/vega/), a powerful visualization grammar. 3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`. ![](/images/app_ui/pr_roc.png) ## Log charts from a script ### Builtin presets These presets have builtin `wandb.plot` methods that make it fast to log charts directly from your script and see the exact visualizations you\\'re looking for in the UI. <Tabs defaultValue=\"line-plot\" values={[ {label: \\'Line plot\\', value: \\'line-plot\\'}, {label: \\'Scatter plot\\', value: \\'scatter-plot\\'}, {label: \\'Bar chart\\', value: \\'bar-chart\\'}, {label: \\'Histogram\\', value: \\'histogram\\'}, {label: \\'PR curve\\', value: \\'pr-curve\\'}, {label: \\'ROC curve\\', value: \\'roc-curve\\'}, ]}> <TabItem value=\"line-plot\"> `wandb.plot.line()` Log a custom line plot—a list of connected and ordered points (x,y) on arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(x_values, y_values)] table = wandb.Table(data=data, columns=[\"x\", \"y\"]) wandb.log( { \"my_custom_plot_id\": wandb.plot.line( table, \"x\", \"y\", title=\"Custom Y vs X Line Plot\" ) } ) ``` You can use this to log curves on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/line_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Line-Plots--VmlldzoyNjk5NTA) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"scatter-plot\"> `wandb.plot.scatter()` Log a custom scatter plot—a list of points (x, y) on a pair of arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(class_x_prediction_scores, class_y_prediction_scores)] table = wandb.Table(data=data, columns=[\"class_x\", \"class_y\"]) wandb.log({\"my_custom_id\": wandb.plot.scatter(table, \"class_x\", \"class_y\")}) ``` You can use this to log scatter points on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/demo_scatter_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Scatter-Plots--VmlldzoyNjk5NDQ) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"bar-chart\"> `wandb.plot.bar()` Log a custom bar chart—a list of labeled values as bars—natively in a few lines: ```python data = [[label, val] for (label, val) in zip(labels, values)] table = wandb.Table(data=data, columns=[\"label\", \"value\"]) wandb.log( { \"my_bar_chart_id\": wandb.plot.bar( table, \"label\", \"value\", title=\"Custom Bar Chart\" ) } ) ``` You',\n",
       "  'metadata': {'source': 'guides/app/features/custom-charts/intro.md',\n",
       "   'raw_tokens': 500,\n",
       "   'cleaned_tokens': 500},\n",
       "  'cleaned_content': '--- slug: /guides/app/features/custom-charts displayed_sidebar: default --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; # Custom Charts Use **Custom Charts** to create charts that aren\\'t possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of [Vega](https://vega.github.io/vega/). * **What\\'s possible**: Read the[ launch announcement →](https://wandb.ai/wandb/posts/reports/Announcing-the-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg) * **Code**: Try a live example in a[ hosted notebook →](https://tiny.cc/custom-charts) * **Video**: Watch a quick [walkthrough video →](https://www.youtube.com/watch?v=3-N9OV6bkSM) * **Example**: Quick Keras and Sklearn [demo notebook →](https://colab.research.google.com/drive/1g-gNGokPWM2Qbc8p1Gofud0\\\\_5AoZdoSD?usp=sharing) ![Supported charts from vega.github.io/vega](/images/app_ui/supported_charts.png) ### How it works 1. **Log data**: From your script, log [config](../../../../guides/track/config.md) and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table` 2. **Customize the chart**: Pull in any of this logged data with a [GraphQL](https://graphql.org) query. Visualize the results of your query with [Vega](https://vega.github.io/vega/), a powerful visualization grammar. 3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`. ![](/images/app_ui/pr_roc.png) ## Log charts from a script ### Builtin presets These presets have builtin `wandb.plot` methods that make it fast to log charts directly from your script and see the exact visualizations you\\'re looking for in the UI. <Tabs defaultValue=\"line-plot\" values={[ {label: \\'Line plot\\', value: \\'line-plot\\'}, {label: \\'Scatter plot\\', value: \\'scatter-plot\\'}, {label: \\'Bar chart\\', value: \\'bar-chart\\'}, {label: \\'Histogram\\', value: \\'histogram\\'}, {label: \\'PR curve\\', value: \\'pr-curve\\'}, {label: \\'ROC curve\\', value: \\'roc-curve\\'}, ]}> <TabItem value=\"line-plot\"> `wandb.plot.line()` Log a custom line plot—a list of connected and ordered points (x,y) on arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(x_values, y_values)] table = wandb.Table(data=data, columns=[\"x\", \"y\"]) wandb.log( { \"my_custom_plot_id\": wandb.plot.line( table, \"x\", \"y\", title=\"Custom Y vs X Line Plot\" ) } ) ``` You can use this to log curves on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/line_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Line-Plots--VmlldzoyNjk5NTA) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"scatter-plot\"> `wandb.plot.scatter()` Log a custom scatter plot—a list of points (x, y) on a pair of arbitrary axes x and y. ```python data = [[x, y] for (x, y) in zip(class_x_prediction_scores, class_y_prediction_scores)] table = wandb.Table(data=data, columns=[\"class_x\", \"class_y\"]) wandb.log({\"my_custom_id\": wandb.plot.scatter(table, \"class_x\", \"class_y\")}) ``` You can use this to log scatter points on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y). ![](/images/app_ui/demo_scatter_plot.png) [See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Scatter-Plots--VmlldzoyNjk5NDQ) [Run the code →](https://tiny.cc/custom-charts) </TabItem> <TabItem value=\"bar-chart\"> `wandb.plot.bar()` Log a custom bar chart—a list of labeled values as bars—natively in a few lines: ```python data = [[label, val] for (label, val) in zip(labels, values)] table = wandb.Table(data=data, columns=[\"label\", \"value\"]) wandb.log( { \"my_bar_chart_id\": wandb.plot.bar( table, \"label\", \"value\", title=\"Custom Bar Chart\" ) } ) ``` You'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the Retriever class from Chapter 1\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.index = None\n",
    "        self.data = None\n",
    "\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    #@weave.op\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(retrieved_docs: List[str], actual_doc: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate for a single query.\n",
    "\n",
    "    :param retrieved_docs: List of retrieved documents\n",
    "    :param actual_doc: The single actual relevant document\n",
    "    :return: Hit rate (1 if the relevant document is retrieved, 0 otherwise)\n",
    "    \"\"\"\n",
    "    return 1 if actual_doc in retrieved_docs else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>hit_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  hit_rate\n",
       "0   What is the difference between `.log()` and `....         1\n",
       "1   How do I switch between accounts on the same m...         1\n",
       "2              How is W&B different from TensorBoard?         1\n",
       "3   What is the difference between team and organi...         1\n",
       "4   What is the difference between team and entity...         1\n",
       "5   Can I just log metrics, no code or dataset exa...         1\n",
       "6   How can I log a metric that doesn't change ove...         1\n",
       "7                How many runs to create per project?         0\n",
       "8                            Can I run wandb offline?         0\n",
       "9                  How do I deal with network issues?         1\n",
       "10  What happens if internet connection is lost wh...         1\n",
       "11                        Where do I find my API key?         1\n",
       "12                  How do I create a W&B Experiment?         1\n",
       "13                               Log a table to a run         0\n",
       "14                     How do I log a list of values?         0\n",
       "15  Is there a way to add extra values to a sweep,...         0\n",
       "16  Can we flag boolean variables as hyperparameters?         0\n",
       "17  How do I programmatically access the human-rea...         1\n",
       "18  How can I save the git commit associated with ...         1\n",
       "19  How can I organize my logged charts and media ...         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hit_rates = []\n",
    "for sample in eval_samples:\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "    hit_rate = calculate_hit_rate(search_results, expected_source)\n",
    "    hit_rates.append({\"query\": query, \"hit_rate\": hit_rate})\n",
    "\n",
    "hit_rate_df = pd.DataFrame(hit_rates)\n",
    "display(hit_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Hit Rate: 0.7000\n",
      "Std-dev Hit Rate: 0.4702\n"
     ]
    }
   ],
   "source": [
    "# we need a single number to rate the retrieval system\n",
    "# the mean hit rate is a good metric to evaluate the retrieval system as a whole\n",
    "\n",
    "print(f\"Mean Hit Rate: {hit_rate_df['hit_rate'].mean():.4f}\")\n",
    "print(f\"Std-dev Hit Rate: {hit_rate_df['hit_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank) is a metric that measures the quality of the retrieval system by evaluating the proportion of queries for which the most relevant document is retrieved.\n",
    "# Let's calculate the MRR score for our retrieval system\n",
    "\n",
    "def calculate_mrr(retrieved_docs: List[str], actual_doc: str) -> float:\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(retrieved_docs, 1):\n",
    "        if result == actual_doc:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>mrr_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  mrr_score\n",
       "0   What is the difference between `.log()` and `....   0.500000\n",
       "1   How do I switch between accounts on the same m...   1.000000\n",
       "2              How is W&B different from TensorBoard?   0.333333\n",
       "3   What is the difference between team and organi...   1.000000\n",
       "4   What is the difference between team and entity...   1.000000\n",
       "5   Can I just log metrics, no code or dataset exa...   1.000000\n",
       "6   How can I log a metric that doesn't change ove...   0.500000\n",
       "7                How many runs to create per project?   0.000000\n",
       "8                            Can I run wandb offline?   0.000000\n",
       "9                  How do I deal with network issues?   1.000000\n",
       "10  What happens if internet connection is lost wh...   1.000000\n",
       "11                        Where do I find my API key?   0.500000\n",
       "12                  How do I create a W&B Experiment?   1.000000\n",
       "13                               Log a table to a run   0.000000\n",
       "14                     How do I log a list of values?   0.000000\n",
       "15  Is there a way to add extra values to a sweep,...   0.000000\n",
       "16  Can we flag boolean variables as hyperparameters?   0.000000\n",
       "17  How do I programmatically access the human-rea...   0.500000\n",
       "18  How can I save the git commit associated with ...   0.500000\n",
       "19  How can I organize my logged charts and media ...   0.333333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mrr_scores = []\n",
    "for sample in eval_samples:\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "    mrr_score = calculate_mrr(search_results, expected_source)\n",
    "    mrr_scores.append({\"query\": query, \"mrr_score\": mrr_score})\n",
    "\n",
    "mrr_scores_df = pd.DataFrame(mrr_scores)\n",
    "display(mrr_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MRR Score: 0.5083\n",
      "MRR Score Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mrr_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.417017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count      mean       std  min  25%  50%  75%  max\n",
       "mrr_score   20.0  0.508333  0.417017  0.0  0.0  0.5  1.0  1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need a single number to rate the retrieval system\n",
    "# the mean mrr score is a good metric to evaluate the retrieval system\n",
    "print(f\"Mean MRR Score: {mrr_scores_df['mrr_score'].mean():.4f}\")\n",
    "\n",
    "# Looking at the mean might not give us the complete picture. We can also look at the distribution of the MRR scores\n",
    "print(\"MRR Score Statistics:\")\n",
    "display(pd.DataFrame(mrr_scores_df[\"mrr_score\"].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can also evaluate the retrieval system on other metrics\n",
    "# Writing these might be tedious, but we can use the `ranx` library to evaluate the retrieval system\n",
    "# Metrics Include\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "# MAP (Mean Average Precision)\n",
    "# Hit Rate\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 Score\n",
    "\n",
    "\n",
    "RETRIEVAL_METRICS = [\"ndcg@5\", \"map@5\", \"mrr\", \"hit_rate\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "\n",
    "def evaluate_retriever(retrieved_docs: List[Dict[str, Any]], actual_doc: str) -> Dict[str, Any]:\n",
    "    qrels = Qrels({\"query\": {actual_doc: 1}})\n",
    "    run = Run({\"query\": {doc[\"source\"]: doc[\"score\"] for doc in retrieved_docs}})\n",
    "    return evaluate(qrels, run, metrics=RETRIEVAL_METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:13<00:00,  1.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>map@5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query   ndcg@5     map@5  \\\n",
       "0   What is the difference between `.log()` and `....  0.63093  0.500000   \n",
       "1   How do I switch between accounts on the same m...  1.00000  1.000000   \n",
       "2              How is W&B different from TensorBoard?  0.50000  0.333333   \n",
       "3   What is the difference between team and organi...  1.00000  1.000000   \n",
       "4   What is the difference between team and entity...  1.00000  1.000000   \n",
       "5   Can I just log metrics, no code or dataset exa...  1.00000  1.000000   \n",
       "6   How can I log a metric that doesn't change ove...  0.63093  0.500000   \n",
       "7                How many runs to create per project?  0.00000  0.000000   \n",
       "8                            Can I run wandb offline?  0.00000  0.000000   \n",
       "9                  How do I deal with network issues?  1.00000  1.000000   \n",
       "10  What happens if internet connection is lost wh...  1.00000  1.000000   \n",
       "11                        Where do I find my API key?  0.63093  0.500000   \n",
       "12                  How do I create a W&B Experiment?  1.00000  1.000000   \n",
       "13                               Log a table to a run  0.00000  0.000000   \n",
       "14                     How do I log a list of values?  0.00000  0.000000   \n",
       "15  Is there a way to add extra values to a sweep,...  0.00000  0.000000   \n",
       "16  Can we flag boolean variables as hyperparameters?  0.00000  0.000000   \n",
       "17  How do I programmatically access the human-rea...  0.63093  0.500000   \n",
       "18  How can I save the git commit associated with ...  0.63093  0.500000   \n",
       "19  How can I organize my logged charts and media ...  0.50000  0.333333   \n",
       "\n",
       "         mrr  hit_rate  precision  recall        f1  \n",
       "0   0.500000       1.0   0.250000     1.0  0.400000  \n",
       "1   1.000000       1.0   0.200000     1.0  0.333333  \n",
       "2   0.333333       1.0   0.333333     1.0  0.500000  \n",
       "3   1.000000       1.0   0.250000     1.0  0.400000  \n",
       "4   1.000000       1.0   0.200000     1.0  0.333333  \n",
       "5   1.000000       1.0   0.250000     1.0  0.400000  \n",
       "6   0.500000       1.0   0.200000     1.0  0.333333  \n",
       "7   0.000000       0.0   0.000000     0.0  0.000000  \n",
       "8   0.000000       0.0   0.000000     0.0  0.000000  \n",
       "9   1.000000       1.0   0.200000     1.0  0.333333  \n",
       "10  1.000000       1.0   0.250000     1.0  0.400000  \n",
       "11  0.500000       1.0   0.200000     1.0  0.333333  \n",
       "12  1.000000       1.0   0.200000     1.0  0.333333  \n",
       "13  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "14  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "15  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "16  0.000000       0.0   0.000000     0.0  0.000000  \n",
       "17  0.500000       1.0   0.250000     1.0  0.400000  \n",
       "18  0.500000       1.0   0.250000     1.0  0.400000  \n",
       "19  0.333333       1.0   0.200000     1.0  0.333333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieval_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = retriever.search(query, k=5)\n",
    "    eval_scores = evaluate_retriever(search_results, expected_source)\n",
    "    retrieval_scores.append({\"query\": query, **eval_scores})\n",
    "\n",
    "retrieval_scores_df = pd.DataFrame(retrieval_scores)\n",
    "display(retrieval_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Overall Retrieval Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ndcg@5</th>\n",
       "      <th>map@5</th>\n",
       "      <th>mrr</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557732</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.161667</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.261667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ndcg@5     map@5       mrr  hit_rate  precision  recall        f1\n",
       "0  0.557732  0.508333  0.508333       0.7   0.161667     0.7  0.261667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Retrieval Score Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ndcg@5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.557732</td>\n",
       "      <td>0.414796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630930</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map@5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.417017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrr</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.417017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_rate</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.470162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.161667</td>\n",
       "      <td>0.113181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.470162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.261667</td>\n",
       "      <td>0.180407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count      mean       std  min  25%       50%   75%       max\n",
       "ndcg@5      20.0  0.557732  0.414796  0.0  0.0  0.630930  1.00  1.000000\n",
       "map@5       20.0  0.508333  0.417017  0.0  0.0  0.500000  1.00  1.000000\n",
       "mrr         20.0  0.508333  0.417017  0.0  0.0  0.500000  1.00  1.000000\n",
       "hit_rate    20.0  0.700000  0.470162  0.0  0.0  1.000000  1.00  1.000000\n",
       "precision   20.0  0.161667  0.113181  0.0  0.0  0.200000  0.25  0.333333\n",
       "recall      20.0  0.700000  0.470162  0.0  0.0  1.000000  1.00  1.000000\n",
       "f1          20.0  0.261667  0.180407  0.0  0.0  0.333333  0.40  0.500000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nMean Overall Retrieval Scores:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Retrieval Score Statistics:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? \n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
    "    0 = represent that the excerpt has nothing to do with the query,\n",
    "    1 = represents that the excerpt seems related to the query but does not help answer it,\n",
    "    2 = represents that the excerpt has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information and\n",
    "    3 = represents that the excerpt is dedicated to the query and contains the exact answer.\n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the topic but not completely, category 2 if excerpt presents something very important related to the entire topic but also has some extra information and category 3 if the excerpt only and entirely refers to the topic. If none of the above satisfies give it category 0.\n",
    "\n",
    "Query: {query}\n",
    "Document: {document}\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "\n",
    "```json\n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> int:\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": RETRIEVAL_EVAL_PROMPT.format(query=query, passage=passage)}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_retriever_evaluation_using_llm(eval_samples):\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>scores</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>[1, 3, 0, 2, 0]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>[3, 0, 0, 0, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>[1, 1, 3, 3, 2]</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>[3, 1, 0, 0, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>[3, 0, 2, 0, 1]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>[3, 1, 2, 2, 1]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>[0, 3, 2, 3, 1]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>[3, 0, 0, 0, 1]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>[3, 3, 3, 2, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>[2, 0, 1, 0, 0]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>[3, 0, 0, 2, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>[0, 3, 3, 0, 0]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>[3, 3, 1, 3, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>[3, 3, 3, 1, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>[0, 3, 1, 2, 2]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>[3, 0, 2, 0, 1]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>[0, 0, 3, 2, 1]</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>[3, 3, 1, 1, 2]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>[0, 3, 2, 1, 0]</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>[2, 1, 3, 1, 2]</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query           scores  \\\n",
       "0   What is the difference between `.log()` and `....  [1, 3, 0, 2, 0]   \n",
       "1   How do I switch between accounts on the same m...  [3, 0, 0, 0, 0]   \n",
       "2              How is W&B different from TensorBoard?  [1, 1, 3, 3, 2]   \n",
       "3   What is the difference between team and organi...  [3, 1, 0, 0, 0]   \n",
       "4   What is the difference between team and entity...  [3, 0, 2, 0, 1]   \n",
       "5   Can I just log metrics, no code or dataset exa...  [3, 1, 2, 2, 1]   \n",
       "6   How can I log a metric that doesn't change ove...  [0, 3, 2, 3, 1]   \n",
       "7                How many runs to create per project?  [3, 0, 0, 0, 1]   \n",
       "8                            Can I run wandb offline?  [3, 3, 3, 2, 2]   \n",
       "9                  How do I deal with network issues?  [2, 0, 1, 0, 0]   \n",
       "10  What happens if internet connection is lost wh...  [3, 0, 0, 2, 0]   \n",
       "11                        Where do I find my API key?  [0, 3, 3, 0, 0]   \n",
       "12                  How do I create a W&B Experiment?  [3, 3, 1, 3, 0]   \n",
       "13                               Log a table to a run  [3, 3, 3, 1, 2]   \n",
       "14                     How do I log a list of values?  [0, 3, 1, 2, 2]   \n",
       "15  Is there a way to add extra values to a sweep,...  [3, 0, 2, 0, 1]   \n",
       "16  Can we flag boolean variables as hyperparameters?  [0, 0, 3, 2, 1]   \n",
       "17  How do I programmatically access the human-rea...  [3, 3, 1, 1, 2]   \n",
       "18  How can I save the git commit associated with ...  [0, 3, 2, 1, 0]   \n",
       "19  How can I organize my logged charts and media ...  [2, 1, 3, 1, 2]   \n",
       "\n",
       "    rank_score  \n",
       "0     0.500000  \n",
       "1     1.000000  \n",
       "2     0.333333  \n",
       "3     1.000000  \n",
       "4     1.000000  \n",
       "5     1.000000  \n",
       "6     0.500000  \n",
       "7     1.000000  \n",
       "8     1.000000  \n",
       "9     0.000000  \n",
       "10    1.000000  \n",
       "11    0.500000  \n",
       "12    1.000000  \n",
       "13    1.000000  \n",
       "14    0.500000  \n",
       "15    1.000000  \n",
       "16    0.333333  \n",
       "17    1.000000  \n",
       "18    0.500000  \n",
       "19    0.333333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank Score: 0.7250\n",
      "Std-dev Rank Score: 0.3301\n"
     ]
    }
   ],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 3 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 3:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets reload the Response Generator and the RAGPipeline from the previous chapter\n",
    "class ResponseGenerator:\n",
    "    def __init__(self, model: str, prompt: str):\n",
    "        self.client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "        self.model = model\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def generate_context(self, context: List[Dict[str, any]]) -> str:\n",
    "        return \"\\n\".join(\n",
    "            [f\"Source: {item['source']}\\nText: {item['text']}\\n\\n\" for item in context]\n",
    "        )\n",
    "\n",
    "    # @weave.op()\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        context_text = self.generate_context(context)\n",
    "        system_message = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": self.prompt.format(context=context_text),\n",
    "        }\n",
    "        user_message = {\"role\": \"user\", \"content\": f\"Question: {query}\\n\\nAnswer:\"}\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model, messages=[system_message, user_message]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "\n",
    "PROMPT = (\n",
    "    \"You are a helpful customer support assistant that can answer questions about W&B\\n\\n\"\n",
    "    \"Your answers must be based only on the provided context.\\n\\n\"\n",
    "    \"<context>\\n{context}\\n</context>\"\n",
    ")\n",
    "response_generator = ResponseGenerator(model=\"gpt-3.5-turbo\", prompt=PROMPT)\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever: Retriever, response_generator: ResponseGenerator, top_k: int = 5):\n",
    "        self.retriever = retriever\n",
    "        self.response_generator = response_generator\n",
    "        self.top_k = top_k\n",
    "\n",
    "    \n",
    "    # @weave.op\n",
    "    def __call__(self, query: str) -> str:\n",
    "        context = self.retriever.search(query, self.top_k)\n",
    "        return self.response_generator.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "def calculate_diff_score(candidate, reference):\n",
    "    return difflib.SequenceMatcher(None, candidate, reference).ratio()\n",
    "\n",
    "\n",
    "def calculate_levenshtein_score(candidate, reference):\n",
    "    return Levenshtein.ratio(candidate, reference)\n",
    "\n",
    "\n",
    "\n",
    "# semantic answer similarity. (SAS) - https://arxiv.org/abs/2108.06130\n",
    "# Originally, one should use a transformer based cross-encoder to measure and classify this. \n",
    "# For example, use something from https://sbert.net/docs/cross_encoder/usage/usage.html\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "def calculate_similarity(candidate, reference):\n",
    "    vectors = retriever.vectorizer.transform([candidate, reference])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    meteor_score = meteor([candidate], reference)\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:37<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_pipeline = RAGPipeline(retriever, response_generator)\n",
    "\n",
    "response_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample['question']\n",
    "    actual_answer = rag_pipeline(query)\n",
    "    expected_answer = sample['answer']\n",
    "    diff_score = calculate_diff_score(actual_answer, expected_answer)\n",
    "    levenshtein_score = calculate_levenshtein_score(actual_answer, expected_answer)\n",
    "    rouge_score = calculate_rouge(actual_answer, expected_answer)\n",
    "    bleu_score = calculate_bleu(actual_answer, expected_answer)\n",
    "    meteor_score = calculate_meteor(actual_answer, expected_answer)\n",
    "    similarity_score = calculate_similarity(actual_answer, expected_answer)\n",
    "\n",
    "    response_scores.append({\n",
    "        \"query\": query,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"diff_score\": diff_score,\n",
    "        \"levenshtein_score\": levenshtein_score,\n",
    "        \"rouge_score\": rouge_score,\n",
    "        \"bleu_score\": bleu_score,\n",
    "        \"meteor_score\": meteor_score,\n",
    "        \"similarity_score\": similarity_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>diff_score</th>\n",
       "      <th>levenshtein_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>0.156752</td>\n",
       "      <td>0.613787</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.249934</td>\n",
       "      <td>0.557228</td>\n",
       "      <td>0.723108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>To switch between accounts on the same machine...</td>\n",
       "      <td>0.417234</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.564265</td>\n",
       "      <td>0.806683</td>\n",
       "      <td>0.858553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>Weights &amp; Biases (W&amp;B) differs from TensorBoar...</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.540210</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.163461</td>\n",
       "      <td>0.592270</td>\n",
       "      <td>0.490417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.815238</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.662632</td>\n",
       "      <td>0.871998</td>\n",
       "      <td>0.921223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>By default, W&amp;B doesn't log any dataset exampl...</td>\n",
       "      <td>0.416481</td>\n",
       "      <td>0.665924</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.228351</td>\n",
       "      <td>0.636014</td>\n",
       "      <td>0.775382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9})` wil...</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.605345</td>\n",
       "      <td>0.726232</td>\n",
       "      <td>0.896947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>We recommend creating roughly 10k runs per pro...</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.527979</td>\n",
       "      <td>0.777745</td>\n",
       "      <td>0.860045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>Yes, you can run `wandb` in offline mode. To r...</td>\n",
       "      <td>0.070252</td>\n",
       "      <td>0.450055</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.391516</td>\n",
       "      <td>0.507570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>If you are experiencing SSL or network errors ...</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>0.676491</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.389579</td>\n",
       "      <td>0.676808</td>\n",
       "      <td>0.793544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>If the internet connection is lost while you'r...</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.473865</td>\n",
       "      <td>0.774275</td>\n",
       "      <td>0.811392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, you can...</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.725227</td>\n",
       "      <td>0.844435</td>\n",
       "      <td>0.943022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>To create a W&amp;B Experiment, you can follow the...</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.162728</td>\n",
       "      <td>0.225610</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.390192</td>\n",
       "      <td>0.469942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>To log a table to a run using W&amp;B, you can fol...</td>\n",
       "      <td>0.180120</td>\n",
       "      <td>0.380254</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.332695</td>\n",
       "      <td>0.602680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>To log a list of values, you can use a diction...</td>\n",
       "      <td>0.235145</td>\n",
       "      <td>0.528445</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.409155</td>\n",
       "      <td>0.437347</td>\n",
       "      <td>0.586928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>0.853002</td>\n",
       "      <td>0.881988</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.917789</td>\n",
       "      <td>0.957065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>0.990132</td>\n",
       "      <td>0.990132</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.889734</td>\n",
       "      <td>0.961063</td>\n",
       "      <td>0.999249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>You can programmatically access the human-read...</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.512844</td>\n",
       "      <td>0.578146</td>\n",
       "      <td>0.650192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>When you call `wandb.init` in your script, the...</td>\n",
       "      <td>0.366791</td>\n",
       "      <td>0.515489</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.151657</td>\n",
       "      <td>0.474757</td>\n",
       "      <td>0.574996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `/` as a separator for organizing log...</td>\n",
       "      <td>You can organize your logged charts and media ...</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.554945</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.248184</td>\n",
       "      <td>0.573904</td>\n",
       "      <td>0.521221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                      expected_answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `/` as a separator for organizing log...   \n",
       "\n",
       "                                        actual_answer  diff_score  \\\n",
       "0   The summary is the value that shows in the tab...    0.156752   \n",
       "1   To switch between accounts on the same machine...    0.417234   \n",
       "2   Weights & Biases (W&B) differs from TensorBoar...    0.251166   \n",
       "3   A team is a collaborative workspace for a grou...    1.000000   \n",
       "4   A team is a collaborative workspace for a grou...    0.624762   \n",
       "5   By default, W&B doesn't log any dataset exampl...    0.416481   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9})` wil...    0.788382   \n",
       "7   We recommend creating roughly 10k runs per pro...    0.872727   \n",
       "8   Yes, you can run `wandb` in offline mode. To r...    0.070252   \n",
       "9   If you are experiencing SSL or network errors ...    0.286316   \n",
       "10  If the internet connection is lost while you'r...    0.317460   \n",
       "11  Once you've signed in to www.wandb.ai, you can...    0.877193   \n",
       "12  To create a W&B Experiment, you can follow the...    0.061971   \n",
       "13  To log a table to a run using W&B, you can fol...    0.180120   \n",
       "14  To log a list of values, you can use a diction...    0.235145   \n",
       "15  You cannot change the Sweep configuration once...    0.853002   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...    0.990132   \n",
       "17  You can programmatically access the human-read...    0.670455   \n",
       "18  When you call `wandb.init` in your script, the...    0.366791   \n",
       "19  You can organize your logged charts and media ...    0.329670   \n",
       "\n",
       "    levenshtein_score  rouge_score  bleu_score  meteor_score  similarity_score  \n",
       "0            0.613787     0.517241    0.249934      0.557228          0.723108  \n",
       "1            0.603175     0.722689    0.564265      0.806683          0.858553  \n",
       "2            0.540210     0.384615    0.163461      0.592270          0.490417  \n",
       "3            1.000000     1.000000    1.000000      0.999992          1.000000  \n",
       "4            0.815238     0.729730    0.662632      0.871998          0.921223  \n",
       "5            0.665924     0.512397    0.228351      0.636014          0.775382  \n",
       "6            0.804979     0.655172    0.605345      0.726232          0.896947  \n",
       "7            0.872727     0.769231    0.527979      0.777745          0.860045  \n",
       "8            0.450055     0.220339    0.046529      0.391516          0.507570  \n",
       "9            0.676491     0.581818    0.389579      0.676808          0.793544  \n",
       "10           0.755102     0.654867    0.473865      0.774275          0.811392  \n",
       "11           0.877193     0.833333    0.725227      0.844435          0.943022  \n",
       "12           0.162728     0.225610    0.000107      0.390192          0.469942  \n",
       "13           0.380254     0.261905    0.153382      0.332695          0.602680  \n",
       "14           0.528445     0.441860    0.409155      0.437347          0.586928  \n",
       "15           0.881988     0.840580    0.756000      0.917789          0.957065  \n",
       "16           0.990132     0.925373    0.889734      0.961063          0.999249  \n",
       "17           0.676136     0.653846    0.512844      0.578146          0.650192  \n",
       "18           0.515489     0.446429    0.151657      0.474757          0.574996  \n",
       "19           0.554945     0.515464    0.248184      0.573904          0.521221  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Overall Generation Scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diff_score</th>\n",
       "      <th>levenshtein_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4883</td>\n",
       "      <td>0.66825</td>\n",
       "      <td>0.594625</td>\n",
       "      <td>0.437911</td>\n",
       "      <td>0.666054</td>\n",
       "      <td>0.747174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diff_score  levenshtein_score  rouge_score  bleu_score  meteor_score  \\\n",
       "0      0.4883            0.66825     0.594625    0.437911      0.666054   \n",
       "\n",
       "   similarity_score  \n",
       "0          0.747174  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Generation Score Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diff_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.488300</td>\n",
       "      <td>0.315345</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.247160</td>\n",
       "      <td>0.391636</td>\n",
       "      <td>0.804537</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>levenshtein_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.668250</td>\n",
       "      <td>0.212530</td>\n",
       "      <td>0.162728</td>\n",
       "      <td>0.537269</td>\n",
       "      <td>0.671030</td>\n",
       "      <td>0.829610</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.594625</td>\n",
       "      <td>0.225095</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.445287</td>\n",
       "      <td>0.617832</td>\n",
       "      <td>0.739605</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.437911</td>\n",
       "      <td>0.282360</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.212128</td>\n",
       "      <td>0.441510</td>\n",
       "      <td>0.619667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteor_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.666054</td>\n",
       "      <td>0.201280</td>\n",
       "      <td>0.332695</td>\n",
       "      <td>0.536610</td>\n",
       "      <td>0.656411</td>\n",
       "      <td>0.816121</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>similarity_score</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.747174</td>\n",
       "      <td>0.182418</td>\n",
       "      <td>0.469942</td>\n",
       "      <td>0.583945</td>\n",
       "      <td>0.784463</td>\n",
       "      <td>0.903016</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count      mean       std       min       25%       50%  \\\n",
       "diff_score          20.0  0.488300  0.315345  0.061971  0.247160  0.391636   \n",
       "levenshtein_score   20.0  0.668250  0.212530  0.162728  0.537269  0.671030   \n",
       "rouge_score         20.0  0.594625  0.225095  0.220339  0.445287  0.617832   \n",
       "bleu_score          20.0  0.437911  0.282360  0.000107  0.212128  0.441510   \n",
       "meteor_score        20.0  0.666054  0.201280  0.332695  0.536610  0.656411   \n",
       "similarity_score    20.0  0.747174  0.182418  0.469942  0.583945  0.784463   \n",
       "\n",
       "                        75%       max  \n",
       "diff_score         0.804537  1.000000  \n",
       "levenshtein_score  0.829610  1.000000  \n",
       "rouge_score        0.739605  1.000000  \n",
       "bleu_score         0.619667  1.000000  \n",
       "meteor_score       0.816121  0.999992  \n",
       "similarity_score   0.903016  1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "response_scores_df = pd.DataFrame(response_scores)\n",
    "display(response_scores_df)\n",
    "\n",
    "GENERATION_METRICS = [col for col in response_scores_df.columns if \"score\" in col]\n",
    "\n",
    "\n",
    "print(\"\\nMean Overall Generation Scores:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Generation Score Statistics:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 3 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct but includes some extra information, is incomplete or misses some evaluation criteria.\n",
    "- 3 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "```json\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 3,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Example 2:\n",
    "```json\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Example 3:\n",
    "```json\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it includes assumptions about functions that are not requested in the user's query\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Example 4:\n",
    "```json\n",
    "{{\n",
    "    \"reason\": \"The generated answer is incorrect, irrelevant, and not factually correct and completely misses the user's intent.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "```\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "async def evaluate_correctness_using_llm_judge(query: str, reference_answer: str, generated_answer: str) -> int:\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": CORRECTNESS_EVAL_PROMPT.format(query=query, reference_answer=reference_answer, generated_answer=generated_answer)}],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_correctness_evaluation_using_llm(response_scores):\n",
    "    tasks = []\n",
    "    for row in response_scores:\n",
    "        query = row[\"query\"]\n",
    "        expected_answer = row[\"expected_answer\"]\n",
    "        generated_answer = row[\"actual_answer\"]\n",
    "        tasks.append(evaluate_correctness_using_llm_judge(query, expected_answer, generated_answer))\n",
    "    scores = await asyncio.gather(*tasks)\n",
    "    scores = list(map(json.loads, scores))\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_correctness_results = asyncio.run(run_correctness_evaluation_using_llm(response_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>actual_answer</th>\n",
       "      <th>diff_score</th>\n",
       "      <th>levenshtein_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>meteor_score</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>reason</th>\n",
       "      <th>final_score</th>\n",
       "      <th>decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between `.log()` and `....</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>The summary is the value that shows in the tab...</td>\n",
       "      <td>0.156752</td>\n",
       "      <td>0.613787</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.249934</td>\n",
       "      <td>0.557228</td>\n",
       "      <td>0.723108</td>\n",
       "      <td>The generated answer correctly addresses the m...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I switch between accounts on the same m...</td>\n",
       "      <td>If you have two W&amp;B accounts working from the ...</td>\n",
       "      <td>To switch between accounts on the same machine...</td>\n",
       "      <td>0.417234</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.564265</td>\n",
       "      <td>0.806683</td>\n",
       "      <td>0.858553</td>\n",
       "      <td>The generated answer closely follows the refer...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is W&amp;B different from TensorBoard?</td>\n",
       "      <td>We love the TensorBoard folks, and we have a T...</td>\n",
       "      <td>Weights &amp; Biases (W&amp;B) differs from TensorBoar...</td>\n",
       "      <td>0.251166</td>\n",
       "      <td>0.540210</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.163461</td>\n",
       "      <td>0.592270</td>\n",
       "      <td>0.490417</td>\n",
       "      <td>The generated answer closely follows the refer...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between team and organi...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>The generated answer is identical to the refer...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between team and entity...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>A team is a collaborative workspace for a grou...</td>\n",
       "      <td>0.624762</td>\n",
       "      <td>0.815238</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.662632</td>\n",
       "      <td>0.871998</td>\n",
       "      <td>0.921223</td>\n",
       "      <td>The generated answer accurately mirrors the re...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Can I just log metrics, no code or dataset exa...</td>\n",
       "      <td>**Dataset Examples**\\n\\nBy default, we don't l...</td>\n",
       "      <td>By default, W&amp;B doesn't log any dataset exampl...</td>\n",
       "      <td>0.416481</td>\n",
       "      <td>0.665924</td>\n",
       "      <td>0.512397</td>\n",
       "      <td>0.228351</td>\n",
       "      <td>0.636014</td>\n",
       "      <td>0.775382</td>\n",
       "      <td>The generated answer closely matches the refer...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can I log a metric that doesn't change ove...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9}` will...</td>\n",
       "      <td>Using `wandb.log({'final_accuracy': 0.9})` wil...</td>\n",
       "      <td>0.788382</td>\n",
       "      <td>0.804979</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.605345</td>\n",
       "      <td>0.726232</td>\n",
       "      <td>0.896947</td>\n",
       "      <td>The generated answer is mostly correct but con...</td>\n",
       "      <td>1</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many runs to create per project?</td>\n",
       "      <td>We recommend you have roughly 10k runs per pro...</td>\n",
       "      <td>We recommend creating roughly 10k runs per pro...</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.527979</td>\n",
       "      <td>0.777745</td>\n",
       "      <td>0.860045</td>\n",
       "      <td>The generated answer has the exact details as ...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can I run wandb offline?</td>\n",
       "      <td>If you're training on an offline machine and w...</td>\n",
       "      <td>Yes, you can run `wandb` in offline mode. To r...</td>\n",
       "      <td>0.070252</td>\n",
       "      <td>0.450055</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.046529</td>\n",
       "      <td>0.391516</td>\n",
       "      <td>0.507570</td>\n",
       "      <td>The generated answer is generally correct, men...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do I deal with network issues?</td>\n",
       "      <td>If you're seeing SSL or network errors:`wandb:...</td>\n",
       "      <td>If you are experiencing SSL or network errors ...</td>\n",
       "      <td>0.286316</td>\n",
       "      <td>0.676491</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.389579</td>\n",
       "      <td>0.676808</td>\n",
       "      <td>0.793544</td>\n",
       "      <td>The generated answer follows the intent of the...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What happens if internet connection is lost wh...</td>\n",
       "      <td>If the wandb library is unable to connect to t...</td>\n",
       "      <td>If the internet connection is lost while you'r...</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.654867</td>\n",
       "      <td>0.473865</td>\n",
       "      <td>0.774275</td>\n",
       "      <td>0.811392</td>\n",
       "      <td>The generated answer is correct, addresses all...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Where do I find my API key?</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, the API...</td>\n",
       "      <td>Once you've signed in to www.wandb.ai, you can...</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.725227</td>\n",
       "      <td>0.844435</td>\n",
       "      <td>0.943022</td>\n",
       "      <td>The generated answer mirrors the reference ans...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How do I create a W&amp;B Experiment?</td>\n",
       "      <td>Create a W&amp;B Experiment in four steps:\\n\\n1. [...</td>\n",
       "      <td>To create a W&amp;B Experiment, you can follow the...</td>\n",
       "      <td>0.061971</td>\n",
       "      <td>0.162728</td>\n",
       "      <td>0.225610</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.390192</td>\n",
       "      <td>0.469942</td>\n",
       "      <td>The generated answer addresses the four key st...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Log a table to a run</td>\n",
       "      <td>Use `wandb.log()` to save your table to the ru...</td>\n",
       "      <td>To log a table to a run using W&amp;B, you can fol...</td>\n",
       "      <td>0.180120</td>\n",
       "      <td>0.380254</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.153382</td>\n",
       "      <td>0.332695</td>\n",
       "      <td>0.602680</td>\n",
       "      <td>The generated answer correctly explains how to...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do I log a list of values?</td>\n",
       "      <td>You can log a list of values iteratively, or ...</td>\n",
       "      <td>To log a list of values, you can use a diction...</td>\n",
       "      <td>0.235145</td>\n",
       "      <td>0.528445</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.409155</td>\n",
       "      <td>0.437347</td>\n",
       "      <td>0.586928</td>\n",
       "      <td>The generated answer is correct as far as it g...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Is there a way to add extra values to a sweep,...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>You cannot change the Sweep configuration once...</td>\n",
       "      <td>0.853002</td>\n",
       "      <td>0.881988</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.917789</td>\n",
       "      <td>0.957065</td>\n",
       "      <td>The generated answer is factually accurate, ma...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Can we flag boolean variables as hyperparameters?</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>You can use the `${args_no_boolean_flags}` mac...</td>\n",
       "      <td>0.990132</td>\n",
       "      <td>0.990132</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.889734</td>\n",
       "      <td>0.961063</td>\n",
       "      <td>0.999249</td>\n",
       "      <td>The generated answer has the exact details as ...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How do I programmatically access the human-rea...</td>\n",
       "      <td>It's available as the `.name` attribute of a `...</td>\n",
       "      <td>You can programmatically access the human-read...</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.676136</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.512844</td>\n",
       "      <td>0.578146</td>\n",
       "      <td>0.650192</td>\n",
       "      <td>The generated answer is identical to the refer...</td>\n",
       "      <td>3</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How can I save the git commit associated with ...</td>\n",
       "      <td>When `wandb.init` is called in your script, we...</td>\n",
       "      <td>When you call `wandb.init` in your script, the...</td>\n",
       "      <td>0.366791</td>\n",
       "      <td>0.515489</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.151657</td>\n",
       "      <td>0.474757</td>\n",
       "      <td>0.574996</td>\n",
       "      <td>The generated answer addresses the main aspect...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How can I organize my logged charts and media ...</td>\n",
       "      <td>We treat `/` as a separator for organizing log...</td>\n",
       "      <td>You can organize your logged charts and media ...</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.554945</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>0.248184</td>\n",
       "      <td>0.573904</td>\n",
       "      <td>0.521221</td>\n",
       "      <td>The generated answer correctly describes the u...</td>\n",
       "      <td>2</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  \\\n",
       "0   What is the difference between `.log()` and `....   \n",
       "1   How do I switch between accounts on the same m...   \n",
       "2              How is W&B different from TensorBoard?   \n",
       "3   What is the difference between team and organi...   \n",
       "4   What is the difference between team and entity...   \n",
       "5   Can I just log metrics, no code or dataset exa...   \n",
       "6   How can I log a metric that doesn't change ove...   \n",
       "7                How many runs to create per project?   \n",
       "8                            Can I run wandb offline?   \n",
       "9                  How do I deal with network issues?   \n",
       "10  What happens if internet connection is lost wh...   \n",
       "11                        Where do I find my API key?   \n",
       "12                  How do I create a W&B Experiment?   \n",
       "13                               Log a table to a run   \n",
       "14                     How do I log a list of values?   \n",
       "15  Is there a way to add extra values to a sweep,...   \n",
       "16  Can we flag boolean variables as hyperparameters?   \n",
       "17  How do I programmatically access the human-rea...   \n",
       "18  How can I save the git commit associated with ...   \n",
       "19  How can I organize my logged charts and media ...   \n",
       "\n",
       "                                      expected_answer  \\\n",
       "0   The summary is the value that shows in the tab...   \n",
       "1   If you have two W&B accounts working from the ...   \n",
       "2   We love the TensorBoard folks, and we have a T...   \n",
       "3   A team is a collaborative workspace for a grou...   \n",
       "4   A team is a collaborative workspace for a grou...   \n",
       "5   **Dataset Examples**\\n\\nBy default, we don't l...   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9}` will...   \n",
       "7   We recommend you have roughly 10k runs per pro...   \n",
       "8   If you're training on an offline machine and w...   \n",
       "9   If you're seeing SSL or network errors:`wandb:...   \n",
       "10  If the wandb library is unable to connect to t...   \n",
       "11  Once you've signed in to www.wandb.ai, the API...   \n",
       "12  Create a W&B Experiment in four steps:\\n\\n1. [...   \n",
       "13  Use `wandb.log()` to save your table to the ru...   \n",
       "14   You can log a list of values iteratively, or ...   \n",
       "15  You cannot change the Sweep configuration once...   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...   \n",
       "17  It's available as the `.name` attribute of a `...   \n",
       "18  When `wandb.init` is called in your script, we...   \n",
       "19  We treat `/` as a separator for organizing log...   \n",
       "\n",
       "                                        actual_answer  diff_score  \\\n",
       "0   The summary is the value that shows in the tab...    0.156752   \n",
       "1   To switch between accounts on the same machine...    0.417234   \n",
       "2   Weights & Biases (W&B) differs from TensorBoar...    0.251166   \n",
       "3   A team is a collaborative workspace for a grou...    1.000000   \n",
       "4   A team is a collaborative workspace for a grou...    0.624762   \n",
       "5   By default, W&B doesn't log any dataset exampl...    0.416481   \n",
       "6   Using `wandb.log({'final_accuracy': 0.9})` wil...    0.788382   \n",
       "7   We recommend creating roughly 10k runs per pro...    0.872727   \n",
       "8   Yes, you can run `wandb` in offline mode. To r...    0.070252   \n",
       "9   If you are experiencing SSL or network errors ...    0.286316   \n",
       "10  If the internet connection is lost while you'r...    0.317460   \n",
       "11  Once you've signed in to www.wandb.ai, you can...    0.877193   \n",
       "12  To create a W&B Experiment, you can follow the...    0.061971   \n",
       "13  To log a table to a run using W&B, you can fol...    0.180120   \n",
       "14  To log a list of values, you can use a diction...    0.235145   \n",
       "15  You cannot change the Sweep configuration once...    0.853002   \n",
       "16  You can use the `${args_no_boolean_flags}` mac...    0.990132   \n",
       "17  You can programmatically access the human-read...    0.670455   \n",
       "18  When you call `wandb.init` in your script, the...    0.366791   \n",
       "19  You can organize your logged charts and media ...    0.329670   \n",
       "\n",
       "    levenshtein_score  rouge_score  bleu_score  meteor_score  \\\n",
       "0            0.613787     0.517241    0.249934      0.557228   \n",
       "1            0.603175     0.722689    0.564265      0.806683   \n",
       "2            0.540210     0.384615    0.163461      0.592270   \n",
       "3            1.000000     1.000000    1.000000      0.999992   \n",
       "4            0.815238     0.729730    0.662632      0.871998   \n",
       "5            0.665924     0.512397    0.228351      0.636014   \n",
       "6            0.804979     0.655172    0.605345      0.726232   \n",
       "7            0.872727     0.769231    0.527979      0.777745   \n",
       "8            0.450055     0.220339    0.046529      0.391516   \n",
       "9            0.676491     0.581818    0.389579      0.676808   \n",
       "10           0.755102     0.654867    0.473865      0.774275   \n",
       "11           0.877193     0.833333    0.725227      0.844435   \n",
       "12           0.162728     0.225610    0.000107      0.390192   \n",
       "13           0.380254     0.261905    0.153382      0.332695   \n",
       "14           0.528445     0.441860    0.409155      0.437347   \n",
       "15           0.881988     0.840580    0.756000      0.917789   \n",
       "16           0.990132     0.925373    0.889734      0.961063   \n",
       "17           0.676136     0.653846    0.512844      0.578146   \n",
       "18           0.515489     0.446429    0.151657      0.474757   \n",
       "19           0.554945     0.515464    0.248184      0.573904   \n",
       "\n",
       "    similarity_score                                             reason  \\\n",
       "0           0.723108  The generated answer correctly addresses the m...   \n",
       "1           0.858553  The generated answer closely follows the refer...   \n",
       "2           0.490417  The generated answer closely follows the refer...   \n",
       "3           1.000000  The generated answer is identical to the refer...   \n",
       "4           0.921223  The generated answer accurately mirrors the re...   \n",
       "5           0.775382  The generated answer closely matches the refer...   \n",
       "6           0.896947  The generated answer is mostly correct but con...   \n",
       "7           0.860045  The generated answer has the exact details as ...   \n",
       "8           0.507570  The generated answer is generally correct, men...   \n",
       "9           0.793544  The generated answer follows the intent of the...   \n",
       "10          0.811392  The generated answer is correct, addresses all...   \n",
       "11          0.943022  The generated answer mirrors the reference ans...   \n",
       "12          0.469942  The generated answer addresses the four key st...   \n",
       "13          0.602680  The generated answer correctly explains how to...   \n",
       "14          0.586928  The generated answer is correct as far as it g...   \n",
       "15          0.957065  The generated answer is factually accurate, ma...   \n",
       "16          0.999249  The generated answer has the exact details as ...   \n",
       "17          0.650192  The generated answer is identical to the refer...   \n",
       "18          0.574996  The generated answer addresses the main aspect...   \n",
       "19          0.521221  The generated answer correctly describes the u...   \n",
       "\n",
       "    final_score   decision  \n",
       "0             2    correct  \n",
       "1             3    correct  \n",
       "2             3    correct  \n",
       "3             3    correct  \n",
       "4             3    correct  \n",
       "5             3    correct  \n",
       "6             1  incorrect  \n",
       "7             3    correct  \n",
       "8             2    correct  \n",
       "9             2    correct  \n",
       "10            3    correct  \n",
       "11            3    correct  \n",
       "12            2    correct  \n",
       "13            2    correct  \n",
       "14            2    correct  \n",
       "15            3    correct  \n",
       "16            3    correct  \n",
       "17            3    correct  \n",
       "18            2    correct  \n",
       "19            2    correct  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_eval_df = pd.DataFrame(llm_judge_correctness_results)\n",
    "response_evals_df = pd.concat([response_scores_df, correctness_eval_df], axis=1)\n",
    "response_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scores_df[\"correctness_score\"] = llm_judge_correctness_results\n",
    "display(response_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` evaluators and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retreival Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evalations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evalations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
