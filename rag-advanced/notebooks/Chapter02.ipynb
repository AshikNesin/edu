{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP # Chapter 2:\n",
    "\n",
    "**Comprehensive Evaluation Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import difflib\n",
    "import Levenshtein\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import meteor\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wandb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import json\n",
    "import pathlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and improving an evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting data for evaluation\n",
    "Get from data from the docs website [FAQs](https://docs.wandb.ai/guides/technical-faq) to test the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # TODO: Remove this once we more to the final project\n",
    "# eval_artifact = wandb.Artifact(\n",
    "#     name=\"eval_dataset\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"Evaluation dataset for RAG\",\n",
    "#     metadata={\n",
    "#         \"total_samples\": 20,\n",
    "#         \"date_collected\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#         \"chapter\": \"Chapter 1\",\n",
    "#     },\n",
    "# )\n",
    "# eval_artifact.add_file(\"../data/eval/eval_dataset.jsonl\")\n",
    "# run.log_artifact(eval_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/eval_dataset:latest\", type=\"dataset\"\n",
    ")\n",
    "eval_dir = eval_artifact.download(\"../data/eval\")\n",
    "eval_dataset = pd.read_json(\n",
    "    f\"{eval_dir}/eval_dataset.jsonl\", lines=True, orient=\"records\"\n",
    ")\n",
    "eval_samples = eval_dataset.to_dict(orient=\"records\")\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Retriever\n",
    "\n",
    "This is a search problem, it's easiest to start with tradiaional Information retrieval metrics.\n",
    "\n",
    "\n",
    "ref: https://weaviate.io/blog/retrieval-evaluation-metrics\n",
    "\n",
    "**TODO** Add weave model and evals in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from Chapter 1\n",
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the Retriever class from Chapter 1\n",
    "class Retriever:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.index = None\n",
    "        self.data = None\n",
    "\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    #@weave.op\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(retrieved_docs: List[str], actual_doc: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the hit rate for a single query.\n",
    "\n",
    "    :param retrieved_docs: List of retrieved documents\n",
    "    :param actual_doc: The single actual relevant document\n",
    "    :return: Hit rate (1 if the relevant document is retrieved, 0 otherwise)\n",
    "    \"\"\"\n",
    "    return 1 if actual_doc in retrieved_docs else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_rates = []\n",
    "for sample in eval_samples:\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "    hit_rate = calculate_hit_rate(search_results, expected_source)\n",
    "    hit_rates.append({\"query\": query, \"hit_rate\": hit_rate})\n",
    "\n",
    "hit_rate_df = pd.DataFrame(hit_rates)\n",
    "display(hit_rate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a single number to rate the retrieval system\n",
    "# the mean hit rate is a good metric to evaluate the retrieval system as a whole\n",
    "\n",
    "print(f\"Mean Hit Rate: {hit_rate_df['hit_rate'].mean():.4f}\")\n",
    "print(f\"Std-dev Hit Rate: {hit_rate_df['hit_rate'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR (Mean Reciprocal Rank) is a metric that measures the quality of the retrieval system by evaluating the proportion of queries for which the most relevant document is retrieved.\n",
    "# Let's calculate the MRR score for our retrieval system\n",
    "\n",
    "def calculate_mrr(retrieved_docs: List[str], actual_doc: str) -> float:\n",
    "    mrr_score = 0\n",
    "    for rank, result in enumerate(retrieved_docs, 1):\n",
    "        if result == actual_doc:\n",
    "            mrr_score = 1 / rank\n",
    "            break\n",
    "    return mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_scores = []\n",
    "for sample in eval_samples:\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = [doc['source'] for doc in retriever.search(query, k=5)]\n",
    "    mrr_score = calculate_mrr(search_results, expected_source)\n",
    "    mrr_scores.append({\"query\": query, \"mrr_score\": mrr_score})\n",
    "\n",
    "mrr_scores_df = pd.DataFrame(mrr_scores)\n",
    "display(mrr_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a single number to rate the retrieval system\n",
    "# the mean mrr score is a good metric to evaluate the retrieval system\n",
    "print(f\"Mean MRR Score: {mrr_scores_df['mrr_score'].mean():.4f}\")\n",
    "\n",
    "# Looking at the mean might not give us the complete picture. We can also look at the distribution of the MRR scores\n",
    "print(\"MRR Score Statistics:\")\n",
    "display(pd.DataFrame(mrr_scores_df[\"mrr_score\"].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating retrieval on other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can also evaluate the retrieval system on other metrics\n",
    "# Writing these might be tedious, but we can use the `ranx` library to evaluate the retrieval system\n",
    "# Metrics Include\n",
    "# NDCG (Normalized Discounted Cumulative Gain)\n",
    "# MAP (Mean Average Precision)\n",
    "# Hit Rate\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 Score\n",
    "\n",
    "\n",
    "RETRIEVAL_METRICS = [\"ndcg@5\", \"map@5\", \"mrr\", \"hit_rate\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "\n",
    "def evaluate_retriever(retrieved_docs: List[Dict[str, Any]], actual_doc: str) -> Dict[str, Any]:\n",
    "    qrels = Qrels({\"query\": {actual_doc: 1}})\n",
    "    run = Run({\"query\": {doc[\"source\"]: doc[\"score\"] for doc in retrieved_docs}})\n",
    "    return evaluate(qrels, run, metrics=RETRIEVAL_METRICS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample[\"question\"]\n",
    "    expected_source = sample[\"source\"]\n",
    "    search_results = retriever.search(query, k=5)\n",
    "    eval_scores = evaluate_retriever(search_results, expected_source)\n",
    "    retrieval_scores.append({\"query\": query, **eval_scores})\n",
    "\n",
    "retrieval_scores_df = pd.DataFrame(retrieval_scores)\n",
    "display(retrieval_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMean Overall Retrieval Scores:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Retrieval Score Statistics:\")\n",
    "display(pd.DataFrame(retrieval_scores_df[RETRIEVAL_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Retrieval Judge\n",
    "\n",
    "**ref: https://arxiv.org/pdf/2406.06519**\n",
    "\n",
    "How do we evaluate if we don't have any ground truth? \n",
    "\n",
    "We can use a powerful LLM as a judge to evaluate the retriever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RETRIEVAL_EVAL_PROMPT =\"\"\"\n",
    "Given a query and a document excerpt, you must provide a score on an integer scale of 0 to 3 with the following meanings:\n",
    "    0 = represent that the excerpt has nothing to do with the query,\n",
    "    1 = represents that the excerpt seems related to the query but does not help answer it,\n",
    "    2 = represents that the excerpt has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information and\n",
    "    3 = represents that the excerpt is dedicated to the query and contains the exact answer.\n",
    "\n",
    "Important Instruction: Assign category 1 if the excerpt is somewhat related to the topic but not completely, category 2 if excerpt presents something very important related to the entire topic but also has some extra information and category 3 if the excerpt only and entirely refers to the topic. If none of the above satisfies give it category 0.\n",
    "\n",
    "Query: {query}\n",
    "Document: {document}\n",
    "\n",
    "Split this problem into steps:\n",
    "Consider the underlying intent of the query. Measure how well the content matches a likely intent of the query(M).\n",
    "Measure how trustworthy the excerpt is (T).\n",
    "Consider the aspects above and the relative importance of each, and decide on a final score (O). \n",
    "Final score must be an integer value only.\n",
    "Do not provide any code in result. Provide each score in the following JSON format: \n",
    "\n",
    "\n",
    "{{\"final_score\": <integer score without providing any reasoning.>}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "async def evaluate_retriever_using_llm_judge(query: str, passage: str) -> int:\n",
    "    response = await client.chat(\n",
    "        message=RETRIEVAL_EVAL_PROMPT.format(query=query, document=passage),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = eval_samples[0]\n",
    "query = sample[\"question\"]\n",
    "search_results = retriever.search(query, k=5)\n",
    "tasks = []\n",
    "for result in search_results:\n",
    "    tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "sample_scores = asyncio.run(asyncio.gather(*tasks))\n",
    "sample_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_retriever_evaluation_using_llm(eval_samples):\n",
    "    scores = []\n",
    "    for sample in eval_samples:\n",
    "        query = sample[\"question\"]\n",
    "        search_results = retriever.search(query, k=5)\n",
    "        tasks = []\n",
    "        for result in search_results:\n",
    "            tasks.append(evaluate_retriever_using_llm_judge(query, result[\"text\"]))\n",
    "        sample_scores = await asyncio.gather(*tasks)\n",
    "        sample_scores = map(json.loads, sample_scores)\n",
    "        sample_scores = list(map(lambda x: x[\"final_score\"], sample_scores))\n",
    "        scores.append({\"query\": query, \"scores\": sample_scores})\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_retrieval_results = asyncio.run(run_retriever_evaluation_using_llm(eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the scores for each document\n",
    "llm_judge_retrieval_results_df = pd.DataFrame(llm_judge_retrieval_results)\n",
    "\n",
    "# we can compute the reciprocal rank of the first document that is relevant to the query i.e. rated as 3 by our llm judge.\n",
    "def compute_rank_score(scores: List[int]) -> float:\n",
    "    rank_score = 0\n",
    "    for rank, result in enumerate(scores, 1):\n",
    "        if result == 3:\n",
    "            rank_score = 1 / rank\n",
    "            return rank_score\n",
    "    return rank_score\n",
    "\n",
    "llm_judge_retrieval_results_df[\"rank_score\"] = llm_judge_retrieval_results_df[\"scores\"].map(compute_rank_score)\n",
    "\n",
    "\n",
    "display(llm_judge_retrieval_results_df)\n",
    "\n",
    "\n",
    "print(f\"Mean Rank Score: {llm_judge_retrieval_results_df['rank_score'].mean():.4f}\")\n",
    "print(f\"Std-dev Rank Score: {llm_judge_retrieval_results_df['rank_score'].std():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets reload the Response Generator and the RAGPipeline from the previous chapter\n",
    "class ResponseGenerator:\n",
    "    def __init__(self, model: str, prompt: str):\n",
    "        self.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "        self.model = model\n",
    "        self.prompt = prompt\n",
    "\n",
    "    # @weave.op()\n",
    "\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        \n",
    "        documents = [{\"source\": item['source'], \"text\": item['text']} for item in context]\n",
    "        response = self.client.chat(\n",
    "            preamble=self.prompt,\n",
    "            message=query,\n",
    "            model=self.model,\n",
    "            documents=documents,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        return response.text\n",
    "    \n",
    "\n",
    "PROMPT = \"Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\"\n",
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=PROMPT)\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, retriever: Retriever, response_generator: ResponseGenerator, top_k: int = 5):\n",
    "        self.retriever = retriever\n",
    "        self.response_generator = response_generator\n",
    "        self.top_k = top_k\n",
    "\n",
    "    \n",
    "    # @weave.op\n",
    "    def __call__(self, query: str) -> str:\n",
    "        context = self.retriever.search(query, self.top_k)\n",
    "        return self.response_generator.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can measure the similarity of the response to the expected answer using difflib and Levenshtein distance\n",
    "# These are simple metrics.\n",
    "\n",
    "def calculate_diff_score(candidate, reference):\n",
    "    return difflib.SequenceMatcher(None, candidate, reference).ratio()\n",
    "\n",
    "\n",
    "def calculate_levenshtein_score(candidate, reference):\n",
    "    return Levenshtein.ratio(candidate, reference)\n",
    "\n",
    "\n",
    "\n",
    "# semantic answer similarity. (SAS) - https://arxiv.org/abs/2108.06130\n",
    "# Originally, one should use a transformer based cross-encoder to measure and classify this. \n",
    "# For example, use something from https://sbert.net/docs/cross_encoder/usage/usage.html\n",
    "# we can also calculate the cosine similarity between the candidate and the reference using our retriever's vectorizer\n",
    "def calculate_similarity(candidate, reference):\n",
    "    vectors = retriever.vectorizer.transform([candidate, reference])\n",
    "    similarity = cosine_similarity(vectors)[0][1]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# or we can use traditional metrics used to measure generation systems.\n",
    "# ref: https://blog.paperspace.com/automated-metrics-for-evaluating-generated-text/\n",
    "\n",
    "def calculate_rouge(candidate, reference):\n",
    "    rouge = Rouge(metrics=[\"rouge-l\"], stats=\"f\")\n",
    "    scores = rouge.get_scores(candidate, reference)\n",
    "    return scores[0][\"rouge-l\"][\"f\"]\n",
    "\n",
    "\n",
    "def calculate_bleu(candidate, reference):\n",
    "    chencherry = SmoothingFunction()\n",
    "    smoothing_function = chencherry.method2\n",
    "\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    score = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)\n",
    "    return score\n",
    "\n",
    "\n",
    "def calculate_meteor(candidate, reference):\n",
    "    reference = word_tokenize(reference)\n",
    "    candidate = word_tokenize(candidate)\n",
    "    meteor_score = meteor([candidate], reference)\n",
    "    return meteor_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RAGPipeline(retriever, response_generator)\n",
    "\n",
    "response_scores = []\n",
    "for sample in tqdm(eval_samples):\n",
    "    query = sample['question']\n",
    "    actual_answer = rag_pipeline(query)\n",
    "    expected_answer = sample['answer']\n",
    "    diff_score = calculate_diff_score(actual_answer, expected_answer)\n",
    "    levenshtein_score = calculate_levenshtein_score(actual_answer, expected_answer)\n",
    "    rouge_score = calculate_rouge(actual_answer, expected_answer)\n",
    "    bleu_score = calculate_bleu(actual_answer, expected_answer)\n",
    "    meteor_score = calculate_meteor(actual_answer, expected_answer)\n",
    "    similarity_score = calculate_similarity(actual_answer, expected_answer)\n",
    "\n",
    "    response_scores.append({\n",
    "        \"query\": query,\n",
    "        \"expected_answer\": expected_answer,\n",
    "        \"actual_answer\": actual_answer,\n",
    "        \"diff_score\": diff_score,\n",
    "        \"levenshtein_score\": levenshtein_score,\n",
    "        \"rouge_score\": rouge_score,\n",
    "        \"bleu_score\": bleu_score,\n",
    "        \"meteor_score\": meteor_score,\n",
    "        \"similarity_score\": similarity_score\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response_scores_df = pd.DataFrame(response_scores)\n",
    "display(response_scores_df)\n",
    "\n",
    "GENERATION_METRICS = [col for col in response_scores_df.columns if \"score\" in col]\n",
    "\n",
    "\n",
    "print(\"\\nMean Overall Generation Scores:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].mean()).T)\n",
    "\n",
    "print(\"\\nOverall Generation Score Statistics:\")\n",
    "display(pd.DataFrame(response_scores_df[GENERATION_METRICS].describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an LLM as a Response Judge\n",
    "\n",
    "Some metrics cannot be defined objectively and are particularly useful for more subjective or complex criteria.\n",
    "We care about correctness, faithfulness, and relevance.\n",
    "\n",
    "- **Answer Correctness** - Is the generated answer correct compared to the reference and thoroughly answers the user's query?\n",
    "- **Answer Relevancy** - Is the generated answer relevant and comprehensive?\n",
    "- **Answer Factfulness** - Is the generated answer factually consistent with the context document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CORRECTNESS_EVAL_PROMPT =\"\"\"\n",
    "You are a Weight & Biases support expert tasked with evaluating the correctness of answers to questions asked by users to a technical support chatbot. \n",
    "You are tasked with judging the correctness of a generated answer based on the user's query, and a reference answer.\n",
    "\n",
    "You will be given the following information:\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\n",
    "<reference_answer>\n",
    "{reference_answer}\n",
    "</reference_answer>\n",
    "\n",
    "<generated_answer>\n",
    "{generated_answer}\n",
    "</generated_answer>\n",
    "\n",
    "Important Instruction: To evaluate the generated answer, follow these steps:\n",
    "\n",
    "1. Intent Analysis: Consider the underlying intent of the query.\n",
    "2. Relevance: Check if the generated answer addresses all aspects of the question.\n",
    "3. Accuracy: Compare the generated answer to the reference answer for completeness and correctness.\n",
    "4. Trustworthiness: Measure how trustworthy the generated answer is when compared to the reference.\n",
    "\n",
    "Assign a score on an integer scale of 0 to 3 with the following meanings:\n",
    "- 0 = The generated answer is incorrect and does not satisfy any of the criteria.\n",
    "- 1 = The generated answer is partially correct, contains mistakes or is not factually correct.\n",
    "- 2 = The generated answer is correct but includes some extra information, is incomplete or misses some evaluation criteria.\n",
    "- 3 = The generated answer is correct, completely answers the query, does not contain any mistakes, and is factually consistent with the reference answer.\n",
    "\n",
    "After your analysis, provide your verdict in the following JSON format:\n",
    "\n",
    "{{\n",
    "    \"reason\": \"<<Provide a brief explanation for your decision here>>\",\n",
    "    \"final_score\": <<Provide a score as per the above guidelines>>,\n",
    "    \"decision\": \"<<Provide your final decision here, either 'correct' or 'incorrect'>>\"\n",
    "}}\n",
    "\n",
    "Here are some examples of correct output:\n",
    "\n",
    "Example 1:\n",
    "{{\n",
    "    \"reason\": \"The generated answer has the exact details as the reference answer and completely answers the user's query.\",\n",
    "    \"final_score\": 3,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 2:\n",
    "{{\n",
    "    \"reason\": \"The generated answer doesn't match the reference answer and deviates from the user's query.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Example 3:\n",
    "{{\n",
    "    \"reason\": \"The generated answer follows the same steps as the reference answer. However, it includes assumptions about functions that are not requested in the user's query\",\n",
    "    \"final_score\": 2,\n",
    "    \"decision\": \"correct\"\n",
    "}}\n",
    "\n",
    "Example 4:\n",
    "{{\n",
    "    \"reason\": \"The generated answer is incorrect, irrelevant, and not factually correct and completely misses the user's intent.\",\n",
    "    \"final_score\": 0,\n",
    "    \"decision\": \"incorrect\"\n",
    "}}\n",
    "\n",
    "Please provide your evaluation based on the given information and format your response according to the specified JSON structure.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cohere.AsyncClient(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "async def evaluate_correctness_using_llm_judge(query: str, reference_answer: str, generated_answer: str) -> int:\n",
    "    response = await client.chat(\n",
    "        message=CORRECTNESS_EVAL_PROMPT.format(query=query, reference_answer=reference_answer, generated_answer=generated_answer),\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_correctness_evaluation_using_llm(response_scores):\n",
    "    tasks = []\n",
    "    for row in response_scores:\n",
    "        query = row[\"query\"]\n",
    "        expected_answer = row[\"expected_answer\"]\n",
    "        generated_answer = row[\"actual_answer\"]\n",
    "        tasks.append(evaluate_correctness_using_llm_judge(query, expected_answer, generated_answer))\n",
    "    scores = await asyncio.gather(*tasks)\n",
    "    scores = list(map(json.loads, scores))\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_correctness_results = asyncio.run(run_correctness_evaluation_using_llm(response_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_eval_df = pd.DataFrame(llm_judge_correctness_results)\n",
    "response_evals_df = pd.concat([response_scores_df, correctness_eval_df], axis=1)\n",
    "response_evals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_evals_df.iloc[14].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement the `Relevance` and `Faithfulness` evaluators and evaluate the pipeline on all the dimensions.\n",
    "2. Generate and share a W&B report with the following sections in the form of tables and charts:\n",
    "    \n",
    "    - Summary of the evaluation\n",
    "    - Retreival Evaluations\n",
    "        - IR Metrics\n",
    "        - LLM As a Retrieval Judge Metric\n",
    "    - Response Evalations\n",
    "        - Traditional NLP Metrics\n",
    "        - LLM Judgement Metrics\n",
    "    - Overall Evalations\n",
    "    - Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
