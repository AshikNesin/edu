{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/rag-irl/rag-advanced/notebooks/Chapter05.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-05} -->\n",
    "\n",
    "## Retrieval and Re-ranking\n",
    "\n",
    "In this chapter, we will focus on improving the retrieval quality and employ reranking to select the best context for our LLM.\n",
    "\n",
    "In the last chapter, we added Query Enhancement module which was able to derive sub-queries from the user query. We retrieved contexts for these sub-queries. We did a basic deduplication of the total context to reduce the same context. However on a semantic level there might be few more contexts that we can drop thus reducing the input token cost.\n",
    "\n",
    "When the same context is retrieved a few times, we can use this knowledge to fuse the confidence scores of such contexts and ask our LLM to focus more on them. Once can employ many strategies to improve the quality of contexts we provide our LLM.\n",
    "\n",
    "Let's see a few here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import asyncio\n",
    "\n",
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "weave_client = weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the chunked data from chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = weave.ref('semantic_chunked_data:v0').get()\n",
    "\n",
    "chunked_data.rows[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding based retriever\n",
    "\n",
    "In the past chapters we were using TF-IDF and BM25 based retrievers. The vector representation from these methods are not \"dense\" i.e, they were not trained on billions or trillions of tokens. LLMs today embed the tokens into a dense representation. These embeddings are learned during the training process and are used to represent the tokens in the high-dimensional vector space. They have a denser knowledge of different lingustic patterns.\n",
    "\n",
    "One stratightforward way of improving our retriever is to use a `DenseRetriever` which uses the embedding model (Cohere embedding model here) to embed the chunks and uses the same embedding model to embed the query.\n",
    "\n",
    "## Reranking the contexts\n",
    "\n",
    "With more contexts it is important to pick the one that adds more knowledge about the given query. For this, a re-ranking model is used that calculates a matching score for a given query and document pair. This score can then be utilized to rearrange vector search results, ensuring that the most relevant results are prioritized at the top of the list. Cohere comes with it's own re-ranking model and is quite popular.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retriever import DenseRetriever, DenseRetrieverWithReranker\n",
    "from scripts.reranker import CohereReranker\n",
    "from scripts.utils import display_source\n",
    "\n",
    "display_source(DenseRetriever)\n",
    "display_source(CohereReranker)\n",
    "display_source(DenseRetrieverWithReranker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the `DenseRetrieverWithReranker` and index the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = DenseRetrieverWithReranker()\n",
    "dense_retriever.index_data(chunked_data.rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the retriever with this new retriever and reranker on our `RETRIEVAL_METRICS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retrieval_metrics import ALL_METRICS as RETRIEVAL_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:Qj4IFICc2EbdXu5A5UuhkPiWgxM1GvJMIvXEyv1DYnM\"\n",
    ").get()\n",
    "\n",
    "len(eval_dataset.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_evaluation = weave.Evaluation(\n",
    "    name=\"Retrieval_Evaluation\",\n",
    "    dataset=eval_dataset,\n",
    "    scorers=RETRIEVAL_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"], \"top_k\": 10, \"top_n\": 5},\n",
    ")\n",
    "dense_retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(dense_retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RAG pipeline end-to-end [WIP]\n",
    "\n",
    "In this evaluation, we will be using our `QueryEnhancer` from chapter 4 and use the new dense retriever which also uses a Cohere reranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the query enhancer, response generator, and RAG pipeline from the previous chapter\n",
    "\n",
    "import cohere\n",
    "from scripts.query_enhancer import QueryEnhancer\n",
    "from scripts.response_generator import QueryEnhanedResponseGenerator\n",
    "from scripts.rag_pipeline import QueryEnhancedRAGPipeline\n",
    "\n",
    "query_enhancer = QueryEnhancer()\n",
    "# lets add the new prompt\n",
    "QUERY_ENHANCED_PROMPT = open(\"prompts/query_enhanced_system.txt\").read()\n",
    "\n",
    "response_generator = QueryEnhanedResponseGenerator(\n",
    "    model=\"command-r-plus\", prompt=QUERY_ENHANCED_PROMPT, client=cohere.AsyncClient()\n",
    ")\n",
    "\n",
    "rag_pipeline = QueryEnhancedRAGPipeline(\n",
    "    query_enhancer=query_enhancer,\n",
    "    retriever=dense_retriever,\n",
    "    response_generator=response_generator,\n",
    "    top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.response_metrics import ALL_METRICS as RESPONSE_METRICS\n",
    "\n",
    "response_evaluations = weave.Evaluation(\n",
    "    name=\"Response_Evaluation\",\n",
    "    dataset=eval_dataset.rows[10:],\n",
    "    scorers=RESPONSE_METRICS,\n",
    "    preprocess_model_input=lambda x: {\"query\": x[\"question\"]},\n",
    ")\n",
    "query_enhanced_response_scores = asyncio.run(\n",
    "    response_evaluations.evaluate(rag_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retriever\n",
    "\n",
    "Even though BM25 is an old model used for retrieval tasks, it is still the state-of-the-art on various benchmark. In machine learning, we ensemble a few weak classifiers to build a stronger classifier, we can adopt the same idea to our retriever pipeline.\n",
    "\n",
    "Below we show the concept of hybrid retriever which uses two or more retrievers and retrievr chunks from all of them followed by re-ranking.\n",
    "\n",
    "### Fusion reranking\n",
    "\n",
    "Since the indiviual retrievers are gonna retrieve the same chunks most of the time, simple re-ranking will not be beneficial since the same chunk from each retriever will have similar score, thus the top_k after re-ranking will have more or less the same context. \n",
    "\n",
    "Instead, we iterate through all the chunks and fuse the score of similar chunk. Finally we sort on the basis of this fused score and return top_k chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.retriever import HybridRetrieverReranker\n",
    "display_source(HybridRetrieverReranker)\n",
    "\n",
    "hybrid_retriever = HybridRetrieverReranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever.index_data(chunked_data.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retrieval_scores = asyncio.run(retrieval_evaluation.evaluate(hybrid_retriever))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
