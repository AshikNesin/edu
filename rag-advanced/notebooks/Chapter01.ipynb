{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "\n",
    "**Set up a basic RAG pipeline (BM25/TFIDF + simple QA model)**\n",
    "\n",
    "In this chapter, we will learn about building a simple RAG pipeline. We will mainly focus on how to preprocess and chunk the data followed by building a simple retrieval engine without using any fancy \"Vector Index\". The idea is to show the inner working of a retrieval pipeline and make you understand the workflow from a user query to a generated response using an LLM.\n",
    "\n",
    "For this chapter you will need a Cohere API key.\n",
    "\n",
    "Create an `.env` file in the `notebooks` directory. The file's content is:\n",
    "\n",
    "```\n",
    "CO_API_KEY=\"<your-cohere-api-key>\"\n",
    "```\n",
    "`.env` combined with `dotenv` allows for better API keys management in notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import wandb\n",
    "import cohere\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will start a Weights and Biases (W&B) run.\n",
    "\n",
    "Throughout this notebook, W&B will be used to store, version, and download text files, creating a lineage. We will begin with an already uploaded raw data file as a [W&B Artifact](https://docs.wandb.ai/guides/artifacts), download it, inspect it, and devise a chunking strategy. Finally, we will upload the processed documents back to W&B as an Artifact, establishing a lineage (DAG) between the raw and processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparambharat\u001b[0m (\u001b[33mrag-course\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mugan/data/wandb/projects/edu/rag-advanced/notebooks/wandb/run-20240708_123628-c7cqe26q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rag-course/dev/runs/c7cqe26q' target=\"_blank\">wild-music-26</a></strong> to <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rag-course/dev' target=\"_blank\">https://wandb.ai/rag-course/dev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rag-course/dev/runs/c7cqe26q' target=\"_blank\">https://wandb.ai/rag-course/dev/runs/c7cqe26q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WANDB_ENTITY = \"rag-course\"\n",
    "WANDB_PROJECT = \"dev\"\n",
    "\n",
    "wandb.require(\"core\")\n",
    "\n",
    "run = wandb.init(\n",
    "    entity=WANDB_ENTITY,\n",
    "    project=WANDB_PROJECT,\n",
    "    group=\"Chapter 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove this once we more to the final project\n",
    "# documents_artifact = wandb.Artifact(\n",
    "#     name=\"wandb_docs\",\n",
    "#     type=\"dataset\",\n",
    "#     description=\"W&B Documentation in Markdown format\",\n",
    "#     metadata={\n",
    "#         \"total_files\": 380,\n",
    "#         \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# documents_artifact.add_dir(\"../data/wandb_docs\")\n",
    "# run.log_artifact(documents_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Use [W&B Artifacts](https://docs.wandb.ai/guides/artifacts) to track and version data as the inputs and outputs of your W&B Runs. For example, a model training run might take in a dataset as input and produce a trained model as output. W&B Artifact is a powerful object storage with rich UI functionalities.\n",
    "\n",
    "Below we are downloading an artifact named `wandb_docs` which will download 380 markdown files in your `../data/wandb_docs` directory. This is our data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/08 12:36:30 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/0z2t11h3/artifact/936064166/wandb_manifest.json?Expires=1720425990&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=GOjDRxHCR6SW%2FDPTIBZwZiH%2Frwi%2FXwsWw4YgrIbYprSNj2Cc%2FImfPK0kbkGgm6XVQMg96G02gF3p%2FDykJfu8%2FfV3rJARpwem9uxOufltVI%2B2wP2V3vkBf8QLz%2BSAWYSLK%2Ftwl4KHoVirr4iIrgXJT7bLXfy%2FMUwRJCvmDGG8YQlHoTmwpmuOC1ClwFJo8a1MV9Qd6MUz5CYOpW1SrVr8G%2FjVEbwhMFhCFYFQ%2B0h98NRsDty3Sae16%2FrgWvixiMA8kP84tEtYdrwUrAzy%2B8cGR0ddF36Urgn1OPtvseGswi8%2F0xivECf1z54ZhfJfwxYFZTWwH9ad8EETSZSghT0Y5w%3D%3D\n"
     ]
    }
   ],
   "source": [
    "documents_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/wandb_docs:latest\", type=\"dataset\"\n",
    ")\n",
    "data_dir = \"../data/wandb_docs\"\n",
    "\n",
    "docs_dir = documents_artifact.download(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the `../data/wandb_docs` directory below, we see that we have downloaded 380 files. The first 5 files are all in markdown (`.md` file format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 380\n",
      "\n",
      "First 5 files:\n",
      "../data/wandb_docs/guides/app/features/anon.md\n",
      "../data/wandb_docs/guides/app/features/custom-charts/intro.md\n",
      "../data/wandb_docs/guides/app/features/custom-charts/walkthrough.md\n",
      "../data/wandb_docs/guides/app/features/intro.md\n",
      "../data/wandb_docs/guides/app/features/notes.md\n"
     ]
    }
   ],
   "source": [
    "docs_dir = pathlib.Path(docs_dir)\n",
    "docs_files = sorted(docs_dir.rglob(\"*.md\"))\n",
    "\n",
    "print(f\"Number of files: {len(docs_files)}\\n\")\n",
    "print(\"First 5 files:\\n{files}\".format(files=\"\\n\".join(map(str, docs_files[:5]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at an example file below. We take the first element of the list (`docs_files`) and use a convenient `Path.read_text` method which returns the decoded contents of the pointed-to file as a string.\n",
    "\n",
    "💡 Looking at the example, we see some format to it. While building an ingestion pipeline, it is a good practice to look through few examples to see if there is any pattern to your data source. It helps to come up with better preprocessing steps and chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "description: Log and visualize data without a W&B account\n",
      "displayed_sidebar: default\n",
      "---\n",
      "\n",
      "# Anonymous Mode\n",
      "\n",
      "Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first.\n",
      "\n",
      "Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)`\n",
      "\n",
      ":::info\n",
      "**Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com.\n",
      ":::\n",
      "\n",
      "### How does someone without an account see results?\n",
      "\n",
      "If someone runs your script and you have to set `anonymous=\"allow\"`:\n",
      "\n",
      "1. **Auto-create temporary account:** W&B checks for an account that's already signed in. If there's no account, we automatically create a new anonymous account and save that API key for the session.\n",
      "2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days.\n",
      "3. **Claim data when it's useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don't claim a run, it will be deleted after 7 days.\n",
      "\n",
      ":::caution\n",
      "**Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you're trying to share results publicly, but hide the author's identity, please contact us at support@wandb.com to share more about your use case.\n",
      ":::\n",
      "\n",
      "### What happens to users with existing accounts?\n",
      "\n",
      "If you set `anonymous=\"allow\"` in your script, we will check to make sure there's not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run.\n",
      "\n",
      "### What are features that aren't available to anonymous users?\n",
      "\n",
      "*   **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account.\n",
      "\n",
      "\n",
      "![](@site/static/images/app_ui/anon_mode_no_data.png)\n",
      "\n",
      "*   **No artifact logging**: Runs will print a warning on the command line that you can't log an artifact to an anonymous run.\n",
      "\n",
      "![](@site/static/images/app_ui/anon_example_warning.png)\n",
      "\n",
      "* **No profile or settings pages**: Certain pages aren't available in the UI, because they're only useful for real accounts.\n",
      "\n",
      "## Example usage\n",
      "\n",
      "[Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works.\n",
      "\n",
      "```python\n",
      "import wandb\n",
      "\n",
      "# Start a run allowing anonymous accounts\n",
      "wandb.init(anonymous=\"allow\")\n",
      "\n",
      "# Log results from your training loop\n",
      "wandb.log({\"acc\": 0.91})\n",
      "\n",
      "# Mark the run as finished\n",
      "wandb.finish()\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs_files[0].read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are storing files as dictionaries with content (raw text) and metadata. Metadata is extra information for that data point which can be used to group together similar data points, or filter out a few data points. We will see in future chapters the importance of metadata and why it should not be ignored while building the ingestion pipeline.\n",
    "\n",
    "The metadata can be derived (`raw_tokens`) or is inherent (`source`) to the data point.\n",
    "\n",
    "Note that we are simply doing word counting and calling it `raw_tokens`. In practice we would be using the [tiktoken tokenizer](https://github.com/openai/tiktoken) to calculate the token counts but this naive calculation is an okay approximation for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '---\\ndescription: Log and visualize data without a W&B account\\ndisplayed_sidebar: default\\n---\\n\\n# Anonymous Mode\\n\\nAre you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first.\\n\\nAllow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)`\\n\\n:::info\\n**Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com.\\n:::\\n\\n### How does someone without an account see results?\\n\\nIf someone runs your script and you have to set `anonymous=\"allow\"`:\\n\\n1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session.\\n2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days.\\n3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days.\\n\\n:::caution\\n**Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case.\\n:::\\n\\n### What happens to users with existing accounts?\\n\\nIf you set `anonymous=\"allow\"` in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run.\\n\\n### What are features that aren\\'t available to anonymous users?\\n\\n*   **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account.\\n\\n\\n![](@site/static/images/app_ui/anon_mode_no_data.png)\\n\\n*   **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run.\\n\\n![](@site/static/images/app_ui/anon_example_warning.png)\\n\\n* **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts.\\n\\n## Example usage\\n\\n[Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works.\\n\\n```python\\nimport wandb\\n\\n# Start a run allowing anonymous accounts\\nwandb.init(anonymous=\"allow\")\\n\\n# Log results from your training loop\\nwandb.log({\"acc\": 0.91})\\n\\n# Mark the run as finished\\nwandb.finish()\\n```\\n',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md', 'raw_tokens': 470}},\n",
       " {'content': '---\\nslug: /guides/app/features/custom-charts\\ndisplayed_sidebar: default\\n---\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\n\\n# Custom Charts\\n\\nUse **Custom Charts** to create charts that aren\\'t possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of [Vega](https://vega.github.io/vega/).\\n\\n* **What\\'s possible**: Read the[ launch announcement →](https://wandb.ai/wandb/posts/reports/Announcing-the-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg)\\n* **Code**: Try a live example in a[ hosted notebook →](https://tiny.cc/custom-charts)\\n* **Video**: Watch a quick [walkthrough video →](https://www.youtube.com/watch?v=3-N9OV6bkSM)\\n* **Example**: Quick Keras and Sklearn [demo notebook →](https://colab.research.google.com/drive/1g-gNGokPWM2Qbc8p1Gofud0\\\\_5AoZdoSD?usp=sharing)\\n\\n![Supported charts from vega.github.io/vega](/images/app_ui/supported_charts.png)\\n\\n### How it works\\n\\n1. **Log data**: From your script, log [config](../../../../guides/track/config.md) and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table`\\n2. **Customize the chart**: Pull in any of this logged data with a [GraphQL](https://graphql.org) query. Visualize the results of your query with [Vega](https://vega.github.io/vega/), a powerful visualization grammar.\\n3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`.\\n\\n![](/images/app_ui/pr_roc.png)\\n\\n## Log charts from a script\\n\\n### Builtin presets\\n\\nThese presets have builtin `wandb.plot` methods that make it fast to log charts directly from your script and see the exact visualizations you\\'re looking for in the UI.\\n\\n<Tabs\\n  defaultValue=\"line-plot\"\\n  values={[\\n    {label: \\'Line plot\\', value: \\'line-plot\\'},\\n    {label: \\'Scatter plot\\', value: \\'scatter-plot\\'},\\n    {label: \\'Bar chart\\', value: \\'bar-chart\\'},\\n    {label: \\'Histogram\\', value: \\'histogram\\'},\\n    {label: \\'PR curve\\', value: \\'pr-curve\\'},\\n    {label: \\'ROC curve\\', value: \\'roc-curve\\'},\\n  ]}>\\n  <TabItem value=\"line-plot\">\\n\\n`wandb.plot.line()`\\n\\nLog a custom line plot—a list of connected and ordered points (x,y) on arbitrary axes x and y.\\n\\n```python\\ndata = [[x, y] for (x, y) in zip(x_values, y_values)]\\ntable = wandb.Table(data=data, columns=[\"x\", \"y\"])\\nwandb.log(\\n    {\\n        \"my_custom_plot_id\": wandb.plot.line(\\n            table, \"x\", \"y\", title=\"Custom Y vs X Line Plot\"\\n        )\\n    }\\n)\\n```\\n\\nYou can use this to log curves on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y).\\n\\n![](/images/app_ui/line_plot.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Line-Plots--VmlldzoyNjk5NTA)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n  <TabItem value=\"scatter-plot\">\\n\\n`wandb.plot.scatter()`\\n\\nLog a custom scatter plot—a list of points (x, y) on a pair of arbitrary axes x and y.\\n\\n```python\\ndata = [[x, y] for (x, y) in zip(class_x_prediction_scores, class_y_prediction_scores)]\\ntable = wandb.Table(data=data, columns=[\"class_x\", \"class_y\"])\\nwandb.log({\"my_custom_id\": wandb.plot.scatter(table, \"class_x\", \"class_y\")})\\n```\\n\\nYou can use this to log scatter points on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y).\\n\\n![](/images/app_ui/demo_scatter_plot.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Scatter-Plots--VmlldzoyNjk5NDQ)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n  <TabItem value=\"bar-chart\">\\n\\n`wandb.plot.bar()`\\n\\nLog a custom bar chart—a list of labeled values as bars—natively in a few lines:\\n\\n```python\\ndata = [[label, val] for (label, val) in zip(labels, values)]\\ntable = wandb.Table(data=data, columns=[\"label\", \"value\"])\\nwandb.log(\\n    {\\n        \"my_bar_chart_id\": wandb.plot.bar(\\n            table, \"label\", \"value\", title=\"Custom Bar Chart\"\\n        )\\n    }\\n)\\n```\\n\\nYou can use this to log arbitrary bar charts. Note that the number of labels and values in the lists must match exactly (i.e. each data point must have both).\\n\\n![](@site/static/images/app_ui/line_plot_bar_chart.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Bar-Charts--VmlldzoyNzExNzk)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n  <TabItem value=\"histogram\">\\n\\n`wandb.plot.histogram()`\\n\\nLog a custom histogram—sort list of values into bins by count/frequency of occurrence—natively in a few lines. Let\\'s say I have a list of prediction confidence scores (`scores`) and want to visualize their distribution:\\n\\n```python\\ndata = [[s] for s in scores]\\ntable = wandb.Table(data=data, columns=[\"scores\"])\\nwandb.log({\"my_histogram\": wandb.plot.histogram(table, \"scores\", title=None)})\\n```\\n\\nYou can use this to log arbitrary histograms. Note that `data` is a list of lists, intended to support a 2D array of rows and columns.\\n\\n![](/images/app_ui/demo_custom_chart_histogram.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Histograms--VmlldzoyNzE0NzM)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n    <TabItem value=\"pr-curve\">\\n\\n`wandb.plot.pr_curve()`\\n\\nCreate a [Precision-Recall curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision\\\\_recall\\\\_curve.html#sklearn.metrics.precision\\\\_recall\\\\_curve) in one line:\\n\\n```python\\nplot = wandb.plot.pr_curve(ground_truth, predictions, labels=None, classes_to_plot=None)\\n\\nwandb.log({\"pr\": plot})\\n```\\n\\nYou can log this whenever your code has access to:\\n\\n* a model\\'s predicted scores (`predictions`) on a set of examples\\n* the corresponding ground truth labels (`ground_truth`) for those examples\\n* (optionally) a list of the labels/class names (`labels=[\"cat\", \"dog\", \"bird\"...]` if label index 0 means cat, 1 = dog, 2 = bird, etc.)\\n* (optionally) a subset (still in list format) of the labels to visualize in the plot\\n\\n![](/images/app_ui/demo_average_precision_lines.png)\\n\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Plot-Precision-Recall-Curves--VmlldzoyNjk1ODY)\\n\\n[Run the code →](https://colab.research.google.com/drive/1mS8ogA3LcZWOXchfJoMrboW3opY1A8BY?usp=sharing)\\n\\n  </TabItem>\\n  <TabItem value=\"roc-curve\">\\n\\n`wandb.plot.roc_curve()`\\n\\nCreate an [ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc\\\\_curve.html#sklearn.metrics.roc\\\\_curve) in one line:\\n\\n```python\\nplot = wandb.plot.roc_curve(\\n    ground_truth, predictions, labels=None, classes_to_plot=None\\n)\\n\\nwandb.log({\"roc\": plot})\\n```\\n\\nYou can log this whenever your code has access to:\\n\\n* a model\\'s predicted scores (`predictions`) on a set of examples\\n* the corresponding ground truth labels (`ground_truth`) for those examples\\n* (optionally) a list of the labels/ class names (`labels=[\"cat\", \"dog\", \"bird\"...]` if label index 0 means cat, 1 = dog, 2 = bird, etc.)\\n* (optionally) a subset (still in list format) of these labels to visualize on the plot\\n\\n![](/images/app_ui/demo_custom_chart_roc_curve.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Plot-ROC-Curves--VmlldzoyNjk3MDE)\\n\\n[Run the code →](https://colab.research.google.com/drive/1\\\\_RMppCqsA8XInV\\\\_jhJz32NCZG6Z5t1RO?usp=sharing)\\n\\n  </TabItem>\\n</Tabs>\\n\\n### Custom presets\\n\\nTweak a builtin preset, or create a new preset, then save the chart. Use the chart ID to log data to that custom preset directly from your script.\\n\\n```python\\n# Create a table with the columns to plot\\ntable = wandb.Table(data=data, columns=[\"step\", \"height\"])\\n\\n# Map from the table\\'s columns to the chart\\'s fields\\nfields = {\"x\": \"step\", \"value\": \"height\"}\\n\\n# Use the table to populate the new custom chart preset\\n# To use your own saved chart preset, change the vega_spec_name\\nmy_custom_chart = wandb.plot_table(\\n    vega_spec_name=\"carey/new_chart\",\\n    data_table=table,\\n    fields=fields,\\n)\\n```\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n![](/images/app_ui/custom_presets.png)\\n\\n## Log data\\n\\nHere are the data types you can log from your script and use in a custom chart:\\n\\n* **Config**: Initial settings of your experiment (your independent variables). This includes any named fields you\\'ve logged as keys to `wandb.config` at the start of your training (e.g. `wandb.config.learning_rate = 0.0001)`\\n* **Summary**: Single values logged during training (your results or dependent variables), e.g. `wandb.log({\"val_acc\" : 0.8})`. If you write to this key multiple times during training via `wandb.log()`, the summary is set to the final value of that key.\\n* **History**: The full time series of the logged scalar is available to the query via the `history` field\\n* **summaryTable**: If you need to log a list of multiple values, use a `wandb.Table()` to save that data, then query it in your custom panel.\\n* **historyTable**: If you need to see the history data, then query `historyTable` in your custom chart panel. Each time you call `wandb.Table()` or log a custom chart, you\\'re creating a new table in history for that step.\\n\\n### How to log a custom table\\n\\nUse `wandb.Table()` to log your data as a 2D array. Typically each row of this table represents one data point, and each column denotes the relevant fields/dimensions for each data point which you\\'d like to plot. As you configure a custom panel, the whole table will be accessible via the named key passed to `wandb.log()`(\"custom\\\\_data\\\\_table\" below), and the individual fields will be accessible via the column names (\"x\", \"y\", and \"z\"). You can log tables at multiple time steps throughout your experiment. The maximum size of each table is 10,000 rows.\\n\\n[Try it in a Google Colab →](https://tiny.cc/custom-charts)\\n\\n```python\\n# Logging a custom table of data\\nmy_custom_data = [[x1, y1, z1], [x2, y2, z2]]\\nwandb.log(\\n    {\"custom_data_table\": wandb.Table(data=my_custom_data, columns=[\"x\", \"y\", \"z\"])}\\n)\\n```\\n\\n## Customize the chart\\n\\nAdd a new custom chart to get started, then edit the query to select data from your visible runs. The query uses [GraphQL](https://graphql.org) to fetch data from the config, summary, and history fields in your runs.\\n\\n![Add a new custom chart, then edit the query](/images/app_ui/customize_chart.gif)\\n\\n### Custom visualizations\\n\\nSelect a **Chart** in the upper right corner to start with a default preset. Next, pick **Chart fields** to map the data you\\'re pulling in from the query to the corresponding fields in your chart. Here\\'s an example of selecting a metric to get from the query, then mapping that into the bar chart fields below.\\n\\n![Creating a custom bar chart showing accuracy across runs in a project](/images/app_ui/demo_make_a_custom_chart_bar_chart.gif)\\n\\n### How to edit Vega\\n\\nClick **Edit** at the top of the panel to go into [Vega](https://vega.github.io/vega/) edit mode. Here you can define a [Vega specification](https://vega.github.io/vega/docs/specification/) that creates an interactive chart in the UI. You can change any aspect of the chart, from the visual style (e.g. change the title, pick a different color scheme, show curves as a series of points instead of as connected lines) to the data itself (use a Vega transform to bin an array of values into a histogram, etc.). The panel preview will update interactively, so you can see the effect of your changes as you edit the Vega spec or query. The [Vega documentation and tutorials ](https://vega.github.io/vega/)are an excellent source of inspiration.\\n\\n**Field references**\\n\\nTo pull data into your chart from W&B, add template strings of the form `\"${field:<field-name>}\"` anywhere in your Vega spec. This will create a dropdown in the **Chart Fields** area on the right side, which users can use to select a query result column to map into Vega.\\n\\nTo set a default value for a field, use this syntax: `\"${field:<field-name>:<placeholder text>}\"`\\n\\n### Saving chart presets\\n\\nApply any changes to a specific visualization panel with the button at the bottom of the modal. Alternatively, you can save the Vega spec to use elsewhere in your project. To save the reusable chart definition, click **Save as** at the top of the Vega editor and give your preset a name.\\n\\n## Articles and guides\\n\\n1. [The W&B Machine Learning Visualization IDE](https://wandb.ai/wandb/posts/reports/The-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg)\\n2. [Visualizing NLP Attention Based Models](https://wandb.ai/kylegoyette/gradientsandtranslation2/reports/Visualizing-NLP-Attention-Based-Models-Using-Custom-Charts--VmlldzoyNjg2MjM)\\n3. [Visualizing The Effect of Attention on Gradient Flow](https://wandb.ai/kylegoyette/gradientsandtranslation/reports/Visualizing-The-Effect-of-Attention-on-Gradient-Flow-Using-Custom-Charts--VmlldzoyNjg1NDg)\\n4. [Logging arbitrary curves](https://wandb.ai/stacey/presets/reports/Logging-Arbitrary-Curves--VmlldzoyNzQyMzA)\\n\\n## Frequently asked questions\\n\\n### Coming soon\\n\\n* **Polling**: Auto-refresh of data in the chart\\n* **Sampling**: Dynamically adjust the total number of points loaded into the panel for efficiency\\n\\n### Gotchas\\n\\n* Not seeing the data you\\'re expecting in the query as you\\'re editing your chart? It might be because the column you\\'re looking for is not logged in the runs you have selected. Save your chart and go back out to the runs table, and select the runs you\\'d like to visualize with the **eye** icon.\\n\\n### How to show a \"step slider\" in a custom chart?\\n\\nThis can be enabled on the “Other settings” page of the custom chart editor. If you change your query to use a `historyTable` instead of the `summaryTable`, you\\'ll get an option to “Show step selector” in the custom chart editor. This gives you a slider that lets you select the step.\\n\\n<!-- ![Show step slider in a custom chart](/images/app_ui/step_sllider_custon_charts.mov>) -->\\n\\n### How to delete a custom chart preset?\\n\\nYou can do this by going into the custom chart editor. Then click on the currently selected chart type, this will open up a menu with all your presets. Hover the mouse on a preset you want to delete and then click on the Trash icon.\\n\\n![](/images/app_ui/delete_custome_chart_preset.gif)\\n\\n\\n### Common use cases\\n\\n* Customize bar plots with error bars\\n* Show model validation metrics which require custom x-y coordinates (like precision-recall curves)\\n* Overlay data distributions from two different models/experiments as histograms\\n* Show changes in a metric via snapshots at multiple points during training\\n* Create a unique visualization not yet available in W&B (and hopefully share it with the world)\\n',\n",
       "  'metadata': {'source': 'guides/app/features/custom-charts/intro.md',\n",
       "   'raw_tokens': 1904}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll store the files as dictionaries with some content and metadata\n",
    "data = []\n",
    "for file in docs_files:\n",
    "    content = file.read_text()\n",
    "    data.append(\n",
    "        {\n",
    "            \"content\": content,\n",
    "            \"metadata\": {\n",
    "                \"source\": str(file.relative_to(docs_dir)),\n",
    "                \"raw_tokens\": len(content.split()),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the total number of tokens of your data source is a good practice. In this case, the total tokens is more than 200k. Surely, most LLM providers cannot process these many tokens. Building a RAG is justified in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens in dataset: 246998\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], data))\n",
    "print(f\"Total Tokens in dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created list of dictionaries with `content` and `metadata` will now be logged as a W&B Artifact called `raw_data`. We started with the `wandb_docs` artifact and now we are are logging the processed data as `raw_data` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact raw_data>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's store the raw data in an artifact for future use and reproducibility\n",
    "raw_artifact = wandb.Artifact(\n",
    "    name=\"raw_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Raw wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_tokens,\n",
    "    },\n",
    ")\n",
    "with raw_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(raw_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking the data\n",
    "\n",
    "Each document contains a large number of tokens, so we need to split it into smaller chunks to manage the token count per chunk. This approach serves three main purposes:\n",
    "\n",
    "* Most embedding models have a limit of 512 tokens per input (based on the tokenizer used during their training).\n",
    "\n",
    "* Chunking allows us to retrieve and send only the most relevant portions to our LLM, significantly reducing the total token count. This helps keep the LLM’s cost and processing time manageable.\n",
    "\n",
    "* When the text is small-sized, embedding models tend to generate better vectors as they can capture more fine-grained details and nuances in the text, resulting in more accurate representations.\n",
    "\n",
    "Below we are chunking each content (text) to a maximum length of 500 tokens (`CHUNK_SIZE`). We are not overlapping (`CHUNK_OVERLAP`) the content of one chunk with another chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are hyperparameters of our ingestion pipeline\n",
    "\n",
    "CHUNK_SIZE = 300\n",
    "CHUNK_OVERLAP = 0\n",
    "\n",
    "\n",
    "def split_into_chunks(\n",
    "    text: str, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP\n",
    ") -> List[str]:\n",
    "    \"\"\"Function to split the text into chunks of a maximum number of tokens\n",
    "    ensure that the chunks are of size CHUNK_SIZE and overlap by chunk_overlap tokens\n",
    "    use the `tokenizer.encode` method to tokenize the text\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start = end - chunk_overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `raw_data` artifact we pushed as Artifacts earlier. Let's download it. By using `use_artifact` method, we are starting a lineage from the `raw_data` artifact to the new artifact we will create further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/08 12:36:35 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/0z2t11h3/artifact/936065098/wandb_manifest.json?Expires=1720425995&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=ilzIBCFPR5buvy4PTWk8PnFcJ2%2BDIR08OLxXl4BTU1HZ%2BkDJfUKg%2FCxW7biIVYJch0UTFe65xA8ktm%2FMFsEKuQ3ai6iG0WTbr4ZDOyhAVB5QO%2FFbwtofzhzwQ2v8fUO7Ox9cJUPTxl7dONaS7xoJzfHjfNNsvCY8c1dRl1xf7z95ZRj3WYazkYvfskrIX9%2B9YshfuYLJlKfbIGRqujQ2zUFQaDUpBiau2pbcobe%2BNkNziSpxK2P3hduf7omfJOHdhm6cZU1QCwjOdj%2F5W%2FWFew0YDwOCjCRNHZk%2B1eJbvs8b2IIq0LMcGhO8d%2FcWnuF%2FSVA5%2B%2BRnQKVLUzPU%2B9Z8rA%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': '---\\ndescription: Log and visualize data without a W&B account\\ndisplayed_sidebar: default\\n---\\n\\n# Anonymous Mode\\n\\nAre you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first.\\n\\nAllow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)`\\n\\n:::info\\n**Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com.\\n:::\\n\\n### How does someone without an account see results?\\n\\nIf someone runs your script and you have to set `anonymous=\"allow\"`:\\n\\n1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session.\\n2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days.\\n3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days.\\n\\n:::caution\\n**Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case.\\n:::\\n\\n### What happens to users with existing accounts?\\n\\nIf you set `anonymous=\"allow\"` in your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run.\\n\\n### What are features that aren\\'t available to anonymous users?\\n\\n*   **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account.\\n\\n\\n![](@site/static/images/app_ui/anon_mode_no_data.png)\\n\\n*   **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run.\\n\\n![](@site/static/images/app_ui/anon_example_warning.png)\\n\\n* **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts.\\n\\n## Example usage\\n\\n[Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works.\\n\\n```python\\nimport wandb\\n\\n# Start a run allowing anonymous accounts\\nwandb.init(anonymous=\"allow\")\\n\\n# Log results from your training loop\\nwandb.log({\"acc\": 0.91})\\n\\n# Mark the run as finished\\nwandb.finish()\\n```\\n',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md', 'raw_tokens': 470}},\n",
       " {'content': '---\\nslug: /guides/app/features/custom-charts\\ndisplayed_sidebar: default\\n---\\n\\nimport Tabs from \\'@theme/Tabs\\';\\nimport TabItem from \\'@theme/TabItem\\';\\n\\n# Custom Charts\\n\\nUse **Custom Charts** to create charts that aren\\'t possible right now in the default UI. Log arbitrary tables of data and visualize them exactly how you want. Control details of fonts, colors, and tooltips with the power of [Vega](https://vega.github.io/vega/).\\n\\n* **What\\'s possible**: Read the[ launch announcement →](https://wandb.ai/wandb/posts/reports/Announcing-the-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg)\\n* **Code**: Try a live example in a[ hosted notebook →](https://tiny.cc/custom-charts)\\n* **Video**: Watch a quick [walkthrough video →](https://www.youtube.com/watch?v=3-N9OV6bkSM)\\n* **Example**: Quick Keras and Sklearn [demo notebook →](https://colab.research.google.com/drive/1g-gNGokPWM2Qbc8p1Gofud0\\\\_5AoZdoSD?usp=sharing)\\n\\n![Supported charts from vega.github.io/vega](/images/app_ui/supported_charts.png)\\n\\n### How it works\\n\\n1. **Log data**: From your script, log [config](../../../../guides/track/config.md) and summary data as you normally would when running with W&B. To visualize a list of multiple values logged at one specific time, use a custom`wandb.Table`\\n2. **Customize the chart**: Pull in any of this logged data with a [GraphQL](https://graphql.org) query. Visualize the results of your query with [Vega](https://vega.github.io/vega/), a powerful visualization grammar.\\n3. **Log the chart**: Call your own preset from your script with `wandb.plot_table()`.\\n\\n![](/images/app_ui/pr_roc.png)\\n\\n## Log charts from a script\\n\\n### Builtin presets\\n\\nThese presets have builtin `wandb.plot` methods that make it fast to log charts directly from your script and see the exact visualizations you\\'re looking for in the UI.\\n\\n<Tabs\\n  defaultValue=\"line-plot\"\\n  values={[\\n    {label: \\'Line plot\\', value: \\'line-plot\\'},\\n    {label: \\'Scatter plot\\', value: \\'scatter-plot\\'},\\n    {label: \\'Bar chart\\', value: \\'bar-chart\\'},\\n    {label: \\'Histogram\\', value: \\'histogram\\'},\\n    {label: \\'PR curve\\', value: \\'pr-curve\\'},\\n    {label: \\'ROC curve\\', value: \\'roc-curve\\'},\\n  ]}>\\n  <TabItem value=\"line-plot\">\\n\\n`wandb.plot.line()`\\n\\nLog a custom line plot—a list of connected and ordered points (x,y) on arbitrary axes x and y.\\n\\n```python\\ndata = [[x, y] for (x, y) in zip(x_values, y_values)]\\ntable = wandb.Table(data=data, columns=[\"x\", \"y\"])\\nwandb.log(\\n    {\\n        \"my_custom_plot_id\": wandb.plot.line(\\n            table, \"x\", \"y\", title=\"Custom Y vs X Line Plot\"\\n        )\\n    }\\n)\\n```\\n\\nYou can use this to log curves on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y).\\n\\n![](/images/app_ui/line_plot.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Line-Plots--VmlldzoyNjk5NTA)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n  <TabItem value=\"scatter-plot\">\\n\\n`wandb.plot.scatter()`\\n\\nLog a custom scatter plot—a list of points (x, y) on a pair of arbitrary axes x and y.\\n\\n```python\\ndata = [[x, y] for (x, y) in zip(class_x_prediction_scores, class_y_prediction_scores)]\\ntable = wandb.Table(data=data, columns=[\"class_x\", \"class_y\"])\\nwandb.log({\"my_custom_id\": wandb.plot.scatter(table, \"class_x\", \"class_y\")})\\n```\\n\\nYou can use this to log scatter points on any two dimensions. Note that if you\\'re plotting two lists of values against each other, the number of values in the lists must match exactly (i.e. each point must have an x and a y).\\n\\n![](/images/app_ui/demo_scatter_plot.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Scatter-Plots--VmlldzoyNjk5NDQ)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n  <TabItem value=\"bar-chart\">\\n\\n`wandb.plot.bar()`\\n\\nLog a custom bar chart—a list of labeled values as bars—natively in a few lines:\\n\\n```python\\ndata = [[label, val] for (label, val) in zip(labels, values)]\\ntable = wandb.Table(data=data, columns=[\"label\", \"value\"])\\nwandb.log(\\n    {\\n        \"my_bar_chart_id\": wandb.plot.bar(\\n            table, \"label\", \"value\", title=\"Custom Bar Chart\"\\n        )\\n    }\\n)\\n```\\n\\nYou can use this to log arbitrary bar charts. Note that the number of labels and values in the lists must match exactly (i.e. each data point must have both).\\n\\n![](@site/static/images/app_ui/line_plot_bar_chart.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Bar-Charts--VmlldzoyNzExNzk)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n  <TabItem value=\"histogram\">\\n\\n`wandb.plot.histogram()`\\n\\nLog a custom histogram—sort list of values into bins by count/frequency of occurrence—natively in a few lines. Let\\'s say I have a list of prediction confidence scores (`scores`) and want to visualize their distribution:\\n\\n```python\\ndata = [[s] for s in scores]\\ntable = wandb.Table(data=data, columns=[\"scores\"])\\nwandb.log({\"my_histogram\": wandb.plot.histogram(table, \"scores\", title=None)})\\n```\\n\\nYou can use this to log arbitrary histograms. Note that `data` is a list of lists, intended to support a 2D array of rows and columns.\\n\\n![](/images/app_ui/demo_custom_chart_histogram.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Custom-Histograms--VmlldzoyNzE0NzM)\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n  </TabItem>\\n    <TabItem value=\"pr-curve\">\\n\\n`wandb.plot.pr_curve()`\\n\\nCreate a [Precision-Recall curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision\\\\_recall\\\\_curve.html#sklearn.metrics.precision\\\\_recall\\\\_curve) in one line:\\n\\n```python\\nplot = wandb.plot.pr_curve(ground_truth, predictions, labels=None, classes_to_plot=None)\\n\\nwandb.log({\"pr\": plot})\\n```\\n\\nYou can log this whenever your code has access to:\\n\\n* a model\\'s predicted scores (`predictions`) on a set of examples\\n* the corresponding ground truth labels (`ground_truth`) for those examples\\n* (optionally) a list of the labels/class names (`labels=[\"cat\", \"dog\", \"bird\"...]` if label index 0 means cat, 1 = dog, 2 = bird, etc.)\\n* (optionally) a subset (still in list format) of the labels to visualize in the plot\\n\\n![](/images/app_ui/demo_average_precision_lines.png)\\n\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Plot-Precision-Recall-Curves--VmlldzoyNjk1ODY)\\n\\n[Run the code →](https://colab.research.google.com/drive/1mS8ogA3LcZWOXchfJoMrboW3opY1A8BY?usp=sharing)\\n\\n  </TabItem>\\n  <TabItem value=\"roc-curve\">\\n\\n`wandb.plot.roc_curve()`\\n\\nCreate an [ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc\\\\_curve.html#sklearn.metrics.roc\\\\_curve) in one line:\\n\\n```python\\nplot = wandb.plot.roc_curve(\\n    ground_truth, predictions, labels=None, classes_to_plot=None\\n)\\n\\nwandb.log({\"roc\": plot})\\n```\\n\\nYou can log this whenever your code has access to:\\n\\n* a model\\'s predicted scores (`predictions`) on a set of examples\\n* the corresponding ground truth labels (`ground_truth`) for those examples\\n* (optionally) a list of the labels/ class names (`labels=[\"cat\", \"dog\", \"bird\"...]` if label index 0 means cat, 1 = dog, 2 = bird, etc.)\\n* (optionally) a subset (still in list format) of these labels to visualize on the plot\\n\\n![](/images/app_ui/demo_custom_chart_roc_curve.png)\\n\\n[See in the app →](https://wandb.ai/wandb/plots/reports/Plot-ROC-Curves--VmlldzoyNjk3MDE)\\n\\n[Run the code →](https://colab.research.google.com/drive/1\\\\_RMppCqsA8XInV\\\\_jhJz32NCZG6Z5t1RO?usp=sharing)\\n\\n  </TabItem>\\n</Tabs>\\n\\n### Custom presets\\n\\nTweak a builtin preset, or create a new preset, then save the chart. Use the chart ID to log data to that custom preset directly from your script.\\n\\n```python\\n# Create a table with the columns to plot\\ntable = wandb.Table(data=data, columns=[\"step\", \"height\"])\\n\\n# Map from the table\\'s columns to the chart\\'s fields\\nfields = {\"x\": \"step\", \"value\": \"height\"}\\n\\n# Use the table to populate the new custom chart preset\\n# To use your own saved chart preset, change the vega_spec_name\\nmy_custom_chart = wandb.plot_table(\\n    vega_spec_name=\"carey/new_chart\",\\n    data_table=table,\\n    fields=fields,\\n)\\n```\\n\\n[Run the code →](https://tiny.cc/custom-charts)\\n\\n![](/images/app_ui/custom_presets.png)\\n\\n## Log data\\n\\nHere are the data types you can log from your script and use in a custom chart:\\n\\n* **Config**: Initial settings of your experiment (your independent variables). This includes any named fields you\\'ve logged as keys to `wandb.config` at the start of your training (e.g. `wandb.config.learning_rate = 0.0001)`\\n* **Summary**: Single values logged during training (your results or dependent variables), e.g. `wandb.log({\"val_acc\" : 0.8})`. If you write to this key multiple times during training via `wandb.log()`, the summary is set to the final value of that key.\\n* **History**: The full time series of the logged scalar is available to the query via the `history` field\\n* **summaryTable**: If you need to log a list of multiple values, use a `wandb.Table()` to save that data, then query it in your custom panel.\\n* **historyTable**: If you need to see the history data, then query `historyTable` in your custom chart panel. Each time you call `wandb.Table()` or log a custom chart, you\\'re creating a new table in history for that step.\\n\\n### How to log a custom table\\n\\nUse `wandb.Table()` to log your data as a 2D array. Typically each row of this table represents one data point, and each column denotes the relevant fields/dimensions for each data point which you\\'d like to plot. As you configure a custom panel, the whole table will be accessible via the named key passed to `wandb.log()`(\"custom\\\\_data\\\\_table\" below), and the individual fields will be accessible via the column names (\"x\", \"y\", and \"z\"). You can log tables at multiple time steps throughout your experiment. The maximum size of each table is 10,000 rows.\\n\\n[Try it in a Google Colab →](https://tiny.cc/custom-charts)\\n\\n```python\\n# Logging a custom table of data\\nmy_custom_data = [[x1, y1, z1], [x2, y2, z2]]\\nwandb.log(\\n    {\"custom_data_table\": wandb.Table(data=my_custom_data, columns=[\"x\", \"y\", \"z\"])}\\n)\\n```\\n\\n## Customize the chart\\n\\nAdd a new custom chart to get started, then edit the query to select data from your visible runs. The query uses [GraphQL](https://graphql.org) to fetch data from the config, summary, and history fields in your runs.\\n\\n![Add a new custom chart, then edit the query](/images/app_ui/customize_chart.gif)\\n\\n### Custom visualizations\\n\\nSelect a **Chart** in the upper right corner to start with a default preset. Next, pick **Chart fields** to map the data you\\'re pulling in from the query to the corresponding fields in your chart. Here\\'s an example of selecting a metric to get from the query, then mapping that into the bar chart fields below.\\n\\n![Creating a custom bar chart showing accuracy across runs in a project](/images/app_ui/demo_make_a_custom_chart_bar_chart.gif)\\n\\n### How to edit Vega\\n\\nClick **Edit** at the top of the panel to go into [Vega](https://vega.github.io/vega/) edit mode. Here you can define a [Vega specification](https://vega.github.io/vega/docs/specification/) that creates an interactive chart in the UI. You can change any aspect of the chart, from the visual style (e.g. change the title, pick a different color scheme, show curves as a series of points instead of as connected lines) to the data itself (use a Vega transform to bin an array of values into a histogram, etc.). The panel preview will update interactively, so you can see the effect of your changes as you edit the Vega spec or query. The [Vega documentation and tutorials ](https://vega.github.io/vega/)are an excellent source of inspiration.\\n\\n**Field references**\\n\\nTo pull data into your chart from W&B, add template strings of the form `\"${field:<field-name>}\"` anywhere in your Vega spec. This will create a dropdown in the **Chart Fields** area on the right side, which users can use to select a query result column to map into Vega.\\n\\nTo set a default value for a field, use this syntax: `\"${field:<field-name>:<placeholder text>}\"`\\n\\n### Saving chart presets\\n\\nApply any changes to a specific visualization panel with the button at the bottom of the modal. Alternatively, you can save the Vega spec to use elsewhere in your project. To save the reusable chart definition, click **Save as** at the top of the Vega editor and give your preset a name.\\n\\n## Articles and guides\\n\\n1. [The W&B Machine Learning Visualization IDE](https://wandb.ai/wandb/posts/reports/The-W-B-Machine-Learning-Visualization-IDE--VmlldzoyNjk3Nzg)\\n2. [Visualizing NLP Attention Based Models](https://wandb.ai/kylegoyette/gradientsandtranslation2/reports/Visualizing-NLP-Attention-Based-Models-Using-Custom-Charts--VmlldzoyNjg2MjM)\\n3. [Visualizing The Effect of Attention on Gradient Flow](https://wandb.ai/kylegoyette/gradientsandtranslation/reports/Visualizing-The-Effect-of-Attention-on-Gradient-Flow-Using-Custom-Charts--VmlldzoyNjg1NDg)\\n4. [Logging arbitrary curves](https://wandb.ai/stacey/presets/reports/Logging-Arbitrary-Curves--VmlldzoyNzQyMzA)\\n\\n## Frequently asked questions\\n\\n### Coming soon\\n\\n* **Polling**: Auto-refresh of data in the chart\\n* **Sampling**: Dynamically adjust the total number of points loaded into the panel for efficiency\\n\\n### Gotchas\\n\\n* Not seeing the data you\\'re expecting in the query as you\\'re editing your chart? It might be because the column you\\'re looking for is not logged in the runs you have selected. Save your chart and go back out to the runs table, and select the runs you\\'d like to visualize with the **eye** icon.\\n\\n### How to show a \"step slider\" in a custom chart?\\n\\nThis can be enabled on the “Other settings” page of the custom chart editor. If you change your query to use a `historyTable` instead of the `summaryTable`, you\\'ll get an option to “Show step selector” in the custom chart editor. This gives you a slider that lets you select the step.\\n\\n<!-- ![Show step slider in a custom chart](/images/app_ui/step_sllider_custon_charts.mov>) -->\\n\\n### How to delete a custom chart preset?\\n\\nYou can do this by going into the custom chart editor. Then click on the currently selected chart type, this will open up a menu with all your presets. Hover the mouse on a preset you want to delete and then click on the Trash icon.\\n\\n![](/images/app_ui/delete_custome_chart_preset.gif)\\n\\n\\n### Common use cases\\n\\n* Customize bar plots with error bars\\n* Show model validation metrics which require custom x-y coordinates (like precision-recall curves)\\n* Overlay data distributions from two different models/experiments as histograms\\n* Show changes in a metric via snapshots at multiple points during training\\n* Create a unique visualization not yet available in W&B (and hopefully share it with the world)\\n',\n",
       "  'metadata': {'source': 'guides/app/features/custom-charts/intro.md',\n",
       "   'raw_tokens': 1904}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/raw_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = raw_artifact.download()\n",
    "raw_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "raw_data = list(map(json.loads, raw_data_file.read_text().splitlines()))\n",
    "raw_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us chunk each document in the raw data Artifact. We create a new list of dictionaries with the chuked text (`content`) and with `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "for doc in raw_data:\n",
    "    chunks = split_into_chunks(doc[\"content\"])\n",
    "    for chunk in chunks:\n",
    "        chunked_data.append(\n",
    "            {\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc[\"metadata\"][\"source\"],\n",
    "                    \"raw_tokens\": len(chunk.split()),\n",
    "                },\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "We clean the chunks for special tokens that we find breaks the OpenAI's `client.chat.completions` api. The data cleaning step is crucial for most ML pipelien and even for a RAG/Agentic pipeline. Usually, higher quality chunks provided to an LLM generates a higher quality response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_tokenization_safe(content: str) -> str:\n",
    "    special_tokens_set = {\n",
    "        \"<|endofprompt|>\",\n",
    "        \"<|endoftext|>\",\n",
    "        \"<|fim_middle|>\",\n",
    "        \"<|fim_prefix|>\",\n",
    "        \"<|fim_suffix|>\",\n",
    "    }\n",
    "\n",
    "    def remove_special_tokens(text: str) -> str:\n",
    "        \"\"\"Removes special tokens from the given text.\n",
    "\n",
    "        Args:\n",
    "            text: A string representing the text.\n",
    "\n",
    "        Returns:\n",
    "            The text with special tokens removed.\n",
    "        \"\"\"\n",
    "        for token in special_tokens_set:\n",
    "            text = text.replace(token, \"\")\n",
    "        return text\n",
    "\n",
    "    cleaned_content = remove_special_tokens(content)\n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 300,\n",
       "   'cleaned_tokens': 300},\n",
       "  'cleaned_content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in'},\n",
       " {'content': 'your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 170,\n",
       "   'cleaned_tokens': 170},\n",
       "  'cleaned_content': 'your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = []\n",
    "for doc in chunked_data:\n",
    "    cleaned_doc = doc.copy()\n",
    "    cleaned_doc[\"cleaned_content\"] = make_text_tokenization_safe(doc[\"content\"])\n",
    "    cleaned_doc[\"metadata\"][\"cleaned_tokens\"] = len(\n",
    "        cleaned_doc[\"cleaned_content\"].split()\n",
    "    )\n",
    "    cleaned_data.append(cleaned_doc)\n",
    "cleaned_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will store the cleaned data as an Artifact named `chunked_data`. The metadata that we are logging along with the artifact allows us to go back to it later and be able to reproduce the cleaning steps. It also allows us to pick the version of the `chunked_data` that we are willing to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact chunked_data>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_raw_tokens = sum(map(lambda x: x[\"metadata\"][\"raw_tokens\"], cleaned_data))\n",
    "total_cleaned_tokens = sum(map(lambda x: x[\"metadata\"][\"cleaned_tokens\"], cleaned_data))\n",
    "\n",
    "chunked_artifact = wandb.Artifact(\n",
    "    name=\"chunked_data\",\n",
    "    type=\"dataset\",\n",
    "    description=\"Chunked wandb documentation\",\n",
    "    metadata={\n",
    "        \"total_files\": len(cleaned_data),\n",
    "        \"date_processed\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"total_raw_tokens\": total_raw_tokens,\n",
    "        \"total_cleaned_tokens\": total_cleaned_tokens,\n",
    "        \"chunk_size\": CHUNK_SIZE,\n",
    "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
    "    },\n",
    ")\n",
    "with chunked_artifact.new_file(\"documents.jsonl\", mode=\"w\") as f:\n",
    "    for item in cleaned_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "run.log_artifact(chunked_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the data\n",
    "\n",
    "One of the key ingredient of most retrieval system is to represent the given modality (text in our case) as a vector. This vector is a numerical representation representing the \"content\" of that modality (text). \n",
    "\n",
    "Text vectorization (text to vector) can be done using various techniques like [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (Term Frequency-Inverse Document Frequency), and embeddings like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GloVe](https://nlp.stanford.edu/projects/glove/), and transformer based architectures like BERT and more, which capture the semantic meaning and relationships between words or sentences. \n",
    "\n",
    "Below, we are downloading the `cleaned_data` artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/08 12:36:39 [DEBUG] GET https://storage.googleapis.com/wandb-production.appspot.com/rag-course/dev/vr8n8v06/artifact/942570916/wandb_manifest.json?Expires=1720425999&GoogleAccessId=gorilla-files-url-signer-man%40wandb-production.iam.gserviceaccount.com&Signature=byvrxs3xky2kWaAkrlcGjhCPzdkNLw04EkXxHJfpF041RI%2B%2BhbEIX%2F3x1VsM8%2FK6t5xQsyBrro8Hp4MtnZZbOka%2BmTWrJfE13pOjiY2mU57zJ6du%2FCf6DN671ErceT4jL58W%2BIVKR9fYcvZLxEFRjOWu%2BNZWKIyywDirMK8MqxhSn5BwyWxeo%2FTvTtyWvgW4%2BmKU3ie5b7AYhHRxSWq0NpX6nfN2gf39EM6TL96jkC%2FVmBsNp1NYG9aMURt6Lmrjm282bSq12ZbonokQSzvPk0Zvp3LaYoq9bmayPHcIjfLhY46EsuUN3o4RfM%2BpM1EVPsS4aRw572b%2B1yt04aFIfA%3D%3D\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 300,\n",
       "   'cleaned_tokens': 300},\n",
       "  'cleaned_content': '--- description: Log and visualize data without a W&B account displayed_sidebar: default --- # Anonymous Mode Are you publishing code that you want anyone to be able to run easily? Use Anonymous Mode to let someone run your code, see a W&B dashboard, and visualize results without needing to create a W&B account first. Allow results to be logged in Anonymous Mode with `wandb.init(`**`anonymous=\"allow\"`**`)` :::info **Publishing a paper?** Please [cite W&B](https://docs.wandb.ai/company/academics#bibtex-citation), and if you have questions about how to make your code accessible while using W&B, reach out to us at support@wandb.com. ::: ### How does someone without an account see results? If someone runs your script and you have to set `anonymous=\"allow\"`: 1. **Auto-create temporary account:** W&B checks for an account that\\'s already signed in. If there\\'s no account, we automatically create a new anonymous account and save that API key for the session. 2. **Log results quickly:** The user can run and re-run the script, and automatically see results show up in the W&B dashboard UI. These unclaimed anonymous runs will be available for 7 days. 3. **Claim data when it\\'s useful**: Once the user finds valuable results in W&B, they can easily click a button in the banner at the top of the page to save their run data to a real account. If they don\\'t claim a run, it will be deleted after 7 days. :::caution **Anonymous run links are sensitive**. These links allow anyone to view and claim the results of an experiment for 7 days, so make sure to only share links with people you trust. If you\\'re trying to share results publicly, but hide the author\\'s identity, please contact us at support@wandb.com to share more about your use case. ::: ### What happens to users with existing accounts? If you set `anonymous=\"allow\"` in'},\n",
       " {'content': 'your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```',\n",
       "  'metadata': {'source': 'guides/app/features/anon.md',\n",
       "   'raw_tokens': 170,\n",
       "   'cleaned_tokens': 170},\n",
       "  'cleaned_content': 'your script, we will check to make sure there\\'s not an existing account first, before creating an anonymous account. This means that if a W&B user finds your script and runs it, their results will be logged correctly to their account, just like a normal run. ### What are features that aren\\'t available to anonymous users? * **No persistent data**: Runs are only saved for 7 days in an anonymous account. Users can claim anonymous run data by saving it to a real account. ![](@site/static/images/app_ui/anon_mode_no_data.png) * **No artifact logging**: Runs will print a warning on the command line that you can\\'t log an artifact to an anonymous run. ![](@site/static/images/app_ui/anon_example_warning.png) * **No profile or settings pages**: Certain pages aren\\'t available in the UI, because they\\'re only useful for real accounts. ## Example usage [Try the example notebook](http://bit.ly/anon-mode) to see how anonymous mode works. ```python import wandb # Start a run allowing anonymous accounts wandb.init(anonymous=\"allow\") # Log results from your training loop wandb.log({\"acc\": 0.91}) # Mark the run as finished wandb.finish() ```'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_artifact = run.use_artifact(\n",
    "    f\"{WANDB_ENTITY}/{WANDB_PROJECT}/chunked_data:latest\", type=\"dataset\"\n",
    ")\n",
    "artifact_dir = chunked_artifact.download()\n",
    "chunked_data_file = pathlib.Path(f\"{artifact_dir}/documents.jsonl\")\n",
    "chunked_data = list(map(json.loads, chunked_data_file.read_text().splitlines()))\n",
    "chunked_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![01_lineage_artifacs_data](../images/01_lineage_artifacts_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are creating a simple `Retriever` class. This class is responsible for vectorizing the chunks using the `index_data` method and provides a convenient method `search`, for querying the vector index using cosine distance similarity.\n",
    "\n",
    "- `index_data` will take a list of chunks and vectorize it using TF-IDF and store it as `index`. \n",
    "\n",
    "- `search` will take a `query` (question) and vectorize it using the same technique (TF-IDF in our case). It then computes the cosine distance between the query vector and the index (list of vectors) and pick the top `k` vectors from the index. These top `k` vectors represent the chunks that are closest (most relevant) to the `query`.\n",
    "\n",
    "---\n",
    "\n",
    "Note that the `Retriever` class is inherited from `weave.Model`. TODO: a bit on pydantic `BaseModel`?\n",
    "\n",
    "A Model is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates. By structuring your code to be compatible with this API, you benefit from a structured way to version your application so you can more systematically keep track of your experiments.\n",
    "\n",
    "To create a model in Weave, you need the following:\n",
    "\n",
    "- a class that inherits from weave.Model\n",
    "- type definitions on all attributes\n",
    "- a typed `predict`, `infer` or `forward` method with `@weave.op()` decorator.\n",
    "\n",
    "Imagine `weave.op()` to be a shameless use of `print` statement. If you have not initialized a weave run by doing `weave.init`, the code will work as it is without any tracking.\n",
    "\n",
    "The `predict` method decodated with `weave.op()` will track the model settings along with the inputs and outputs anytime you call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever(weave.Model):\n",
    "    vectorizer: TfidfVectorizer = TfidfVectorizer()\n",
    "    index: list = None\n",
    "    data: list = None\n",
    "\n",
    "    @weave.op()\n",
    "    def index_data(self, data):\n",
    "        self.data = data\n",
    "        docs = [doc[\"cleaned_content\"] for doc in data]\n",
    "        self.index = self.vectorizer.fit_transform(docs)\n",
    "\n",
    "    @weave.op()\n",
    "    def search(self, query, k=5):\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        cosine_distances = cdist(\n",
    "            query_vec.todense(), self.index.todense(), metric=\"cosine\"\n",
    "        )[0]\n",
    "        top_k_indices = cosine_distances.argsort()[:k]\n",
    "        output = []\n",
    "        for idx in top_k_indices:\n",
    "            output.append(\n",
    "                {\n",
    "                    \"source\": self.data[idx][\"metadata\"][\"source\"],\n",
    "                    \"text\": self.data[idx][\"cleaned_content\"],\n",
    "                    \"score\": 1 - cosine_distances[idx],\n",
    "                }\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, k: int):\n",
    "        return self.search(query, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our Retriever in action. We will index our chunked data and then ask a question to retriev related chunks.\n",
    "\n",
    "We will not be using weave here to show that the code works as it is and that weave is a lightweight wrapper, the benefit of which we will show in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'guides/technical-faq/general.md', 'text': '--- displayed_sidebar: default --- # General ### What does `wandb.init` do to my training process? When `wandb.init()` is called from your training script an API call is made to create a run object on our servers. A new process is started to stream and collect metrics, thereby keeping all threads and logic out of your primary process. Your script runs normally and writes to local files, while the separate process streams them to our servers along with system metrics. You can always turn off streaming by running `wandb off` from your training directory, or setting the `WANDB_MODE` environment variable to `offline`. ### Does your tool track or store training data? You can pass a SHA or other unique identifier to `wandb.config.update(...)` to associate a dataset with a training run. W&B does not store any data unless `wandb.save` is called with the local file name. ### What formula do you use for your smoothing algorithm? We use the same exponential moving average formula as TensorBoard. You can find an [explanation here](https://stackoverflow.com/questions/42281844/what-is-the-mathematics-behind-the-smoothing-parameter-in-tensorboards-scalar). ### How do I get the random run name in my script? Call `wandb.run.save()` and then get the name with `wandb.run.name` . ### What is the difference between `.log()` and `.summary`? The summary is the value that shows in the table while the log will save all the values for plotting later. For example, you might want to call `wandb.log` every time the accuracy changes. Usually, you can just use .log. `wandb.log()` will also update the summary value by default unless you have set the summary manually for that metric The scatterplot and parallel coordinate plots will also use the summary value while the line plot plots all of the values set by .log The reason we have both is that some people like to set the summary manually because', 'score': 0.28612762608021347}\n",
      "{'source': 'guides/technical-faq/troubleshooting.md', 'text': '--- displayed_sidebar: default --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; # Troubleshooting ### If wandb crashes, will it possibly crash my training run? It is extremely important to us that we never interfere with your training runs. We run wandb in a separate process to make sure that if wandb somehow crashes, your training will continue to run. If the internet goes out, wandb will continue to retry sending data to [wandb.ai](https://wandb.ai). ### Why is a run marked crashed in W&B when it’s training fine locally? This is likely a connection problem — if your server loses internet access and data stops syncing to W&B, we mark the run as crashed after a short period of retrying. ### Does logging block my training? \"Is the logging function lazy? I don\\'t want to be dependent on the network to send the results to your servers and then carry on with my local operations.\" Calling `wandb.log` writes a line to a local file; it does not block any network calls. When you call `wandb.init` we launch a new process on the same machine that listens for filesystem changes and talks to our web service asynchronously from your training process. ### How do I stop wandb from writing to my terminal or my jupyter notebook output? Set the environment variable [`WANDB_SILENT`](../track/environment-variables.md) to `true`. <Tabs defaultValue=\"python\" values={[ {label: \\'Python\\', value: \\'python\\'}, {label: \\'Jupyter Notebook\\', value: \\'notebook\\'}, {label: \\'Command Line\\', value: \\'command-line\\'}, ]}> <TabItem value=\"python\"> ```python os.environ[\"WANDB_SILENT\"] = \"true\" ``` </TabItem> <TabItem value=\"notebook\"> ```python %env WANDB_SILENT=true ``` </TabItem> <TabItem value=\"command-line\"> ```python WANDB_SILENT=true ``` </TabItem> </Tabs> ### How do I kill a job with wandb? Press `Ctrl+D` on your keyboard to stop a script that is instrumented with wandb. ### How do I deal with network issues? If you\\'re seeing SSL or network errors:`wandb:', 'score': 0.257494558146534}\n",
      "{'source': 'guides/track/log/logging-faqs.md', 'text': '--- description: >- Answers to frequently asked questions about tracking data from machine learning experiments with W&B Experiments. displayed_sidebar: default --- import Tabs from \\'@theme/Tabs\\'; import TabItem from \\'@theme/TabItem\\'; # Logging FAQs <head> <title>Frequently Asked Questions About Logging Data from Experiments</title> </head> ### How can I organize my logged charts and media in the W&B UI? We treat `/` as a separator for organizing logged panels in the W&B UI. By default, the component of the logged item\\'s name before a `/` is used to define a group of panel called a \"Panel Section\". ```python wandb.log({\"val/loss\": 1.1, \"val/acc\": 0.3}) wandb.log({\"train/loss\": 0.1, \"train/acc\": 0.94}) ``` In the [Workspace](../../app/pages/workspaces.md) settings, you can change whether panels are grouped by just the first component or by all components separated by `/`. ### How can I compare images or media across epochs or steps? Each time you log images from a step, we save them to show in the UI. Expand the image panel, and use the step slider to look at images from different steps. This makes it easy to compare how a model\\'s output changes during training. ### What if I want to log some metrics on batches and some metrics only on epochs? If you\\'d like to log certain metrics in every batch and standardize plots, you can log x axis values that you want to plot with your metrics. Then in the custom plots, click edit and select a custom x-axis. ```python wandb.log({\"batch\": batch_idx, \"loss\": 0.3}) wandb.log({\"epoch\": epoch, \"val_acc\": 0.94}) ``` ### How do I log a list of values? <!-- Logging lists directly is not supported. Instead, list-like collections of numerical data are converted to [histograms](../../../ref/python/data-types/histogram.md). To log all of the entries in a list, give a name to each entry in the list and use those names as keys', 'score': 0.2520496105116601}\n",
      "{'source': 'guides/track/log/media.md', 'text': 'table = wandb.Table(columns=columns) table.add_data(\"I love my phone\", \"1\", \"1\") table.add_data(\"My phone sucks\", \"0\", \"-1\") wandb.log({\"examples\": table}) ``` You can also pass a pandas `DataFrame` object. ```python table = wandb.Table(dataframe=my_dataframe) ``` </TabItem> <TabItem value=\"html\"> ```python wandb.log({\"custom_file\": wandb.Html(open(\"some.html\"))}) wandb.log({\"custom_string\": wandb.Html(\\'<a href=\"https://mysite\">Link</a>\\')}) ``` Custom html can be logged at any key, and this exposes an HTML panel on the run page. By default we inject default styles, you can disable default styles by passing `inject=False`. ```python wandb.log({\"custom_file\": wandb.Html(open(\"some.html\"), inject=False)}) ``` </TabItem> </Tabs> ## Frequently Asked Questions ### How can I compare images or media across epochs or steps? Each time you log images from a step, we save them to show in the UI. Expand the image panel, and use the step slider to look at images from different steps. This makes it easy to compare how a model\\'s output changes during training. ### What if I want to integrate W&B into my project, but I don\\'t want to upload any images or media? W&B can be used even for projects that only log scalars — you specify any files or data you\\'d like to upload explicitly. Here\\'s [a quick example in PyTorch](http://wandb.me/pytorch-colab) that does not log images. ### How do I log a PNG? [`wandb.Image`](../../../ref/python/data-types/image.md) converts `numpy` arrays or instances of `PILImage` to PNGs by default. ```python wandb.log({\"example\": wandb.Image(...)}) # Or multiple images wandb.log({\"example\": [wandb.Image(...) for img in images]}) ``` ### How do I log a video? Videos are logged using the [`wandb.Video`](../../../ref/python/data-types/video.md) data type: ```python wandb.log({\"example\": wandb.Video(\"myvideo.mp4\")}) ``` Now you can view videos in the media browser. Go to your project workspace, run workspace, or report and click \"Add visualization\" to add a rich media panel. ### How do I navigate and zoom in point clouds? You can hold control and use the mouse to move around inside the space. ###', 'score': 0.23935694811788255}\n",
      "{'source': 'guides/integrations/huggingface.md', 'text': \"manage model lifecycle, facilitate easy tracking and auditing throughout the ML lifecyle, and [automate](https://docs.wandb.ai/guides/models/automation) downstream actions with webhooks or jobs. See the [Model Registry](../model_registry) documentation for how to link a model Artifact to the Model Registry. ### 5) Visualise evaluation outputs during training Visualing your model outputs during training or evaluation is often essential to really understand how your model is training. By using the callbacks system in the Transformers Trainer, you can log additional helpful data to W&B such as your models' text generation outputs or other predictions to W&B Tables. See the **[Custom logging section](#custom-logging-log-and-view-evaluation-samples-during-training)** below for a full guide on how to log evaluation outupts while training to log to a W&B Table like this: ![Shows a W&B Table with evaluation outputs](/images/integrations/huggingface_eval_tables.png) ### 6) Finish your W&B Run (Notebook only) If your training is encapsulated in a Python script, the W&B run will end when your script finishes. If you are using a Jupyter or Google Colab notebook, you'll need to tell us when you're done with training by calling `wandb.finish()`. ```python trainer.train() # start training and logging to W&B # post-training analysis, testing, other logged code wandb.finish() ``` ### 7) Visualize your results Once you have logged your training results you can explore your results dynamically in the [W&B Dashboard](../track/app.md). It's easy to compare across dozens of runs at once, zoom in on interesting findings, and coax insights out of complex data with flexible, interactive visualizations. ## Advanced features and FAQs ### How do I save the best model? If `load_best_model_at_end=True` is set in the `TrainingArguments` that are passed to the `Trainer`, then W&B will save the best performing model checkpoint to Artifacts. If you'd like to centralize all your best model versions across your team to organize them by ML task, stage them for production,\", 'score': 0.228041701733378}\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I use W&B to log metrics in my training script?\"\n",
    "search_results = retriever.search(query)\n",
    "for result in search_results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a response\n",
    "\n",
    "There are two components of any RAG pipeline - a `Retriever` and a `ResponseGenerator`. Earlier, we designed a simple retriever. Here we are designing a simple `ResponseGenerator`. \n",
    "\n",
    "The `generate_response` method takes the user question along with the retrieved context (chunks) as inputs and makes a LLM call using the `model` and `prompt` (system prompt). This way the generated answer is grounded on the documentation (our usecase). In this course we are using Cohere's `command-r` model.\n",
    "\n",
    "As earlier, we have wrapped this `ResponseGenerator` class with weave for tracking the inputs and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerator(weave.Model):\n",
    "    model: str\n",
    "    prompt: str\n",
    "    client: cohere.Client = None\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.client = cohere.Client(api_key=os.environ[\"CO_API_KEY\"])\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_context(self, context: List[Dict[str, any]]) -> str:\n",
    "        return [{\"source\": item['source'], \"text\": item['text']} for item in context]\n",
    "    \n",
    "    @weave.op()\n",
    "    def generate_response(self, query: str, context: List[Dict[str, any]]) -> str:\n",
    "        contexts = self.generate_context(context)\n",
    "        response = self.client.chat(\n",
    "            preamble=self.prompt,\n",
    "            message=query,\n",
    "            model=self.model,\n",
    "            documents=contexts,\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str, context: List[Dict[str, any]]):\n",
    "        return self.generate_response(query, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the system prompt. Consider this to be set of instructions on what to do with the user question and the retrieved contexts. In practice, the system prompt can be very detailed and involved (depending on the usecase) but we are showing a simple prompt. Later we will iterate on it and show how improving the system prompt improves the quality of the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "Answer to the following question about W&B. Provide an helful and complete answer based only on the provided documents.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the response for the question \"How do I use W&B to log metrics in my training script?\". We have already retrieved the context in the previous section and passing both the question and the context to the `generate_response` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the W&B to log metrics in your training script by following these steps:\n",
      "1. First, import wandb at the top of your training script:\n",
      "```python\n",
      "import wandb\n",
      "```\n",
      "2. Then, call `wandb.init()` at the beginning of your training script. This creates a run object on W&B servers and starts a new process to stream and collect metrics.\n",
      "3. Your script runs as normal and writes to local files, while the separate process streams them to the W&B servers along with system metrics. \n",
      "4. To log a metric, call `wandb.log()` at the relevant point in your script. For example, you might call this every time the accuracy changes.\n",
      "\n",
      "Here is an example of logging two metrics, 'val/loss' and 'val/acc':\n",
      "```python\n",
      "wandb.log({\"val/loss\": 1.1, \"val/acc\": 0.3})\n",
      "```\n",
      "You can also log images, videos and data tables using specific W&B functions such as `wandb.Image()`, `wandb.Video()` and `wandb.Table().()`\n",
      "\n",
      "Make sure to call `wandb.finish()` at the end of your training script to conclude the W&B run. You don't need to include this if your training script is encapsulated in a Python script, as the W&B run will end automatically when your script finishes.\n"
     ]
    }
   ],
   "source": [
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=PROMPT)\n",
    "answer = response_generator.generate_response(query, search_results)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Retrieval Augmented Generation (RAG) Pipeline\n",
    "\n",
    "Below we are bringing everything together. As stated a simple RAG pipeline constitute of a retriever and a response generator. \n",
    "\n",
    "The `__call__` method is calling the `predict` method which is decorated with `weave.op()`. It takes the user query, retrieves relevant context using the retriever and finally synthesize a response grounded on the data source (documentation in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline(weave.Model):\n",
    "    retriever: Retriever = None\n",
    "    response_generator: ResponseGenerator = None\n",
    "    top_k: int = 5\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, query: str):\n",
    "        context = self.retriever.predict(query, self.top_k)\n",
    "        return self.response_generator.predict(query, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the `RAGPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = Retriever()\n",
    "retriever.index_data(chunked_data)\n",
    "\n",
    "# Initialize the response generator\n",
    "response_generator = ResponseGenerator(model=\"command-r\", prompt=PROMPT)\n",
    "\n",
    "# Bring them together as a RAG pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retriever=retriever,\n",
    "    response_generator=response_generator,\n",
    "    top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to use `weave.init(<project-name>)` to see what we get from using W&B Weave. Once intialized, we will start tracking the inputs and the outputs along with underlying attributes (model name, top_k, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave version 0.50.7 is available!  To upgrade, please run:\n",
      " $ pip install weave --upgrade\n",
      "Logged in as Weights & Biases user: parambharat.\n",
      "View Weave data at https://wandb.ai/rag-course/dev/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.weave_client.WeaveClient at 0x7d70fd3ff190>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weave.init(f\"{WANDB_ENTITY}/{WANDB_PROJECT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍩 https://wandb.ai/rag-course/dev/r/call/c1bdd7af-8ec2-46c9-837e-7350d030121c\n",
      "To get started with wandb.init(), first import wandb into your training script and then call `wandb.init()` which <co: 3>creates a run object on W&B servers and streams metrics from a separate process so as not to slow down your primary process. Your script will run as normal while the separate process streams the metrics to the servers. \n",
      "\n",
      "You can also use W&B with Jupyter Notebooks. To get started, first install W&B and link your account in your notebook with the following code:\n",
      "```notebook \n",
      "!pip install wandb -qqq\n",
      "import wandb\n",
      "wandb.login()\n",
      "```\n",
      "Then, set up your experiment and save your hyperparameters using `wandb.init()`. For example:\n",
      "```python\n",
      "wandb.init(\n",
      "    project=\"jupyter-projo\",\n",
      "    config={\n",
      "        \"batch_size\": 128,\n",
      "        \"learning_rate\": 0.01,\n",
      "        \"dataset\": \"CIFAR-100\",\n",
      "    },\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = rag_pipeline.predict(\"How do I get get started with wandb?\")\n",
    "print(response, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link starting with a 🍩. This is the trace timeline for all the executions that happened in our simple RAG application. Go to the link and drill down to find everything that got tracked.\n",
    "\n",
    "![weave trace timeline](../images/01_weave_trace_timeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add exercise for chapter 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-edu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
